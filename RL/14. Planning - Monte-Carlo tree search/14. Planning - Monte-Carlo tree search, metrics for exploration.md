[[Обучение_с_подкреплением_Введение_2_е_издание_2020_Ричард_С_Саттон.pdf | Саттон]] со страницы 226:
Model-based алгоритм
![[Pasted image 20240126073439.png]]
![[Pasted image 20240126073533.png]]
![[Pasted image 20240126073552.png]]
![[Pasted image 20240126073617.png]]
![[Pasted image 20240126073822.png]]
>[!info]- Предыстория для понимания
![[Pasted image 20240123222407.png]]
Пример:
Learning - нет доступа к тому, что внутри чёрного ящика
Planning - есть доступ к тому, что внутри ящика
![[Pasted image 20240123225813.png]]
$P(s_next|s,a)$ - функция перехода между состояниями
И есть функция reward.
![[Pasted image 20240124161305.png]]
Задача поиска пути (в алгоритмах был Дейкстра) есть граф состояний;
есть действия, которые детерминистично приводят из одного состояния в другое;
есть -награда, которые по сути являются рёбрами графа.
Надо найти путь короче.
![[Pasted image 20240124161700.png]]
![[Pasted image 20240124161715.png]]
![[Pasted image 20240124162122.png]]
Рассматриваем с точки зрения оценки длины пути, а не в порядке расстояния от начала.
![[Pasted image 20240124162704.png]]
Если Дейкстра рассматривает узлы во все стороны с одинаковой силой, то при использовании эвристики преимущества будут получать те узлы, которые находятся ближе к концу пути.
![[Pasted image 20240124163306.png]]
Всегда ли надо рассматривать все возможные переходы и считать честный максимум? Можно ли где-то сэкономить?
Поэтому, если видно, что какие-то действия приведут нас к уменьшению награды - можно заранее сделать прунинг(?) - обрубание ветвей - не рассматриваем всё то, что уже хуже нашего оптимального действия и можно также не рассматривать те варианты, которые точно не выгодны нашему противнику.
![[Pasted image 20240124163833.png]]
Теперь учтём, что наши переходы ещё и случайные и хотим получать в мат. ожидании максимальную награду. Награды выбираются по вероятностному распределению.
Как в таком случае учесть действия противника (например, он ходит совершенно рандомно и даже может выполнить те действия, которые точно ему не выгодны)?
Тогда вместо минимизации будет мат ожидание.
![[Pasted image 20240124164213.png]]
![[Pasted image 20240124164228.png]]
![[Pasted image 20240124164751.png]]
![[Pasted image 20240124165045.png]]
![[Pasted image 20240124165109.png]]
![[Pasted image 20240124165139.png]]
Можно делать свой UCB (upper confidence bound верхняя доверительная граница) для каждого узла.
На данных этапах мы просто понимаем: какие именно части дерева нам интереснее, но ещё не решаем задачу.


# MCTS
![[Pasted image 20240124165852.png]]
Алгоритм состоит из четырёх стадий, которые зациклены между собой.
### Первая стадия
Построили себе немножко дерева и знаем мат ожидание полезности вершин
Можем выбрать какой из узлов после корня кажется наиболее приоритетным, и потом у следующего и т.д. и т.п., выбираем, пока не сходимся в оптимум.
Оптимум - узел показался наиболее интересным UCB.
Этот узел не только больше всего исследован, но у него и reward лучше.
![[Pasted image 20240124170016.png]]
Знаем: сколько раз заходили в этот узел и знаем мат ожидание полезности.
($n_{s,a}$ - количество раз сколько были в узле, который последний обведён жирным; $N_{s}$ - количество раз, сколько были в предпоследнем обведённом жирным узле, т.е. узле-родителе)
![[Pasted image 20240124220527.png]]
### Вторая стадия
Можно из выбранного последнего узла попланировать: вывести последующие узлы.
Как можно примерно понять: новые узлы выгодны или нет?
![[Pasted image 20240124170308.png]]
### Третья стадия
Оценим награды полезного состояния (у новых узлов) - выполняем случайные действия и смотрим - какую награду они дадут.
Проблемы:
-оценка зашумлена
-оценка смещена
-случайное действие может всё испортить, и оценка полезности будет занижена
![[Pasted image 20240124170915.png]]
### Четвёртая стадия
Распространяем полезность состояния вверх по дереву
![[Pasted image 20240124171639.png]]

Как выбрать действие? Или иными словами выбрать полезность: мат ожидание или UCB?
Берём полезность наибольшего мат ожидания.

В любой момент времени можно остановить дерево и получить оценки.

==чатик==
Поиск по дереву Монте-Карло (MCTS) — это эвристический алгоритм поиска, используемый в процессах принятия решений, особенно в компьютерных играх, таких как го и шахматы. Он использует случайную выборку для оценки состояний игры и выбирает действие, которое приводит к наилучшему среднему вознаграждению.

Вот базовое описание того, как работает MCTS:

**Выбор**: начиная с корневого узла, алгоритм выбирает последовательные дочерние узлы, пока не достигнет конечного узла. Процесс выбора руководствуется формулой верхней доверительной границы 1, применяемой к деревьям (UCB1), которая уравновешивает исследование и эксплуатацию.

**Расширение**: если выбранный листовой узел находится в конечном состоянии (т. е. конец игры), алгоритм создает все возможные дочерние узлы и возвращает результат игры соответствующему дочернему узлу.

**Моделирование**: если выбранный листовой узел не находится в терминальном состоянии, алгоритм случайным образом выбирает один из его дочерних узлов и доводит игру до терминального состояния. Результат игры затем возвращается соответствующему дочернему узлу.

**Обратное распространение ошибки**: алгоритм обновляет статистику узлов на пути от листового узла до корневого узла. Это включает в себя обновление количества посещений и общей суммы вознаграждения.

**Принятие решения**: алгоритм повторяет процессы выбора, расширения, моделирования и обратного распространения ошибки до тех пор, пока не будет выполнено условие остановки (например, максимальное количество итераций). В качестве окончательного решения выбирается действие, ведущее к дочернему узлу с наибольшим количеством посещений.
Хотите узнать больше о том, как реализовать MCTS на конкретном языке программирования или варианте использования?
