...
**Terms**

| Term                             | Definition                                                                                                                                                                                             | Formula                                                                                                                                                                                                                                                                                                   |
|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Bellman Optimality Equation        | An equation in dynamic programming that expresses the optimal value of a state (or state-action pair) in terms of the expected immediate reward and the value of the next state (or state-action pair). | $V^*(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot V^*(s') \right)$ for state-value function.<br>$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot \max_{a'} Q^*(s', a')$ for state-action value function. |
| Convergence                       | The point at which an iterative algorithm stabilizes, and further iterations do not significantly change the results. In reinforcement learning, it often refers to the stability of value estimates. | Not applicable.                                                                                                                                                                                                                                                                                          |
| Deterministic Environment         | An environment where the outcome of an action is entirely predictable and does not involve randomness.                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Exploration-Exploitation Tradeoff | The balance between exploring unknown options (exploration) and exploiting known options (exploitation) to maximize cumulative rewards in reinforcement learning.                             | Not applicable.                                                                                                                                                                                                                                                                                          |
| Markov Decision Process (MDP)      | A mathematical model used to describe decision-making in situations where outcomes are partially random and partially under the control of a decision-maker.                                        | $P(y \| x_1, x_2, ..., x_n) = P(y \| x_n)$<br> |
| Model-Free Learning               | Learning approaches in reinforcement learning that do not rely on a known model of the environment. Instead, they learn directly from interactions with the environment.                          | Not applicable.                                                                                                                                                                                                                                                                                          |
| Policy                           | A strategy or a set of rules that defines an agent's behavior in a given environment. In reinforcement learning, policies guide decision-making.                                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Solution Space                    | The set of all possible solutions to a given problem.                                                                                        | Not applicable.                                                                                                                                                                                                                                                                                          |
| State-Action Value Function (Q)    | Represents the expected cumulative reward from a given state and action under a certain policy. It assesses the value of taking a specific action in a specific state.                           | $Q(s, a) = E \left[ R_t + \gamma Q(s', a') \mid s, a \right]$where $R_t$ is the immediate reward, $s'$ is the next state, and $a'$ is the next action.                                                                                                                                        |
| State-Value Function               | In reinforcement learning, it represents the expected cumulative reward from a given state under a certain policy. It evaluates how good it is for the agent to be in a specific state.             | $V(s) = E \left[ R_t + \gamma V(s') \mid s \right]$ where $R_t$ is the immediate reward, $s'$ is the next state, and $\gamma$ is the discount factor.                                                                                                                                      |
| Stochasticity                     | The presence of randomness or uncertainty in an environment or a system. In reinforcement learning, it may refer to uncertain outcomes of actions or transitions between states.                  | Not applicable.                                                                                                                                                                                                                                                                                          |



**Methods**


| Method Name | Key Concept | Steps | Application in RL | When Used |
| ---- | ---- | ---- | ---- | ---- |
| Cross-Entropy Method (CEM) | Iterative improvement of a probability distribution to optimise solutions. | 1. Initialisation: Set up an initial parameterised distribution over the solution space.  <br>2. Generate Samples: Randomly sample solutions (e.g., policies) from the distribution.  <br>3. Evaluate Solutions: Assess the performance of each sampled solution using an objective function.  <br>4. Select Top Solutions: Identify the top-performing solutions based on evaluation scores.  <br>5. Update Distribution: Adjust distribution parameters to favour solutions similar to the top-performing ones.  <br>6. Repeat: Iteratively execute steps 2 to 5 for a fixed number of iterations or until convergence criteria are met.  <br>7. Convergence Check: Assess solution quality or distribution stability; decide whether to continue iterations or stop. | Policy optimisation in reinforcement learning. | Exploration in complex environments. <br>Approximate method is used when the environment is too large to be enumerated. |
| Value-Based Policy Iteration | Iterative improvement of a value function to derive an optimal policy. | 1. Initialisation: Set initial values for the state-value function or state-action value function.  <br>2. Policy Evaluation: Iteratively update value estimates based on the current policy.  <br>3. Policy Improvement: Greedily improve the policy based on the current value estimates.  4. Repeat: Iterate steps 2 and 3 until convergence. | Deriving an optimal policy by iteratively refining the value function. | Deterministic environments with known models. |
| Value Iteration | Iterative improvement of a value function to find the optimal value estimates. | 1. Initialisation: Set initial values for the state-value function or state-action value function.  <br>2. Iterative Update: Repeatedly update value estimates using the Bellman optimality equation.  <br>3. Convergence Check: Assess convergence of value estimates. | Finding optimal value estimates for states or state-action pairs. | Markov decision processes with known dynamics. |
| Q-Learning | Learning the optimal action-value function through exploration and exploitation. | 1. Initialisation: Set initial values for the state-action value function (Q-function).  2. Exploration-Exploitation: Choose actions based on an exploration-exploitation strategy.  3. Update Q-Values: Update Q-values based on observed rewards and transitions using the Bellman equation.  4. Repeat: Iterate steps 2 and 3 through multiple episodes or time steps. | Learning optimal Q-values to make decisions in an uncertain environment. | Model-free learning in environments with uncertainties. |
| Monte-Carlo | A model-free approach used to estimate V-functions by sampling episodes in the environment. <br>Averaging the returns observed from multiple simulated episodes. | 1.Interact with env. simulate episode(s) => run end-to-end<br>2.For each state, calculate average rewards, V-functions from available observations | Episodic environments with a terminal state.<br>Effective in scenarios with non-deterministic exploration. | Model-free<br>Off-policy<br>Stochastic<br>Needs a whole go through the episode |
| Temporal Difference (TD) | Estimate V-function based on temporal difference errors |  |  |  |


- **Model-based**: when we know the policy
	- can apply dynamic programming
	- can plan ahead
- **Model-free**: don't know the police, need to explore
	- no guarantee
	- can try stuff out
- **Trajectory**: a seq of (s, a, r)
- **Monte-Carlo**: used to estimate V-function by sampling episodes in the environment, then averaging the returns observed from multiple simulated episodes
	- steps:
		- sample episodes
		- for each state, calculate average rewards, V-functions
	- model free
	- off-policy
	- non-deterministic
	- needs a whole trajectory
- **TD** (temporal difference)
	- estimate V-function based on temporal difference errors
	- applicable in indefinite environment
	- calculates V(s) for each step (takes V(s+1) from knows Vs)
	- $V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)$
	- under alfa - temporal difference
	- Rt+1 + gamma - the goal
- **In between**: a combination of Monte-Carlo and TD with executing a few steps 
- **Q-learning**: a model-free reinforcement learning algorithm that enables an agent to learn a policy, represented by the action-value function Q, to make decisions in an environment. The primary idea behind Q-learning is to iteratively update the Q-values based on the observed rewards and the maximum expected future rewards.
	- off-policy: learns a policy that is different from used at a data collection phase
	- Q-values are updated iteratively as the agent learns from interactions
	- Used TD learning approach
	- Steps:
		- initialise Q(s,a) with 0s
		- choose exploration-exploitation (optional)
		- sample <s, a, r, s'>
		- compute new $\hat{Q}$ $(s,a) = r(s,a) + \gamma max_a Q(s',a)$
		- update $Q(s,a) = \alpha \hat{Q}(s,a) + (1 - \alpha)Q(s,a)$
	- $Q(S_t,A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma maxQ(S_{t+1},a) - Q(S_t,A_t)]$
	- использует действие с самой высокой ожидаемой ценностью
	- получаем оптимальные Q, V, $\pi$
- SARSA
	- $Q(S_t,A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]$
	- использует выбранное действие
	- on-policy




**Q-Learning Main Idea:**

Q-Learning is a model-free reinforcement learning algorithm that enables an agent to learn a policy, represented by the action-value function (\(Q\)), to make decisions in an environment. The primary idea behind Q-learning is to iteratively update the Q-values based on the observed rewards and the maximum expected future rewards.

**Key Concepts:**

1. **Action-Value Function (\(Q\)):**
   - \(Q(s, a)\) represents the expected cumulative rewards of taking action \(a\) in state \(s\).
   - Q-values are updated iteratively as the agent learns from interactions.

2. **Temporal Difference Learning:**
   - Q-learning uses a temporal difference (TD) learning approach.
   - Q-values are updated based on the temporal difference error, representing the difference between the current estimate and a new estimate considering immediate and future rewards.

3. **Greedy Policy:**
   - The agent follows a greedy policy by choosing the action with the highest Q-value in a given state.
   - This ensures exploitation of the current knowledge.

4. **Exploration-Exploitation Trade-off:**
   - Q-learning balances exploration and exploitation through an exploration strategy (e.g., epsilon-greedy).
   - With a certain probability (\(\epsilon\)), the agent explores by choosing a random action.

**Main Steps:**

1. **Initialization:**
   - Initialize the Q-values arbitrarily for all state-action pairs.

2. **Exploration and Action Selection:**
   - Choose an action in the current state based on the exploration-exploitation strategy.

3. **Environment Interaction:**
   - Execute the chosen action and observe the immediate reward and the next state.

4. **Q-Value Update:**
   - Update the Q-value for the current state-action pair based on the observed reward and the maximum expected future rewards.
   \[ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] \]
   - \(\alpha\) is the learning rate, \(R\) is the immediate reward, \(\gamma\) is the discount factor, \(s'\) is the next state, and \(a'\) is the action in the next state.

5. **Repeat:**
   - Continue the process through multiple iterations, allowing the agent to learn an optimal policy.

**Objective:**
The objective of Q-learning is to converge to an optimal action-value function (\(Q^*\)) that represents the maximum expected cumulative rewards for each state-action pair, enabling the agent to make optimal decisions in the environment.