...
**Terms**

| Term                             | Definition                                                                                                                                                                                             | Formula                                                                                                                                                                                                                                                                                                   |
|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Bellman Optimality Equation        | An equation in dynamic programming that expresses the optimal value of a state (or state-action pair) in terms of the expected immediate reward and the value of the next state (or state-action pair). | $V^*(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot V^*(s') \right)$ for state-value function.<br>$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot \max_{a'} Q^*(s', a')$ for state-action value function. |
| Convergence                       | The point at which an iterative algorithm stabilizes, and further iterations do not significantly change the results. In reinforcement learning, it often refers to the stability of value estimates. | Not applicable.                                                                                                                                                                                                                                                                                          |
| Deterministic Environment         | An environment where the outcome of an action is entirely predictable and does not involve randomness.                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Exploration-Exploitation Tradeoff | The balance between exploring unknown options (exploration) and exploiting known options (exploitation) to maximize cumulative rewards in reinforcement learning.                             | Not applicable.                                                                                                                                                                                                                                                                                          |
| Markov Decision Process (MDP)      | A mathematical model used to describe decision-making in situations where outcomes are partially random and partially under the control of a decision-maker.                                        | $V^{\pi}(s) = \sum_{a} \pi(a \mid s) \sum_{s', r} P(s', r \mid s, a) \cdot \left[ r + \gamma \cdot V^{\pi}(s') \right]$ where $V^{\pi}(s)$ is the value of state $s$ under policy $\pi$, $P(s', r \mid s, a)$ is the transition probability, $r$ is the immediate reward, and $\gamma$ is the discount factor. |
| Model-Free Learning               | Learning approaches in reinforcement learning that do not rely on a known model of the environment. Instead, they learn directly from interactions with the environment.                          | Not applicable.                                                                                                                                                                                                                                                                                          |
| Policy                           | A strategy or a set of rules that defines an agent's behavior in a given environment. In reinforcement learning, policies guide decision-making.                                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Solution Space                    | The set of all possible solutions to a given problem.                                                                                        | Not applicable.                                                                                                                                                                                                                                                                                          |
| State-Action Value Function (Q)    | Represents the expected cumulative reward from a given state and action under a certain policy. It assesses the value of taking a specific action in a specific state.                           | $Q(s, a) = E \left[ R_t + \gamma Q(s', a') \mid s, a \right]$where $R_t$ is the immediate reward, $s'$ is the next state, and $a'$ is the next action.                                                                                                                                        |
| State-Value Function               | In reinforcement learning, it represents the expected cumulative reward from a given state under a certain policy. It evaluates how good it is for the agent to be in a specific state.             | $V(s) = E \left[ R_t + \gamma V(s') \mid s \right]$ where $R_t$ is the immediate reward, $s'$ is the next state, and $\gamma$ is the discount factor.                                                                                                                                      |
| Stochasticity                     | The presence of randomness or uncertainty in an environment or a system. In reinforcement learning, it may refer to uncertain outcomes of actions or transitions between states.                  | Not applicable.                                                                                                                                                                                                                                                                                          |



**Methods**


| Method Name                   | Key Concept                                                                      | Steps                                                                                                                                                         | Application in RL                                                         | When Used                                                                                                                                                   |
|-------------------------------|----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Cross-Entropy Method (CEM)     | Iterative improvement of a probability distribution to optimise solutions.       | 1. Initialisation: Set up an initial parameterised distribution over the solution space.  2. Generate Samples: Randomly sample solutions (e.g., policies) from the distribution.  3. Evaluate Solutions: Assess the performance of each sampled solution using an objective function.  4. Select Top Solutions: Identify the top-performing solutions based on evaluation scores.  5. Update Distribution: Adjust distribution parameters to favour solutions similar to the top-performing ones.  6. Repeat: Iteratively execute steps 2 to 5 for a fixed number of iterations or until convergence criteria are met.  7. Convergence Check: Assess solution quality or distribution stability; decide whether to continue iterations or stop.                                     | Policy optimisation in reinforcement learning.                           | Exploration in complex environments.                                                                                                                      |
| Value-Based Policy Iteration  | Iterative improvement of a value function to derive an optimal policy.           | 1. Initialisation: Set initial values for the state-value function or state-action value function.  2. Policy Evaluation: Iteratively update value estimates based on the current policy.  3. Policy Improvement: Greedily improve the policy based on the current value estimates.  4. Repeat: Iterate steps 2 and 3 until convergence.                                    | Deriving an optimal policy by iteratively refining the value function.     | Deterministic environments with known models.                                                                                                              |
| Value Iteration               | Iterative improvement of a value function to find the optimal value estimates.   | 1. Initialisation: Set initial values for the state-value function or state-action value function.  2. Iterative Update: Repeatedly update value estimates using the Bellman optimality equation.  3. Convergence Check: Assess convergence of value estimates.                                            | Finding optimal value estimates for states or state-action pairs.          | Markov decision processes with known dynamics.                                                                                                            |
| Q-Learning                    | Learning the optimal action-value function through exploration and exploitation. | 1. Initialisation: Set initial values for the state-action value function (Q-function).  2. Exploration-Exploitation: Choose actions based on an exploration-exploitation strategy.  3. Update Q-Values: Update Q-values based on observed rewards and transitions using the Bellman equation.  4. Repeat: Iterate steps 2 and 3 through multiple episodes or time steps. | Learning optimal Q-values to make decisions in an uncertain environment.  | Model-free learning in environments with uncertainties.                                                                                                      |





