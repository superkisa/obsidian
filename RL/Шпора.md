...
**Terms**

| Term                             | Definition                                                                                                                                                                                             | Formula                                                                                                                                                                                                                                                                                                   |
|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Bellman Optimality Equation        | An equation in dynamic programming that expresses the optimal value of a state (or state-action pair) in terms of the expected immediate reward and the value of the next state (or state-action pair). | $V^*(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot V^*(s') \right)$ for state-value function.<br>$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot \max_{a'} Q^*(s', a')$ for state-action value function. |
| Convergence                       | The point at which an iterative algorithm stabilizes, and further iterations do not significantly change the results. In reinforcement learning, it often refers to the stability of value estimates. | Not applicable.                                                                                                                                                                                                                                                                                          |
| Deterministic Environment         | An environment where the outcome of an action is entirely predictable and does not involve randomness.                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Exploration-Exploitation Tradeoff | The balance between exploring unknown options (exploration) and exploiting known options (exploitation) to maximize cumulative rewards in reinforcement learning.                             | Not applicable.                                                                                                                                                                                                                                                                                          |
| Markov Decision Process (MDP)      | A mathematical model used to describe decision-making in situations where outcomes are partially random and partially under the control of a decision-maker.                                        | $P(y \| x_1, x_2, ..., x_n) = P(y \| x_n)$<br> |
| Model-Free Learning               | Learning approaches in reinforcement learning that do not rely on a known model of the environment. Instead, they learn directly from interactions with the environment.                          | Not applicable.                                                                                                                                                                                                                                                                                          |
| Policy                           | A strategy or a set of rules that defines an agent's behavior in a given environment. In reinforcement learning, policies guide decision-making.                                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Solution Space                    | The set of all possible solutions to a given problem.                                                                                        | Not applicable.                                                                                                                                                                                                                                                                                          |
| State-Action Value Function (Q)    | Represents the expected cumulative reward from a given state and action under a certain policy. It assesses the value of taking a specific action in a specific state.                           | $Q(s, a) = E \left[ R_t + \gamma Q(s', a') \mid s, a \right]$where $R_t$ is the immediate reward, $s'$ is the next state, and $a'$ is the next action.                                                                                                                                        |
| State-Value Function               | In reinforcement learning, it represents the expected cumulative reward from a given state under a certain policy. It evaluates how good it is for the agent to be in a specific state.             | $V(s) = E \left[ R_t + \gamma V(s') \mid s \right]$ where $R_t$ is the immediate reward, $s'$ is the next state, and $\gamma$ is the discount factor.                                                                                                                                      |
| Stochasticity                     | The presence of randomness or uncertainty in an environment or a system. In reinforcement learning, it may refer to uncertain outcomes of actions or transitions between states.                  | Not applicable.                                                                                                                                                                                                                                                                                          |



**Methods**


| Method Name | Key Concept | Steps | Application in RL | When Used |
| ---- | ---- | ---- | ---- | ---- |
| Cross-Entropy Method (CEM) | Iterative improvement of a probability distribution to optimise solutions. | 1. Initialisation: Set up an initial parameterised distribution over the solution space.  <br>2. Generate Samples: Randomly sample solutions (e.g., policies) from the distribution.  <br>3. Evaluate Solutions: Assess the performance of each sampled solution using an objective function.  <br>4. Select Top Solutions: Identify the top-performing solutions based on evaluation scores.  <br>5. Update Distribution: Adjust distribution parameters to favour solutions similar to the top-performing ones.  <br>6. Repeat: Iteratively execute steps 2 to 5 for a fixed number of iterations or until convergence criteria are met.  <br>7. Convergence Check: Assess solution quality or distribution stability; decide whether to continue iterations or stop. | Policy optimisation in reinforcement learning. | Exploration in complex environments. <br>Approximate method is used when the environment is too large to be enumerated. |
| Value-Based Policy Iteration | Iterative improvement of a value function to derive an optimal policy. | 1. Initialisation: Set initial values for the state-value function or state-action value function.  <br>2. Policy Evaluation: Iteratively update value estimates based on the current policy.  <br>3. Policy Improvement: Greedily improve the policy based on the current value estimates.  4. Repeat: Iterate steps 2 and 3 until convergence. | Deriving an optimal policy by iteratively refining the value function. | Deterministic environments with known models. |
| Value Iteration | Iterative improvement of a value function to find the optimal value estimates. | 1. Initialisation: Set initial values for the state-value function or state-action value function.  <br>2. Iterative Update: Repeatedly update value estimates using the Bellman optimality equation.  <br>3. Convergence Check: Assess convergence of value estimates. | Finding optimal value estimates for states or state-action pairs. | Markov decision processes with known dynamics. |
| Q-Learning | Learning the optimal action-value function through exploration and exploitation. | 1. Initialisation: Set initial values for the state-action value function (Q-function).  2. Exploration-Exploitation: Choose actions based on an exploration-exploitation strategy.  3. Update Q-Values: Update Q-values based on observed rewards and transitions using the Bellman equation.  4. Repeat: Iterate steps 2 and 3 through multiple episodes or time steps. | Learning optimal Q-values to make decisions in an uncertain environment. | Model-free learning in environments with uncertainties. |
| Monte-Carlo | A model-free approach used to estimate V-functions by sampling episodes in the environment. <br>Averaging the returns observed from multiple simulated episodes. | 1.Interact with env. simulate episode(s) => run end-to-end<br>2.For each state, calculate average rewards, V-functions from available observations | Episodic environments with a terminal state.<br>Effective in scenarios with non-deterministic exploration. | Model-free<br>Off-policy<br>Stochastic<br>Needs a whole go through the episode |
| Temporal Difference (TD) | Estimate V-function based on temporal difference errors |  |  |  |


- **Off-policy:** agent can't pick actions
- **Policy-based**: we explicitly build a representation of a policy (mapping $\pi: s \to a$) and keep it in memory during learning.
	- In _policy-based methods_, we search directly for the optimal policy. We can optimize the parameter  $θ$  **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
	- In _policy-gradient methods_, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter $θ$ **directly** by performing the gradient ascent on the performance of the objective function $J(θ)$.
- **Value-based:** we don't store any explicit policy, only a value function. The policy is here implicit and can be derived directly from the value function (pick the action with the best value).
- **Model-based**: when we know the policy
	- can apply dynamic programming
	- can plan ahead
- **Model-free**: don't know the police, need to explore
	- no guarantee
	- can try stuff out
- **Trajectory**: a seq of (s, a, r)
- **Monte-Carlo**: used to estimate V-function by sampling episodes in the environment, then averaging the returns observed from multiple simulated episodes
	- steps:
		- sample episodes
		- for each state, calculate average rewards, V-functions
	- model free
	- off-policy
	- non-deterministic
	- needs a whole trajectory
- **TD** (temporal difference)
	- estimate V-function based on temporal difference errors
	- applicable in indefinite environment
	- calculates V(s) for each step (takes V(s+1) from knows Vs)
	- $V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)$
	- under alfa - temporal difference
	- Rt+1 + gamma - the goal
- **In between**: a combination of Monte-Carlo and TD with executing a few steps 
- **Q-learning**: a model-free reinforcement learning algorithm that enables an agent to learn a policy, represented by the action-value function Q, to make decisions in an environment. The primary idea behind Q-learning is to iteratively update the Q-values based on the observed rewards and the maximum expected future rewards.
	- off-policy: learns a policy that is different from used at a data collection phase
	- Q-values are updated iteratively as the agent learns from interactions
	- Used TD learning approach
	- Steps:
		- initialise Q(s,a) with 0s
		- choose exploration-exploitation (optional)
		- sample <s, a, r, s'>
		- compute new $\hat{Q}$ $(s,a) = r(s,a) + \gamma max_a Q(s',a)$
		- update $Q(s,a) = \alpha \hat{Q}(s,a) + (1 - \alpha)Q(s,a)$
	- $Q(S_t,A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma maxQ(S_{t+1},a) - Q(S_t,A_t)]$
	- использует действие с самой высокой ожидаемой ценностью
	- получаем оптимальные Q, V, $\pi$
- **SARSA**:
	- $Q(S_t,A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]$
	- использует выбранное действие/random action
	- target policy: the ideal policy the agent seeks to use to interact with its environment (t+1); behaviour policy - current policy. if they match, it's an on-policy algorithm, if they don't match - it's an off-policy algorithm
	- on-policy
	- SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm used for estimating the action-value function in a Markov Decision Process (MDP). The algorithm updates its Q-values based on the experiences it encounters while interacting with the environment.
	- Steps:
		- initialise Q-values $Q(s, a)$ for all states $s$ and actions $a$. This could be done randomly or set to some initial values.
		- initialise policy that the agent will follow. This policy can be epsilon-greedy, meaning it chooses the action with the highest Q-value with probability 1-epsilon and selects a random action with probability epsilon.
		- for each episode:
			- initialise the state s
			- choose the action using the policy
			- repeat until the episode terminates:
				- take action a
				- observe reward r and the next state s'
				- choose next action a' using the policy pi(s')
				- update Q-value using SARSA update rule:
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$$
				   - Update the policy $\pi$ based on the updated Q-values.
			- Repeat until convergence or a sufficient number of episodes
	SARSA ensures that the Q-values are updated based on the actions actually taken by the policy, making it an on-policy algorithm. The resulting policy is closely tied to the exploration strategy used during learning.
- **EV SARSA**:
	- ![[Pasted image 20240123234025.png]]
	- 
- **Model-free RL**: In model-free RL, the agent learns a policy or a value function directly from interacting with the environment.
- **Bellman equations**: describes the relationship between the value of a state or action and the expected future rewards
	- Bellman expectation equation:
		$V(s) = \sum_{a} \pi(a|s) \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V(s') \right] \right)$
		$Q(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q(s', a') \right]$
	- Bellman optimality equation: for the optimal policy $\pi^*$ and the corresponding optimal state-value function $V^*$, it is given by:
		$V^*(s) = \max_a \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^*(s') \right] \right)$
		$V^*(s) = \max_a Q^*(s, a)$
		$Q^*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} Q^*(s', a')]$
	- Optimal policy:
		$\pi^(s) = \arg \max_a \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^*(s') \right] \right)$
		- To extract the optimal policy from the Bellman Optimality Equation for \(V^*\), you would typically follow these steps:
			1. For each state $s$, evaluate the expression inside the argmax for all possible actions $a$.
			2. Select the action $a$ that maximizes the expression.
			3. Assign the selected action to the state $s$ in the optimal policy $\pi^*$.
			The process is repeated for each state, resulting in the complete optimal policy $\pi^*$
		- Optimal policy can also be directly derived from the Bellman Optimality Equation for the action-value function $(Q^*)$, where the optimal policy is given by:
			$\pi^*(s) = \arg \max_a Q^*(s, a)$
- **DQN**: deep Q network:
	- off-policy
	- extension of Q-Learning that uses deep neural networks to approximate the Q-function.
	- input: actions+states => Q-functions or states => actions+Q-functions
	- problems:
		- non i.i.d. data (states depend on each other)
		- stuck learning
		- moving target
	- It introduces experience replay
		- samples randomly from prev experience to solve a problem of non i.i.d data (s,a,r,s')
	- target networks to stabilise training and improve convergence
		- to freeze target for a while (a separate target network with delayed updates to stabilize learning)
	- parallelise agents to avoid stuck solutions (local minimums)
	- uses two networks for predicting Q and Q+1
	- update rule:
		$\text{Loss} = \mathbb{E}\left[ (r + \gamma \max_{a'} Q^{\text{target}}(s', a'; \theta^-) - Q(s, a; \theta))^2 \right]$
		where $Q^{\text{target}}$ is the target Q-network, $\theta$ are the current network parameters, and $\theta^-$ are the target network parameters.
	In the context of reinforcement learning, an "approximate method" refers to a class of algorithms that uses function approximation to represent and estimate certain components of the reinforcement learning problem, such as value functions or policies. These methods are designed to handle problems with large state or action spaces where it is impractical to maintain or compute exact values for each state-action pair.
- **Approximate methods**
	- **Approximate Value-Based Methods:**
		   - These methods aim to approximate the value function (either state-value function \(V\) or action-value function \(Q\)).
		   - Examples include Q-learning and Deep Q Networks (DQN).
		   - Function approximation, often using neural networks, is employed to generalize the value estimates across similar states or state-action pairs.
		   - The approximated value functions guide the decision-making process by selecting actions that lead to higher expected cumulative rewards.
	- **Approximate Policy-Based Methods:**
		   - These methods focus on approximating the policy directly, mapping states to actions.
		   - Examples include policy gradient methods like REINFORCE and actor-critic methods.
		   - Function approximation, typically using neural networks, is used to represent and update the policy parameters.
		   - The goal is to find a parameterized policy that maximizes the expected cumulative rewards.


