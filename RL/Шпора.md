...
**Terms**

| Term                             | Definition                                                                                                                                                                                             | Formula                                                                                                                                                                                                                                                                                                   |
|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Bellman Optimality Equation        | An equation in dynamic programming that expresses the optimal value of a state (or state-action pair) in terms of the expected immediate reward and the value of the next state (or state-action pair). | $V^*(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot V^*(s') \right)$ for state-value function.<br>$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot \max_{a'} Q^*(s', a')$ for state-action value function. |
| Convergence                       | The point at which an iterative algorithm stabilizes, and further iterations do not significantly change the results. In reinforcement learning, it often refers to the stability of value estimates. | Not applicable.                                                                                                                                                                                                                                                                                          |
| Deterministic Environment         | An environment where the outcome of an action is entirely predictable and does not involve randomness.                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Exploration-Exploitation Tradeoff | The balance between exploring unknown options (exploration) and exploiting known options (exploitation) to maximize cumulative rewards in reinforcement learning.                             | Not applicable.                                                                                                                                                                                                                                                                                          |
| Markov Decision Process (MDP)      | A mathematical model used to describe decision-making in situations where outcomes are partially random and partially under the control of a decision-maker.                                        | $P(y \| x_1, x_2, ..., x_n) = P(y \| x_n)$<br> |
| Model-Free Learning               | Learning approaches in reinforcement learning that do not rely on a known model of the environment. Instead, they learn directly from interactions with the environment.                          | Not applicable.                                                                                                                                                                                                                                                                                          |
| Policy                           | A strategy or a set of rules that defines an agent's behavior in a given environment. In reinforcement learning, policies guide decision-making.                                                       | Not applicable.                                                                                                                                                                                                                                                                                          |
| Solution Space                    | The set of all possible solutions to a given problem.                                                                                        | Not applicable.                                                                                                                                                                                                                                                                                          |
| State-Action Value Function (Q)    | Represents the expected cumulative reward from a given state and action under a certain policy. It assesses the value of taking a specific action in a specific state.                           | $Q(s, a) = E \left[ R_t + \gamma Q(s', a') \mid s, a \right]$where $R_t$ is the immediate reward, $s'$ is the next state, and $a'$ is the next action.                                                                                                                                        |
| State-Value Function               | In reinforcement learning, it represents the expected cumulative reward from a given state under a certain policy. It evaluates how good it is for the agent to be in a specific state.             | $V(s) = E \left[ R_t + \gamma V(s') \mid s \right]$ where $R_t$ is the immediate reward, $s'$ is the next state, and $\gamma$ is the discount factor.                                                                                                                                      |
| Stochasticity                     | The presence of randomness or uncertainty in an environment or a system. In reinforcement learning, it may refer to uncertain outcomes of actions or transitions between states.                  | Not applicable.                                                                                                                                                                                                                                                                                          |



**Methods**


| Method Name | Key Concept | Steps | Application in RL | When Used |
| ---- | ---- | ---- | ---- | ---- |
| Cross-Entropy Method (CEM) | Iterative improvement of a probability distribution to optimise solutions. | 1. Initialisation: Set up an initial parameterised distribution over the solution space.  <br>2. Generate Samples: Randomly sample solutions (e.g., policies) from the distribution.  <br>3. Evaluate Solutions: Assess the performance of each sampled solution using an objective function.  <br>4. Select Top Solutions: Identify the top-performing solutions based on evaluation scores.  <br>5. Update Distribution: Adjust distribution parameters to favour solutions similar to the top-performing ones.  <br>6. Repeat: Iteratively execute steps 2 to 5 for a fixed number of iterations or until convergence criteria are met.  <br>7. Convergence Check: Assess solution quality or distribution stability; decide whether to continue iterations or stop. | Policy optimisation in reinforcement learning. | Exploration in complex environments. <br>Approximate method is used when the environment is too large to be enumerated. |
| Value-Based Policy Iteration | Iterative improvement of a value function to derive an optimal policy. | 1. Initialisation: Set initial values for the state-value function or state-action value function.  <br>2. Policy Evaluation: Iteratively update value estimates based on the current policy.  <br>3. Policy Improvement: Greedily improve the policy based on the current value estimates.  4. Repeat: Iterate steps 2 and 3 until convergence. | Deriving an optimal policy by iteratively refining the value function. | Deterministic environments with known models. |
| Value Iteration | Iterative improvement of a value function to find the optimal value estimates. | 1. Initialisation: Set initial values for the state-value function or state-action value function.  <br>2. Iterative Update: Repeatedly update value estimates using the Bellman optimality equation.  <br>3. Convergence Check: Assess convergence of value estimates. | Finding optimal value estimates for states or state-action pairs. | Markov decision processes with known dynamics. |
| Q-Learning | Learning the optimal action-value function through exploration and exploitation. | 1. Initialisation: Set initial values for the state-action value function (Q-function).  2. Exploration-Exploitation: Choose actions based on an exploration-exploitation strategy.  3. Update Q-Values: Update Q-values based on observed rewards and transitions using the Bellman equation.  4. Repeat: Iterate steps 2 and 3 through multiple episodes or time steps. | Learning optimal Q-values to make decisions in an uncertain environment. | Model-free learning in environments with uncertainties. |
| Monte-Carlo | A model-free approach used to estimate V-functions by sampling episodes in the environment. <br>Averaging the returns observed from multiple simulated episodes. | 1.Interact with env. simulate episode(s) => run end-to-end<br>2.For each state, calculate average rewards, V-functions from available observations | Episodic environments with a terminal state.<br>Effective in scenarios with non-deterministic exploration. | Model-free<br>Off-policy<br>Stochastic<br>Needs a whole go through the episode |
| Temporal Difference (TD) | Estimate V-function based on temporal difference errors |  |  |  |


- **Off-policy:** agent can't pick actions
- **Policy-based**: we explicitly build a representation of a policy (mapping $\pi: s \to a$) and keep it in memory during learning.
	- solve easier problems
	- innate exploration and stochasticity
	- continuous action space
	- learns from full sessions only??
	- In _policy-based methods_, we search directly for the optimal policy. We can optimize the parameter  $θ$  **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
	- In _policy-gradient methods_, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter $θ$ **directly** by performing the gradient ascent on the performance of the objective function $J(θ)$.
- **Value-based:** we don't store any explicit policy, only a value function. The policy is here implicit and can be derived directly from the value function (pick the action with the best value).
	- solves harder problems
	- artificial exploration
	- learns from partial experience (TD)
	- evaluates strategy for free
- **Model-based**: when we know the policy
	- can apply dynamic programming
	- can plan ahead
- **Model-free**: don't know the police, need to explore
	- no guarantee
	- can try stuff out
- **Trajectory**: a seq of (s, a, r)
- **Monte-Carlo**: used to estimate V-function by sampling episodes in the environment, then averaging the returns observed from multiple simulated episodes
	- б дисперсия, м смещение
	- steps:
		- sample episodes
		- for each state, calculate average rewards, V-functions
	- model free
	- off-policy
	- non-deterministic
	- needs a whole trajectory
- **TD** (temporal difference)
	- м дисперсия, б смещение 
	- estimate V-function based on temporal difference errors
	- applicable in indefinite environment
	- calculates V(s) for each step (takes V(s+1) from knows Vs)
	- $V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)$
	- under alfa - temporal difference
	- Rt+1 + gamma - the goal
- **In between**: a combination of Monte-Carlo and TD with executing a few steps 
- **Q-learning**: a model-free reinforcement learning algorithm that enables an agent to learn a policy, represented by the action-value function Q, to make decisions in an environment. The primary idea behind Q-learning is to iteratively update the Q-values based on the observed rewards and the maximum expected future rewards.
	- off-policy: learns a policy that is different from used at a data collection phase
	- Q-values are updated iteratively as the agent learns from interactions
	- Used TD learning approach
	- Steps:
		- initialise Q(s,a) with 0s
		- choose exploration-exploitation (optional)
		- sample <s, a, r, s'>
		- compute new $\hat{Q}$ $(s,a) = r(s,a) + \gamma max_a Q(s',a)$
		- update $Q(s,a) = \alpha \hat{Q}(s,a) + (1 - \alpha)Q(s,a)$
	- $Q(S_t,A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma maxQ(S_{t+1},a) - Q(S_t,A_t)]$
	- использует действие с самой высокой ожидаемой ценностью
	- получаем оптимальные Q, V, $\pi$
- **SARSA**:
	- $Q(S_t,A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]$
	- использует выбранное действие/random action
	- target policy: the ideal policy the agent seeks to use to interact with its environment (t+1); behaviour policy - current policy. if they match, it's an on-policy algorithm, if they don't match - it's an off-policy algorithm
	- on-policy
	- SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm used for estimating the action-value function in a Markov Decision Process (MDP). The algorithm updates its Q-values based on the experiences it encounters while interacting with the environment.
	- Steps:
		- initialise Q-values $Q(s, a)$ for all states $s$ and actions $a$. This could be done randomly or set to some initial values.
		- initialise policy that the agent will follow. This policy can be epsilon-greedy, meaning it chooses the action with the highest Q-value with probability 1-epsilon and selects a random action with probability epsilon.
		- for each episode:
			- initialise the state s
			- choose the action using the policy
			- repeat until the episode terminates:
				- take action a
				- observe reward r and the next state s'
				- choose next action a' using the policy pi(s')
				- update Q-value using SARSA update rule:
$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$$
				   - Update the policy $\pi$ based on the updated Q-values.
			- Repeat until convergence or a sufficient number of episodes
	SARSA ensures that the Q-values are updated based on the actions actually taken by the policy, making it an on-policy algorithm. The resulting policy is closely tied to the exploration strategy used during learning.
- **EV SARSA**:
	- ![[Pasted image 20240123234025.png]]
	- 
- **Model-free RL**: In model-free RL, the agent learns a policy or a value function directly from interacting with the environment.
- **Bellman equations**: describes the relationship between the value of a state or action and the expected future rewards
	- Bellman expectation equation:
		$V(s) = \sum_{a} \pi(a|s) \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V(s') \right] \right)$
		$Q(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q(s', a') \right]$
	- Bellman optimality equation: for the optimal policy $\pi^*$ and the corresponding optimal state-value function $V^*$, it is given by:
		$V^*(s) = \max_a \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^*(s') \right] \right)$
		$V^*(s) = \max_a Q^*(s, a)$
		$Q^*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} Q^*(s', a')]$
	- Optimal policy:
		$\pi^(s) = \arg \max_a \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^*(s') \right] \right)$
		- To extract the optimal policy from the Bellman Optimality Equation for \(V^*\), you would typically follow these steps:
			1. For each state $s$, evaluate the expression inside the argmax for all possible actions $a$.
			2. Select the action $a$ that maximizes the expression.
			3. Assign the selected action to the state $s$ in the optimal policy $\pi^*$.
			The process is repeated for each state, resulting in the complete optimal policy $\pi^*$
		- Optimal policy can also be directly derived from the Bellman Optimality Equation for the action-value function $(Q^*)$, where the optimal policy is given by:
			$\pi^*(s) = \arg \max_a Q^*(s, a)$
- **DQN**: deep Q network:
	- off-policy, value-based
	- inp: state; output: q-func
	- extension of Q-Learning that uses deep neural networks to approximate the Q-function.
	- input: actions+states => Q-functions or states => actions+Q-functions
	- problems:
		- non i.i.d. data (states depend on each other)
		- stuck learning
		- moving target
	- It introduces experience replay
		- samples randomly from prev experience to solve a problem of non i.i.d data (s,a,r,s')
	- target networks to stabilise training and improve convergence
		- to freeze target for a while (a separate target network with delayed updates to stabilize learning)
	- parallelise agents to avoid stuck solutions (local minimums)
	- uses two networks for predicting Q and Q+1
	- update rule:
		$\text{Loss} = \mathbb{E}\left[ (r + \gamma \max_{a'} Q^{\text{target}}(s', a'; \theta^-) - Q(s, a; \theta))^2 \right]$
		where $Q^{\text{target}}$ is the target Q-network, $\theta$ are the current network parameters, and $\theta^-$ are the target network parameters.
- **Approximate methods**
		In the context of reinforcement learning, an "approximate method" refers to a class of algorithms that uses function approximation to represent and estimate certain components of the reinforcement learning problem, such as value functions or policies. These methods are designed to handle problems with large state or action spaces where it is impractical to maintain or compute exact values for each state-action pair.
	- **Approximate Value-Based Methods:**
		   - These methods aim to approximate the value function (either state-value function \(V\) or action-value function \(Q\)).
		   - Examples include Q-learning and Deep Q Networks (DQN).
		   - Function approximation, often using neural networks, is employed to generalize the value estimates across similar states or state-action pairs.
		   - The approximated value functions guide the decision-making process by selecting actions that lead to higher expected cumulative rewards.
	- **Approximate Policy-Based Methods:**
		   - These methods focus on approximating the policy directly, mapping states to actions.
		   - Examples include policy gradient methods like REINFORCE and actor-critic methods.
		   - Function approximation, typically using neural networks, is used to represent and update the policy parameters.
		   - The goal is to find a parameterized policy that maximizes the expected cumulative rewards.
- **Policy gradient**
	- Objective function
		- $$J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \,|\, \pi_\theta \right]$$
		- Expected cumulative reward
	- Optimisation
		- Gradient ascent, to iteratively update the policy parameters in the direction that increases the expected cumulative rewards.
			$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$
			$\alpha$ is the learning rate, and $\nabla_\theta J(\theta)$ is the gradient of the objective function with respect to the policy parameters.
	- представляет политику как непрерывную обучаемую функцию
	- to directly optimise the policy of an agent to maximise expected cumulative rewards
	- policy defines the probability distribution over actions given a certain state
	- the primary goal is to find a policy that maximises the expected reward over time
	- policies are often parametrised by weights or parameters
	- policy gradient methods use a gradient ascent approach
	- the gradient of the expected cumulative reward with respect to the policy parameters indicates how the policy should be adjusted.
	- learning from experience:
		- **Experience:** The agent interacts with the environment, takes actions according to its current policy, receives rewards, and learns from this experience.
		- **Update:** After each interaction, the policy parameters are updated to increase the probability of actions that resulted in higher rewards.
- **Actor-critic algorithm** on policy
	- Steps
		- actor update
			- 1. **Policy (Actor):**
			   - Define a policy $\pi_\theta(a|s)$ representing the probability of taking action $a$ in state $s$ with parameters $\theta$.
			- 2. **Objective Function:**
			   - Define the objective function $J(\theta)$ as the expected cumulative reward: ==$J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \,|\, \pi_\theta \right]$
			- 3. **Policy Gradient:**
			   - Compute the gradient of the objective function with respect to the policy parameters $\theta$: $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot Q_w(s_t, a_t) \right]$
			- 4. **Update Rule:**
			   - Update the policy parameters using the policy gradient: $\theta \leftarrow \theta + \alpha_\theta \nabla_\theta J(\theta)$, where $\alpha_\theta$ is the learning rate for the actor.
		- critic update
			- 1. **Q Function (Critic):**
				- Define a Q function $Q_w(s, a)$ representing the estimated expected cumulative reward for taking action $a$ in state $s$, with parameters $w$.
			- 2. **Temporal Difference (TD) Error:**
			   - Compute the TD error for each time step: $\delta_t = r_t + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)$
			- 3. **Update Rule:**
			   - Update the critic's parameters using the TD error: $w \leftarrow w + \alpha_w \delta_t \nabla_w Q_w(s_t, a_t)$, where $\alpha_w$ is the learning rate for the critic.
		- iterative learning
			- 1. **Interaction:**
				   - Interact with the environment to collect experiences.
			- 2. **Actor-Critic Interaction:**
				   - Use the critic's estimates to guide the actor's updates.
			- 3. **Repeat:**
				   - Repeat the process for multiple iterations or until convergence.
		   - data
			   - actor: input: state; output: action
			   - critic: input: state, action; output: Q,V
		These steps outline the Actor-Critic algorithm, where the actor learns a policy to maximise expected cumulative rewards, and the critic estimates the value function to guide the actor's learning process.
- **Soft Actor-Critic**
	- off-policy (from replay buffer), model-free
	- sample efficiency, stability, continuous action spaces
	- suited for high-dimensional state and action spaces
	- entropy-regularised reinforcement learning. add entropy(measure of randomness) to the reward function => policy more diverse and explorative
	- max cum rew = rewards + entropy
	- q function and value network
	- actor-critic architecture
	- temperature: trade-off between max reward and entropy
	- training steps:
		- interact w env
		- experience replay
		- policy and value updates
		- entropy regularisation
- **deviation soft actor critic**
	- extension of soft actor critic
	- reference policy (i.e. from demo sessions, domain knowledge)
	- deviation regularisation: penalty for deviations from reference policy
	- entropy to balance exploration-exploitation
- **A3C (Asynchronous Advantage Actor-Critic)**
	- A-C architecture
	- asynch training
	- global and local models: local models copy global and then sync
	- advantage function: A(s,a)=Q(s,a)−V(s), how good is action relative to current policy , need positive
	- policy and value updates asynch
	- entropy regularisation
	- steps:
		- asynch. interaction w env
		- local model updates
		- synch w global model
		- advantage calculation ...
		- The policy (actor) and value function (critic) are updated using gradient descent methods, incorporating the advantages and entropy regularisation
	- benefits:
		- parallelisation
		- real-time appl.
- **GAE** (generalised advantage estimation)
	- technique
	- parameter lambda to control the trade-off between bias and variance in estimating the advantage (to balance short and long term rewards)
	- $A(s, a) = \delta_t + (\gamma \lambda) \delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \ldots + (\gamma \lambda)^{T-t-1} \delta_{T-1}$
		- $\delta_t$ is the temporal difference error at time $t$, representing the difference between the estimated value and the actual return at time $t$: $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$.
	- A higher $\lambda$ places more emphasis on longer-term rewards (Monte-Carlo), and a lower $\lambda$ prioritises short-term advantages (TD).
	- This is particularly useful in situations where estimating precise advantages is challenging due to the complexity of the environment or the presence of sparse rewards.
- **BANDIT**
	- multi-armed: refers to a scenario where you have multiple options or choices, often depicted as "arms" (like the levers on a slot machine).
	- bandit: represents the uncertainty and risk associated with choosing different options, akin to pulling the lever of a slot machine.
	- pull arm => reward => (each level has unknown probability distribution of rewards) => limited number of trials => which levers to pull => maximise total reward
	- exploration-exploitation trade-off
	- The multi-armed bandit problem is a fundamental concept that lays the groundwork for understanding decision-making strategies in situations with limited information and a need to optimise cumulative rewards over time.
	- ![[Pasted image 20240124204358.png]]
- **REINFORCE**
	- Steps
		- **Initialization:** Start with some initial values for the parameters of the policy function. This could be a neural network if the policy is parameterized by one.
		- **Collect samples**: For each episode, interact with the environment by following the current policy. Collect the sequence of states, actions, and rewards. Сэмплит и делает градиент, по каждой траектории оптимизирует модель
		- **Compute returns**: For each time step in an episode, compute the return, which is the sum of future rewards from that time step onward. $G_t = \sum_{k=t}^{T} \gamma^{k-t} \cdot R_k$
		- **Compute policy gradient** with respect to the policy parameters. This involves calculating the gradient of the log probability of the chosen actions multiplied by the returns. $\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=1}^{T} \nabla_{\theta} \log(\pi_{\theta}(a_t|s_t)) \cdot G_t \right]$
		- **Update policy parameters**: in the direction of the policy gradient to increase the likelihood of actions that lead to higher returns. $\theta_{\text{new}} = \theta_{\text{old}} + \alpha \cdot \nabla_{\theta} J(\theta)$
		- Repeat
	- Key points
		- Стохастическая политика
		- **Exploration:** REINFORCE inherently encourages exploration by assigning probabilities to different actions.
		- **Variance:** REINFORCE can have high variance, leading to slow convergence.
		- **Neural Network Policies:** REINFORCE is often applied when the policy is parameterized by a neural network, making it suitable for problems with complex action spaces.
- **Reinforce w baseline**
	- extension of the basic REINFORCE algorithm that incorporates a baseline term in the policy gradient updates. The baseline is subtracted from the returns to reduce the variance of the gradient estimates, leading to potentially faster and more stable learning. The baseline can be a value estimate that represents the expected return in a particular state or a constant term.
	- $\nabla_\theta J(\theta) = \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (G_t - b)$
	- benefits:
		- variance reduction (with a good choice of baseline)
			- Q-func, average return, const
		- improved sample efficiency
	- steps:
		- collect trajectories
		- compute returns G
		- compute baseline
		- update policy parameters (via policy gradient)
		- repeat
- **DDPG (Deep Deterministic Policy Gradient)**
	- designed for problems where actions need to be deterministic
	- well-suited for environments with continuous action spaces
	- approximates policy
	- inp: state; outp: action
	- Actor-Critic architecture. The "Actor" learns and updates the deterministic policy, while the "Critic" estimates the Q function.
	- experience replay => helps in breaking correlations between consecutive experiences and improves sample efficiency.
	- target networks: uses two sets of networks for both the Actor and the Critic: the "target" networks and the "online" networks. Target networks slowly track the parameters of the online networks.
	- The policy (Actor) is updated using the gradient of the expected return with respect to the policy parameters.
	- гауссов шум на входе повышает exploration
- **TD3 (twin delayed DDPG)**
	- enhancement over DDPG to improve stability during training
	- uses two Critic networks to estimate the value of state-action pairs. This helps reduce overestimation biases in the value estimates, leading to more accurate evaluations.
	- Double Q-Learning with clipping. This mitigates the problem of overestimating Q-values, enhancing the accuracy of value estimates.
	- delayed policy update
	- "target policy smoothing," involving adding noise to the target actions during the value estimation process. This helps prevent the policy from becoming too deterministic.
- **Expl-expl**
	- чистая экспл: argmax Q
	- чистое иссл: random action
	- e-geedy: higher e => higher exploration
		- different stategies, i.e. затухающий
	- Q-func: optimistic initialisation
		- start with high Q
		- encourages early exploration
		- $Q(s, a) \leftarrow Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$
	- softmax strategy
		- maps Q-func to probabilities
		- naturally balances expl-expl
		- temperature: The softmax function includes a temperature parameter (τ) that controls the level of exploration. A higher temperature increases the spread of probabilities, making the action selection more exploratory, while a lower temperature leads to more deterministic selections based on the preferences
		- $P(a_i) = \frac{e^{\frac{z_i}{\tau}}}{\sum_{j=1}^{N} e^{\frac{z_j}{\tau}}}$
	- UCB (upper confidence bound)
		- select actions based on their estimated values while incorporating an exploration bonus that depends on the uncertainty or confidence in those estimates.
		- $UCB(a) = \text{Estimated Value}(a) + c \times \sqrt{\frac{\log(t)}{N(a)}}$
		- key points
			- early exploration
			- late exploitation
			- parameter c controls the trade-off between exploration and exploitation. A higher c promotes more exploration.
	- Thompson sampling
		1.**Initialization:** Set up prior distributions for each arm.
		2.**Sampling:** Sample values for each arm.
		3.**Action Selection:** Select the arm with the highest expected reward according to the sampled values.
		4.**Observation:** Observe the actual reward obtained by pulling the selected arm.
		5.**Update Beliefs:** Update the posterior distributions for each arm based on the observed reward.
		6.**Repeat:**
- Planning: Monte Carlo tree search
	- algorithm used in decision-making processes, particularly in game playing and other domains with sequential decision problems. MCTS is known for its ability to handle environments with large state spaces and uncertainty. The algorithm incrementally builds a search tree by repeatedly sampling trajectories through the state space and updating the tree based on the observed outcomes.
	- Steps
		- ![[Pasted image 20240126073617.png]]
		- ![[Pasted image 20240126073822.png]]
		- Metrics & node values
			- Visit count
			- Accumulated reward
			- UCB
			- C - exploration constant
- **Metrics for exploration**
	- 1. **Visit Count (N):**
		   - High visit counts => the agent has explored a particular state or taken a specific action frequently.
	- 2. **Exploration Rate:**
		   - $\text{Exploration Rate} = \frac{\text{Number of Exploratory Actions}}{\text{Total Number of Actions}}$
	- 3. **Entropy of Action Selection:**
		   - The entropy measures the uncertainty or randomness in the agent's action selection. High entropy indicates more exploratory behavior.
			   $\text{Entropy} = - \sum_{i} P(a_i) \log P(a_i)$
		   - $P(a_i)$ is the probability of selecting action $a_i$.
	- 4. **Information Gain:**
		   - $\text{Information Gain} = \text{Entropy Before Action} - \text{Entropy After Action}$
	- 5. **Variance in Value Estimates:**
		   - The variance in the estimates of the value function for different states or actions. Higher variance suggests more uncertainty and, therefore, more exploration.
	- 6. **Average Reward in Unexplored Regions:**
		   - The average reward obtained in states or regions that have been visited infrequently. Agents may be incentivised to explore areas with lower visit counts to gather more information.
	- 8. **Diversity of Trajectories:**
		   - Measures the diversity of trajectories taken by the agent. Higher diversity indicates more exploration in different parts of the state space.
	- 9. **Normalised Advantage:**
		   - In the context of policy-based methods, the advantage function can be normalised to encourage more exploration.
			   $\text{Normalized Advantage} = \frac{A(s, a)}{\sigma(A(s, a))}$
		   - $A(s, a)$ is the advantage function for state-action pair $(s, a)$, and $\sigma$ is the standard deviation.
- RecSys
	- e-greedy
		- steps
			- 

    












