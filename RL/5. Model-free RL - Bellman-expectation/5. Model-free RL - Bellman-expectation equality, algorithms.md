==вопросы==
 - для состояния: одно действие - один реворд или одно действие - много ревордов?
 - 
![[Pasted image 20240121134456.png]]

We already know them from Value-based RL

![[Pasted image 20240121134805.png]]

Для поиска значений используется BackUp Tree

![[Pasted image 20240121143547.png]]

![[Pasted image 20240121143612.png]]

![[Pasted image 20240121143631.png]]

то есть уравнения Bellman позволяют посчитать эти функции и через эти функции мы можем оценить качество нашей политики.

Но нужно понять, как искать оптимальную политику и что такое вообще оптимальная политика.

![[Pasted image 20240121143916.png]]

![[Pasted image 20240121143931.png]]
![[Pasted image 20240122051639.png]]
![[Pasted image 20240121143948.png]]

Отметим: 
1) v* и q* уже от политики не зависят.
2) Может быть болше, чем 1 оптимальная политика.
3) В любой конечномерной MDP (множество состояний конечно) есть как минимум одна детерменированная оптимальная политика.

![[Pasted image 20240121144100.png]]

![[Pasted image 20240121144132.png]]

Теперь, зная это все, давайте посмотрим, как можно улучшать агента и его политику.

![[Pasted image 20240121153145.png]]

![[Pasted image 20240121153200.png]]

![[Pasted image 20240121153214.png]]

![[Pasted image 20240121153228.png]]

![[Pasted image 20240121153240.png]]

![[Pasted image 20240121153325.png]]

![[Pasted image 20240121154124.png]]

![[Pasted image 20240121154616.png]]

![[Pasted image 20240121154700.png]]

![[Pasted image 20240121154742.png]]

Об этих алгоритмах смотрите подробнее в Policy iteration (2 часть 2 билета) и Value Iteration (3 билет).

Но, вкратце, отличаются эти политики только тем, насколько долго мы оцениваем качество политики перед тем, как ее улучшить.

==чатик==
Model-free reinforcement learning (RL) is a class of RL algorithms that do not rely on an explicit model of the environment. In model-free RL, the agent learns a policy or a value function directly from interacting with the environment. One key concept in model-free RL is the Bellman equation, which describes the relationship between the value of a state or action and the expected future rewards.

The Bellman equation for the state-value function (V) is given by:

$$V(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]$$

where:
- s is the current state,
- $\pi(a|s)$ is the policy, representing the probability of taking action $a$ in state $s$,
- $p(s', r | s, a)$ is the transition dynamics, representing the probability of transitioning to state $s'$ and receiving reward $r$ given the current state $s$ and action $a$,
- $\gamma$ is the discount factor,
- $V(s')$ is the value of the next state.

Similarly, the Bellman equation for the action-value function (Q) is given by:

$$Q(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} Q(s', a')]$$

where:
- $Q(s, a)$ is the value of taking action $a$ in state $s$,
- $\max_{a'} Q(s', a')$ represents the maximum value over all possible actions in the next state.

The Bellman-expectation equality is a fundamental property that connects the value functions to the optimal policy. For the optimal policy $\pi^*$ and the corresponding optimal state-value function $V^*$, it is given by:

$V^*(s) = \max_a Q^*(s, a)$

$Q^*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} Q^*(s', a')]$


Certainly! Let's clarify the Bellman Expectation Equation and the Bellman Optimality Equation.

### Bellman Expectation Equation:

The Bellman Expectation Equation describes the expected value of a state or state-action pair under a given policy. For the state-value function (V) and the action-value function (Q), the Bellman Expectation Equation is as follows:

1. **State-Value Function (V):**
   
   $$V(s) = \sum_{a} \pi(a|s) \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V(s') \right] \right)$$

   Here:
   - $V(s)$ is the expected cumulative future rewards (value) from state $s$,
   - $\pi(a|s)$ is the probability of taking action $a$ in state $s$,
   - $p(s', r | s, a)$ is the transition probability to reach state $s'$ and receive reward $r$ when taking action $a$ in state $s$,
   - $\gamma$ is the discount factor,
   - $V(s')$ is the expected cumulative future rewards from the next state $s'$.

2. **Action-Value Function (Q):**
   
   $$Q(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q(s', a') \right]$$

   Here:
   - $Q(s, a)$ is the expected cumulative future rewards (action-value) from taking action  $a$ in state $s$,
   - $\pi(a'|s')$ is the probability of taking action $a'$ in the next state $s'$,
   - $Q(s', a')$ is the expected cumulative future rewards from the next state $s'$ and taking action $a'$.

### Bellman Optimality Equation:

The Bellman Optimality Equation expresses the relationship between the value of a state (or state-action pair) and the optimal strategy. For the optimal state-value function (\(V^*\)) and the optimal action-value function (\(Q^*\)), the Bellman Optimality Equation is given by:

1. **State-Value Function (Optimal, \(V^*\)):**

   $$V^*(s) = \max_a \left( \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^*(s') \right] \right)$$

   Here:
   - $V^*(s)$ is the maximum expected cumulative future rewards (optimal value) from state $s$,
   - The maximization is over all possible actions $a$.

2. **Action-Value Function (Optimal, $Q^*$):**

   $$Q^*(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \max_{a'} Q^*(s', a') \right]$$

   Here:
   - $Q^*(s, a)$ is the maximum expected cumulative future rewards (optimal action-value) from taking action $a$ in state $s$,
   - The maximization is over all possible actions $a'$.

These equations play a crucial role in formulating and understanding the optimal policy and value functions in reinforcement learning. Algorithms like Q-learning and DQN often use these equations to guide their updates and converge towards an optimal solution.