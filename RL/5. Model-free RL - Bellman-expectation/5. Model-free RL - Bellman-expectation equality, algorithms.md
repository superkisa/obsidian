![[Pasted image 20240121134456.png]]

We already know them from Value-based RL

![[Pasted image 20240121134805.png]]

Для поиска значений используется BackUp Tree

![[Pasted image 20240121143547.png]]

![[Pasted image 20240121143612.png]]

![[Pasted image 20240121143631.png]]

то есть уравнения Bellman позволяют посчитать эти функции и через эти функции мы можем оценить качество нашей политики.

Но нужно понять, как искать оптимальную политику и что такое вообще оптимальная политика.

![[Pasted image 20240121143916.png]]

![[Pasted image 20240121143931.png]]

![[Pasted image 20240121143948.png]]

Отметим: 
1) v* и q* уже от политики не зависят.
2) Может быть болше, чем 1 оптимальная политика.
3) В любой конечномерной MDP (множество состояний конечно) есть как минимум одна детерменированная оптимальная политика.

![[Pasted image 20240121144100.png]]

![[Pasted image 20240121144132.png]]

Теперь, зная это все, давайте посмотрим, как можно улучшать агента и его политику.

![[Pasted image 20240121153145.png]]

![[Pasted image 20240121153200.png]]

![[Pasted image 20240121153214.png]]

![[Pasted image 20240121153228.png]]

![[Pasted image 20240121153240.png]]

![[Pasted image 20240121153325.png]]

![[Pasted image 20240121154124.png]]

