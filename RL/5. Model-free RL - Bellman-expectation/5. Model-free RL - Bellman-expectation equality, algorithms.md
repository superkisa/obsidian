![[Pasted image 20240121134456.png]]

We already know them from Value-based RL

![[Pasted image 20240121134805.png]]

Для поиска значений используется BackUp Tree

![[Pasted image 20240121143547.png]]

![[Pasted image 20240121143612.png]]

![[Pasted image 20240121143631.png]]

то есть уравнения Bellman позволяют посчитать эти функции и через эти функции мы можем оценить качество нашей политики.

Но нужно понять, как искать оптимальную политику и что такое вообще оптимальная политика.

![[Pasted image 20240121143916.png]]

![[Pasted image 20240121143931.png]]

![[Pasted image 20240121143948.png]]

Отметим: 
1) v* и q* уже от политики не зависят.
2) Может быть болше, чем 1 оптимальная политика.
3) В любой конечномерной MDP (множество состояний конечно) есть как минимум одна детерменированная оптимальная политика.

![[Pasted image 20240121144100.png]]

![[Pasted image 20240121144132.png]]


