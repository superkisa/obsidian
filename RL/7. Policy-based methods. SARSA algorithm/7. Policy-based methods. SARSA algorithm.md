Лиза в процессе
![[Pasted image 20240122231004.png]]
![[Pasted image 20240122230457.png]]
Классический способ обучения - агент обучается на своих же действиях.
Второй случай - агент обучается, но обучается не всегда на своих действиях (пример: автомобиль с автопилотом, но при этом присутствует человек, который некоторое время управляет автомобилем для собирания данных).
![[Pasted image 20240122233835.png]]
Policy-based methods in Reinforcement Learning (RL) focus on learning the optimal policy directly. Unlike value-based methods, which learn the value of being in a particular state, policy-based methods learn the action to take given a state.

In policy-based methods, the policy is defined as a mapping from states to actions. The goal is to find a policy that maximizes the expected cumulative reward. This policy can be deterministic (always choosing the same action given a state) or stochastic (choosing actions according to a certain probability distribution).
The policy can be parameterized using a function, often represented as π(θ). This function maps states to actions with certain probabilities. For instance, in a continuous action space, a Gaussian policy could be used, which assigns higher probabilities to actions close to the current action.

One common challenge with policy-based methods is that they often converge to a local optimum rather than the global optimum, and evaluating a policy can be computationally expensive.

## The difference between policy-based and policy-gradient methods

Policy-gradient methods, what we’re going to study in this unit, is a subclass of policy-based methods. In policy-based methods, the optimization is most of the time _on-policy_ since for each update, we only use data (trajectories) collected **by our most recent version of $π_θ$​.

The difference between these two methods **lies on how we optimize the parameter $θ$:

- In _policy-based methods_, we search directly for the optimal policy. We can optimize the parameter  $θ$  **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
- In _policy-gradient methods_, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter $θ$ **directly** by performing the gradient ascent on the performance of the objective function $J(θ)$.

Before diving more into how policy-gradient methods work (the objective function, policy gradient theorem, gradient ascent, etc.), let’s study the advantages and disadvantages of policy-based methods.