Вопрос не дописан
![[Pasted image 20240123001201.png]]

![[Pasted image 20240122231004.png]]
![[Pasted image 20240122230457.png]]
Классический способ обучения - агент обучается на своих же действиях.
Второй случай - агент обучается, но обучается не всегда на своих действиях (пример: автомобиль с автопилотом, но при этом присутствует человек, который некоторое время управляет автомобилем для собирания данных).
![[Pasted image 20240122233835.png]]
### On-policy vs. Off-Policy

This division is based on whether you update your $Q$ values based on actions undertaken according to your current policy or not. Let's say your current policy is a completely _random policy_. You're in state $s$ and make an action $a$ that leads you to state $s'$. Will you update your $Q(s, a)$ based on the best possible action you can take in $s'$ or based on an action according to your current policy (random action)? The first choice method is called **off-policy** and the latter - **on-policy**. E.g. _Q-learning_ does the first and _SARSA_ does the latter.

### Policy-based vs. Value-based

In **Policy-based** methods we explicitly build a representation of a policy (mapping $\pi: s \to a$) and keep it in memory during learning.

In **Value-based** we don't store any explicit policy, only a value function. The policy is here implicit and can be derived directly from the value function (pick the action with the best value).

**Actor-critic** is a mix of the two.

### Model-based vs. Model-free

The problem we're often dealing with in RL is that whenever you are in state $s$ and make an action $a$ you might not necessarily know the next state $s'$ that you'll end up in (the environment influences the agent).

In **Model-based** approach you either have an access to the model (environment) so you know the probability distribution over states that you end up in, or you first try to build a model (often - approximation) yourself. This might be useful because it allows you to do _planning_ (you can "think" about making moves ahead without actually performing any actions).

In **Model-free** you're not given a model and you're not trying to explicitly figure out how it works. You just collect some experience and then derive (hopefully) optimal policy.

## **How is SARSA different from the Q-learning algorithm?**

|   |   |
|---|---|
|SARSA|Q-Learning|
|In the SARSA algorithm, the agent uses the On-policy for learning where the agent learns from the current set of actions in the current state and the target policy or the action to be performed.|In the Q-learning algorithm, the agent uses the off-policy learning technique where the agent learns the actions to be performed from the previous states and the awards received from the previous set of actions.|
|The learning of the agent is improved by using the current set of actions performed in the current state|The learning of the agent is improved by performing a greedy search where only the maximum reward received for the particular set of actions in that particular state is considered.|
|Previous states and previous rewards are not considered for newer states of operation|Previous states and previous rewards are considered for newer states of operations.|

## **How to Use SARSA Practically?**

Policy-based methods in Reinforcement Learning (RL) focus on learning the optimal policy directly. Unlike value-based methods, which learn the value of being in a particular state, policy-based methods learn the action to take given a state.

In policy-based methods, the policy is defined as a mapping from states to actions. The goal is to find a policy that maximizes the expected cumulative reward. This policy can be deterministic (always choosing the same action given a state) or stochastic (choosing actions according to a certain probability distribution).
The policy can be parameterized using a function, often represented as π(θ). This function maps states to actions with certain probabilities. For instance, in a continuous action space, a Gaussian policy could be used, which assigns higher probabilities to actions close to the current action.

One common challenge with policy-based methods is that they often converge to a local optimum rather than the global optimum, and evaluating a policy can be computationally expensive.

## The difference between policy-based and policy-gradient methods

Policy-gradient methods, what we’re going to study in this unit, is a subclass of policy-based methods. In policy-based methods, the optimization is most of the time _on-policy_ since for each update, we only use data (trajectories) collected **by our most recent version of $π_θ$​.

The difference between these two methods **lies on how we optimize the parameter $θ$:

- In _policy-based methods_, we search directly for the optimal policy. We can optimize the parameter  $θ$  **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
- In _policy-gradient methods_, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter $θ$ **directly** by performing the gradient ascent on the performance of the objective function $J(θ)$.

==чатик==
SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm used for estimating the action-value function in a Markov Decision Process (MDP). The algorithm updates its Q-values based on the experiences it encounters while interacting with the environment.

Here's the SARSA algorithm with some explanation of its components:

1. **Initialize Q-Values:**
   - Initialize the action-value function \(Q(s, a)\) for all states \(s\) and actions \(a\). This could be done randomly or set to some initial values.

2. **Initialize Policy:**
   - Initialize the policy \(\pi\) that the agent will follow. This policy can be epsilon-greedy, meaning it chooses the action with the highest Q-value with probability \(1-\epsilon\) and selects a random action with probability \(\epsilon\).

3. **For each episode:**
   - Initialize the state s.
   - Choose the action a using the policy $\pi(s)$.
   - Repeat until the episode terminates:
      - Take action a.
      - Observe the reward r and the next state s'.
      - Choose the next action a' using the policy $\pi(s')$.
      - Update the Q-value using the SARSA update rule:

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]$$

   - Update the policy $\pi$ based on the updated Q-values.

4. **Repeat until convergence or a sufficient number of episodes.**

In the SARSA update rule:
- \(Q(s, a)\) is the current estimate of the Q-value for taking action \(a\) in state \(s\).
- \(r\) is the immediate reward obtained after taking action \(a\) in state \(s\).
- \(\gamma\) is the discount factor.
- \(Q(s', a')\) is the estimated Q-value for the next state \(s'\) and the next action \(a'\).
- \(\alpha\) is the learning rate, controlling the step size of the updates.

SARSA ensures that the Q-values are updated based on the actions actually taken by the policy, making it an on-policy algorithm. The resulting policy is closely tied to the exploration strategy used during learning.