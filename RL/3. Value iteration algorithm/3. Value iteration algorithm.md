Перед тем, как приступать к этому разделу, посмотрите билет 5 - Bellman Equations.

![[Pasted image 20240121155248.png]]

![[Pasted image 20240121155300.png]]

Алгоритм схож с Policy Iteration, только тут одна формула оценки политики, то есть один шаг, в отличии от Policy Iteration

![[Pasted image 20240121155616.png]]

[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | По книжке, со страницы 115]]:
![[Pasted image 20240122073122.png]]
![[Pasted image 20240122073136.png]]
Сравнение VI с PI:
![[Pasted image 20240123075624.png]]

==чатик==
Value Iteration is a dynamic programming algorithm used in reinforcement learning to find the optimal state-value function for a Markov Decision Process (MDP). The goal is to iteratively update the estimates of state values until they converge to the optimal values. Here's a step-by-step explanation of the Value Iteration algorithm:

### Value Iteration Algorithm:

1. **Initialisation:**
   - Initialise the state-value function $V(s)$ for all states $s$ to arbitrary values (commonly set to zero).

2. **Iteration:**
   - Repeat the following steps until convergence:
     - For each state $s$, update the state-value estimate using the Bellman optimality equation:
       $V(s) \leftarrow \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot V(s') \right)$
     - $a$ represents possible actions, $R(s, a)$ is the immediate reward, $P(s' \mid s, a)$ is the transition probability, and $\gamma$ is the discount factor.

3. **Convergence Check:**
   - Check for convergence by measuring the change in state values. If the values have converged or if the change is below a predefined threshold, terminate the algorithm.

4. **Policy Extraction (Optional):**
   - If the optimal state values are sufficient, the optimal policy can be extracted from the optimal state values. The policy is obtained by selecting actions that maximize the right-hand side of the Bellman optimality equation.

### Pseudocode:

```plaintext
Initialize V(s) arbitrarily for all s

repeat:
  delta = 0
  for each state s:
    v = V(s)
    V(s) = max_a (R(s, a) + gamma * sum_s' P(s' | s, a) * V(s'))
    delta = max(delta, |v - V(s)|)

until delta < epsilon  // epsilon is a small positive threshold for convergence
```

### Example:
Let's consider a simple grid world with rewards, transitions, and a discount factor. Assume a reward of -1 for each step, a reward of +10 for reaching the goal, and a reward of -5 for hitting an obstacle. The discount factor (\(\gamma\)) is 0.9.

The algorithm would iteratively update the state values until convergence, and the final state values would represent the optimal values for each state in terms of expected cumulative reward.

While it is technically possible to express the Value Iteration algorithm in a recursive manner, this is generally less common in practice due to the potential for stack overflow issues in deeply recursive calls and the associated overhead. The iterative version is more commonly used for efficiency and ease of implementation.

**Value iteration vs. value policy iteration**
Certainly! Let's break it down in simple terms:

**Value Iteration:**

1. **One-Step Process:**
   - Imagine you're in a world, and you want to figure out the best way to move around to get the most rewards.
   - In each step, you look around and decide which move gives you the highest expected reward.
   - You keep doing this for each location in the world until your decisions don't change much. This is your optimal policy.

**Value-Based Policy Iteration:**

simple:
1. **Two-Step Process:**
   - Similar to Value Iteration, you're in a world and want the best way to move around for maximum rewards.
   - First, you start with some random decisions about where to move. You see how good or bad these decisions are by checking the rewards you get.
   - Then, you update your decisions to be better based on what you learned from the rewards. You keep going back and forth, updating decisions and checking rewards until your decisions don't change much. This is your optimal policy.

**Difference:**

- **Value Iteration:**
   - You decide and learn at the same time. It's like walking around and immediately figuring out the best paths as you go.

- **Value-Based Policy Iteration:**
   - You first make some random decisions, then see how good they are, and adjust your decisions based on what you've learned. It's a two-step dance of deciding and learning.

**Choosing:**

- **If you like figuring things out step by step as you go, go for Value Iteration.**
  
- **If you prefer taking small steps, first trying things out, and then adjusting based on what you've learned, go for Value-Based Policy Iteration.**

I hope this simplification helps!

formal:
I apologize for any confusion. Let's simplify the comparison between Value Iteration and Value-Based Policy Iteration.

### Value Iteration:

1. **Single-Step Process:**
   - Value Iteration combines policy evaluation and policy improvement into a single-step process within each iteration.
   - It updates state values directly using the Bellman optimality equation:
    $V(s) \leftarrow \max_a \left( R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \cdot V(s') \right)$
   - The optimal policy is implicitly derived from the updated state values.

2. **Iteration Loop:**
   - The algorithm repeats the single-step process until the state values converge.

### Value-Based Policy Iteration:

1. **Two-Step Process:**
   - Value-Based Policy Iteration separates policy evaluation and policy improvement into two distinct steps.
   - It first evaluates the current policy, updating state values.
   - Then, it improves the policy based on the updated state values.

2. **Iteration Loop:**
   - The two-step process is repeated iteratively until the policy converges.

### Key Difference:

- **Implicit vs. Explicit Policy Improvement:**
   - In Value Iteration, policy improvement is implicit in the process of updating state values.
   - In Value-Based Policy Iteration, policy improvement is an explicit step after policy evaluation.

### Recommendation:

- If you prefer a simpler, one-step process that directly updates state values and implicitly improves the policy, go for **Value Iteration.**

- If you prefer a more modular approach with separate steps for policy evaluation and improvement, go for **Value-Based Policy Iteration.**

I hope this clarifies the distinction between the two algorithms. If you have specific questions or if there's a particular aspect causing confusion, feel free to ask!