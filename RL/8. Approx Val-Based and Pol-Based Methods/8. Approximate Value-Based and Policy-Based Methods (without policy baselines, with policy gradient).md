[Лекция 6](https://youtu.be/s9XNphD2VBE?si=24glzfk2O3XdUfmE)
Лиза в процессе
Есть 2 состояния.
Функция А обеспечивает лучшее значение policy, функция В - имеет меньшую среднеквадратичную оценку, поэтому предпочтём её в соответствии с задачей оптимизаций.
![[Pasted image 20240123183104.png]]
Когда говорим о Q-learning - пытаемся решить прокси-задачу: сначала оцениваем функцию Q, а затем используем её для оценки качества действий во всех состояниях.
Но лучшая Q-функция иногда является неоптимальной.
![[Pasted image 20240123183208.png]]
Бывают разные стратегии
![[Pasted image 20240123184052.png]]
Детерминистические - Q-learning - определяются текущим состоянием
Стохастический - более общий (включает в себя детерминистический) - если делать стохастическую стратегию, то можно сойтись к детерминистической.
В каких ситуациях стохастическая политика лучше детерминистической?
Когда необходимо, чтобы оппонент не предсказал твои действия и не перенял их (если он это сделает, то будем ему постоянно проигрывать).
При стохастическом подходе, используя разные методы, можно изучить окружающую среду. 
Есть 2 сценария:
![[Pasted image 20240123202721.png]]
Первый случай - имеем значения окружающей среды и policy выбирается в дальнейшем.
Второй подход - изучаем policy, а затем выбираем подходящее значение.
![[Pasted image 20240123203840.png]]

In **model-based RL**, the primary objective of the learner is to estimate the underlying model of the environment and subsequently enhance the policy based on this estimated model. Most work in tabular RL fall within this category — they estimate the reward model and transition kernel using the empirical means and update the policy by performing the value iteration on the estimated model. Additionally, some works extend this approach to RL with linear function approximation and general function approximation. 

**Policy-based RL**, in contrast, uses direct policy updates to improve the agent’s performance. Typical algorithms such as policy gradient (Sutton et al., 1999), natural policy gradient (Kakade, 2001), proximal policy optimization (Schulman et al., 2017) fall into this category. A long line of works proposes policy-based algorithms with provable convergence guarantees and sample efficiency. See e.g., Liu et al. (2019); Agarwal et al. (2020, 2021); Cai et al. (2020); Shani et al. (2020); Zhong et al. (2021); Cen et al. (2022); Xiao (2022); Wu et al. (2022); Lan (2023); Zhong and Zhang (2023); Liu et al. (2023a); Sherman et al. (2023) and references therein.

In **value-based RL**, the focus shifts to the approximation of the value function, and policy updates are driven by the estimated value function. A plethora of provable value-based algorithms exists, spanning tabular RL (Jin et al., 2018), linear RL, and beyond. These works mainly explore efficient RL through the lens of sample complexity, with less consideration for representation complexity, which is the focus of our work.

**Policy-based RL**: the learner directly approximates the optimal policy $π^∗$ . The function class $F$ takes the form $F=F_1×···×F_h$ with $F_h⊂{π_h : S→ ∆(A)}$ for any $h∈[H]$. 
**Value-based RL**: the learner utilizes the function class $F=F_1×···×F_H$ to capture the optimal value function $Q^∗$, where $F_h⊂{Q_h : S × A→ [0, H]}$ for any $h ∈ [H]$.


==чатик==
In reinforcement learning, both approximate value-based methods and policy-based methods are used to solve problems. Let's discuss each category and differentiate between those with and without policy baselines and those using policy gradients.

### Approximate Value-Based Methods:

1. **Without Policy Baselines:**
   - **Q-Learning:**
     - **Description:** Q-Learning is a model-free, off-policy algorithm that aims to learn the optimal action-value function.
     - **Update Rule:** $Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$

2. **With Policy Baselines:**
   - **Deep Q Networks (DQN):**
     - **Description:** DQN is an extension of Q-Learning that uses deep neural networks to approximate the action-value function.
     - **Update Rule:** Utilizes experience replay and target networks for stability.

### Policy-Based Methods:

1. **Without Policy Baselines:**
   - **REINFORCE:**
     - **Description:** REINFORCE is a policy-based algorithm that directly optimizes the policy parameters to maximize expected cumulative rewards.
     - **Update Rule:** $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$

2. **With Policy Baselines:**
   - **Actor-Critic Methods:**
     - **Description:** Actor-Critic methods combine policy-based (actor) and value-based (critic) approaches.
     - **Update Rule:** $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (Q(s_t, a_t) - V(s_t)) \right]$
     - **Policy Update:** $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) \cdot A(s, a)$

3. **With Policy Gradients:**
   - **Proximal Policy Optimization (PPO):**
     - **Description:** PPO is an actor-critic algorithm that aims to find a policy that maximizes expected cumulative rewards with a trust region constraint.
     - **Update Rule:** Utilizes a surrogate objective to ensure small policy updates.

### Key Differences:

- **Value-Based Methods:**
  - Approximate the value function (Q-function).
  - Use value estimates to make decisions.

- **Policy-Based Methods:**
  - Directly parameterize and optimize the policy.
  - Learn a policy that maps states to actions.

- **With/Without Policy Baselines:**
  - Policy baselines are used to reduce variance in gradient estimates.

- **Policy Gradients:**
  - Directly optimize the policy parameters using gradient information.

The choice between these methods often depends on the specific problem, the characteristics of the environment, and the requirements of the learning task. Hybrid approaches, such as actor-critic methods, aim to leverage the strengths of both value-based and policy-based methods.