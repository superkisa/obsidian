[Лекция 6](https://youtu.be/s9XNphD2VBE?si=24glzfk2O3XdUfmE)
Лиза в процессе
Есть 2 состояния.
Функция А обеспечивает лучшее значение policy, функция В - имеет меньшую среднеквадратичную оценку, поэтому предпочтём её в соответствии с задачей оптимизаций.
![[Pasted image 20240123183104.png]]
Когда говорим о Q-learning - пытаемся решить прокси-задачу: сначала оцениваем функцию Q, а затем используем её для оценки качества действий во всех состояниях.
Но лучшая Q-функция иногда является неоптимальной.
![[Pasted image 20240123183208.png]]
Бывают разные стратегии
![[Pasted image 20240123184052.png]]
Детерминистические - Q-learning - определяются текущим состоянием
Стохастический - более общий (включает в себя детерминистический) - если делать стохастическую стратегию, то можно сойтись к детерминистической.
В каких ситуациях стохастическая политика лучше детерминистической?
Когда необходимо, чтобы оппонент не предсказал твои действия и не перенял их (если он это сделает, то будем ему постоянно проигрывать).
При стохастическом подходе, используя разные методы, можно изучить окружающую среду. 
Есть 2 сценария:
![[Pasted image 20240123202721.png]]
Первый случай - имеем значения окружающей среды и policy выбирается в дальнейшем.
Второй подход - изучаем policy, а затем выбираем подходящее значение.
![[Pasted image 20240123203840.png]]

In **model-based RL**, the primary objective of the learner is to estimate the underlying model of the environment and subsequently enhance the policy based on this estimated model. Most work in tabular RL fall within this category — they estimate the reward model and transition kernel using the empirical means and update the policy by performing the value iteration on the estimated model. Additionally, some works extend this approach to RL with linear function approximation and general function approximation. 

**Policy-based RL**, in contrast, uses direct policy updates to improve the agent’s performance. Typical algorithms such as policy gradient (Sutton et al., 1999), natural policy gradient (Kakade, 2001), proximal policy optimization (Schulman et al., 2017) fall into this category. A long line of works proposes policy-based algorithms with provable convergence guarantees and sample efficiency. See e.g., Liu et al. (2019); Agarwal et al. (2020, 2021); Cai et al. (2020); Shani et al. (2020); Zhong et al. (2021); Cen et al. (2022); Xiao (2022); Wu et al. (2022); Lan (2023); Zhong and Zhang (2023); Liu et al. (2023a); Sherman et al. (2023) and references therein.

In **value-based RL**, the focus shifts to the approximation of the value function, and policy updates are driven by the estimated value function. A plethora of provable value-based algorithms exists, spanning tabular RL (Jin et al., 2018), linear RL, and beyond. These works mainly explore efficient RL through the lens of sample complexity, with less consideration for representation complexity, which is the focus of our work.

**Policy-based RL**: the learner directly approximates the optimal policy $π^∗$ . The function class $F$ takes the form $F=F_1×···×F_h$ with $F_h⊂{π_h : S→ ∆(A)}$ for any $h∈[H]$. 
**Value-based RL**: the learner utilizes the function class $F=F_1×···×F_H$ to capture the optimal value function $Q^∗$, where $F_h⊂{Q_h : S × A→ [0, H]}$ for any $h ∈ [H]$.


==чатик==
In reinforcement learning, both approximate value-based methods and policy-based methods are used to solve problems. Let's discuss each category and differentiate between those with and without policy baselines and those using policy gradients.

### Approximate Value-Based Methods:

1. **Without Policy Baselines:**
   - **Q-Learning:**
     - **Description:** Q-Learning is a model-free, off-policy algorithm that aims to learn the optimal action-value function.
     - **Update Rule:** $Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$

2. **With Policy Baselines:**
   - **Deep Q Networks (DQN):**
     - **Description:** DQN is an extension of Q-Learning that uses deep neural networks to approximate the action-value function.
     - **Update Rule:** Utilizes experience replay and target networks for stability.

### Policy-Based Methods:

1. **Without Policy Baselines:**
   - **REINFORCE:**
     - **Description:** REINFORCE is a policy-based algorithm that directly optimizes the policy parameters to maximize expected cumulative rewards.
     - **Update Rule:** $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$

2. **With Policy Baselines:**
   - **Actor-Critic Methods:**
     - **Description:** Actor-Critic methods combine policy-based (actor) and value-based (critic) approaches.
     - **Update Rule:** $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (Q(s_t, a_t) - V(s_t)) \right]$
     - **Policy Update:** $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) \cdot A(s, a)$

3. **With Policy Gradients:**
   - **Proximal Policy Optimization (PPO):**
     - **Description:** PPO is an actor-critic algorithm that aims to find a policy that maximizes expected cumulative rewards with a trust region constraint.
     - **Update Rule:** Utilizes a surrogate objective to ensure small policy updates.

### Key Differences:

- **Value-Based Methods:**
  - Approximate the value function (Q-function).
  - Use value estimates to make decisions.

- **Policy-Based Methods:**
  - Directly parameterize and optimize the policy.
  - Learn a policy that maps states to actions.

- **With/Without Policy Baselines:**
  - Policy baselines are used to reduce variance in gradient estimates.

- **Policy Gradients:**
  - Directly optimize the policy parameters using gradient information.

The choice between these methods often depends on the specific problem, the characteristics of the environment, and the requirements of the learning task. Hybrid approaches, such as actor-critic methods, aim to leverage the strengths of both value-based and policy-based methods.

Policy-based methods in reinforcement learning focus on directly learning a policy, a mapping from states to actions, without explicitly estimating or representing the value function. The main idea behind policy-based methods is to optimize the parameters of a policy in order to find an optimal or near-optimal strategy for interacting with the environment.

**Main idea**
Here are the key components and the main idea of policy-based methods:

1. **Policy Parameterization:**
   - The policy is typically parameterized by a set of parameters, often denoted as \(\theta\).
   - The policy function is denoted as $\pi(a|s, \theta)$, representing the probability of taking action $a$ in state $s$ with parameters $\theta$.

2. **Objective Function:**
   - The objective is to find the parameters $\theta$ that maximize the expected cumulative rewards. The objective function is often denoted as $J(\theta)$.
   - The objective function is formulated as the expected sum of rewards, weighted by the probabilities of the actions taken under the policy.

$$J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \,|\, \pi_\theta \right]$$

   - $\gamma$ is the discount factor, and $R_{t+1}$ is the reward obtained at time step $t+1$.

3. **Optimization:**
   - Policy-based methods employ optimization algorithms, such as gradient ascent, to iteratively update the policy parameters in the direction that increases the expected cumulative rewards.
	$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$
   - $\alpha$ is the learning rate, and $\nabla_\theta J(\theta)$ is the gradient of the objective function with respect to the policy parameters.

4. **Stochastic Policies:**
   - Policies are often stochastic, meaning they output probability distributions over actions rather than deterministic actions.
   - This stochasticity can facilitate exploration and improve the robustness of the learning process.

5. **Exploration-Exploitation Trade-off:**
   - Policy-based methods naturally handle the exploration-exploitation trade-off by adjusting the probabilities of different actions in response to the observed rewards.

6. **Policy Gradients:**
   - Policy gradients are a common technique used in policy-based methods. The gradient of the objective function is computed with respect to the policy parameters, and this gradient is used to update the policy in the direction of higher expected rewards.

Policy-based methods have some advantages, such as their ability to handle continuous action spaces and stochastic policies. However, they may suffer from high variance in gradient estimates, and finding a good policy can require more samples compared to value-based methods. Examples of policy-based algorithms include REINFORCE, PPO (Proximal Policy Optimization), and TRPO (Trust Region Policy Optimization).