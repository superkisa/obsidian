[Лекция 6](https://youtu.be/s9XNphD2VBE?si=24glzfk2O3XdUfmE)
Лиза в процессе
Есть 2 состояния.
Функция А обеспечивает лучшее значение policy, функция В - имеет меньшую среднеквадратичную оценку, поэтому предпочтём её в соответствии с задачей оптимизаций.
![[Pasted image 20240123183104.png]]
Когда говорим о Q-learning - пытаемся решить прокси-задачу: сначала оцениваем функцию Q, а затем используем её для оценки качества действий во всех состояниях.
Но лучшая Q-функция иногда является неоптимальной.
![[Pasted image 20240123183208.png]]
Бывают разные стратегии
![[Pasted image 20240123184052.png]]
Детерминистические - Q-learning - определяются текущим состоянием
Стохастический - более общий (включает в себя детерминистический) - если делать стохастическую стратегию, то можно сойтись к детерминистической.
В каких ситуациях стохастическая политика лучше детерминистической?
Когда необходимо, чтобы оппонент не предсказал твои действия и не перенял их (если он это сделает, то будем ему постоянно проигрывать).
При стохастическом подходе, используя разные методы, можно изучить окружающую среду. 
Есть 2 сценария:
![[Pasted image 20240123202721.png]]
Первый случай - имеем значения окружающей среды и policy выбирается в дальнейшем.
Второй подход - изучаем policy, а затем выбираем подходящее значение.
![[Pasted image 20240123203840.png]]

In **model-based RL**, the primary objective of the learner is to estimate the underlying model of the environment and subsequently enhance the policy based on this estimated model. Most work in tabular RL fall within this category — they estimate the reward model and transition kernel using the empirical means and update the policy by performing the value iteration on the estimated model. Additionally, some works extend this approach to RL with linear function approximation and general function approximation. 

**Policy-based RL**, in contrast, uses direct policy updates to improve the agent’s performance. Typical algorithms such as policy gradient (Sutton et al., 1999), natural policy gradient (Kakade, 2001), proximal policy optimization (Schulman et al., 2017) fall into this category. A long line of works proposes policy-based algorithms with provable convergence guarantees and sample efficiency. See e.g., Liu et al. (2019); Agarwal et al. (2020, 2021); Cai et al. (2020); Shani et al. (2020); Zhong et al. (2021); Cen et al. (2022); Xiao (2022); Wu et al. (2022); Lan (2023); Zhong and Zhang (2023); Liu et al. (2023a); Sherman et al. (2023) and references therein.

In **value-based RL**, the focus shifts to the approximation of the value function, and policy updates are driven by the estimated value function. A plethora of provable value-based algorithms exists, spanning tabular RL (Jin et al., 2018), linear RL, and beyond. These works mainly explore efficient RL through the lens of sample complexity, with less consideration for representation complexity, which is the focus of our work.


