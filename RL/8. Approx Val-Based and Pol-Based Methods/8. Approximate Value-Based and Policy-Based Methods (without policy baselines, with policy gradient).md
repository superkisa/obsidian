По этим методам вся суть в том, что мы не всегда можем себе позволить считать политику, или Value-функции в табличном виде, т.к. какая-нибудь Го или Шахматы - не поддаются таким методам. Поэтому нам нужны методы решения таких задач с очень большими пространствами состояний в виде аппроксимации Value-функций, или функции политики, с помощью различных методов, типа нейросетей. Тут могут даже применяться полиномы, или гармонические функции, или тупо линейные функции - нужно смотреть на задачу.
[[Обучение_с_подкреплением_Введение_2_е_издание_2020_Ричард_С_Саттон.pdf | ацкий Саттон]] со страницы 238:
![[Pasted image 20240126075401.png]]
![[Pasted image 20240126075621.png]]
without policy baselines, with policy gradient - значит, что у нас нет какой-то политики на которую мы равняемся, но мы её можем получить из рандомной, градиентом по политике.

[Лекция 6](https://youtu.be/s9XNphD2VBE?si=24glzfk2O3XdUfmE)
Есть 2 состояния.
Функция А обеспечивает лучшее значение policy, функция В - имеет меньшую среднеквадратичную оценку, поэтому предпочтём её в соответствии с задачей оптимизаций.
![[Pasted image 20240123183104.png]]
Когда говорим о Q-learning - пытаемся решить прокси-задачу: сначала оцениваем функцию Q, а затем используем её для оценки качества действий во всех состояниях.
Но лучшая Q-функция иногда является неоптимальной.
![[Pasted image 20240123183208.png]]
Бывают разные стратегии
![[Pasted image 20240123184052.png]]
Детерминистические - Q-learning - определяются текущим состоянием
Стохастический - более общий (включает в себя детерминистический) - если делать стохастическую стратегию, то можно сойтись к детерминистической.
В каких ситуациях стохастическая политика лучше детерминистической?
Когда необходимо, чтобы оппонент не предсказал твои действия и не перенял их (если он это сделает, то будем ему постоянно проигрывать).
При стохастическом подходе, используя разные методы, можно изучить окружающую среду. 
Есть 2 сценария:
![[Pasted image 20240123202721.png]]
Первый случай - имеем значения окружающей среды и policy выбирается в дальнейшем.
Второй подход - изучаем policy, а затем выбираем подходящее значение.
![[Pasted image 20240123203840.png]]

In **model-based RL**, the primary objective of the learner is to estimate the underlying model of the environment and subsequently enhance the policy based on this estimated model. Most work in tabular RL fall within this category — they estimate the reward model and transition kernel using the empirical means and update the policy by performing the value iteration on the estimated model. Additionally, some works extend this approach to RL with linear function approximation and general function approximation. 

**Policy-based RL**, in contrast, uses direct policy updates to improve the agent’s performance. Typical algorithms such as policy gradient (Sutton et al., 1999), natural policy gradient (Kakade, 2001), proximal policy optimization (Schulman et al., 2017) fall into this category. A long line of works proposes policy-based algorithms with provable convergence guarantees and sample efficiency. See e.g., Liu et al. (2019); Agarwal et al. (2020, 2021); Cai et al. (2020); Shani et al. (2020); Zhong et al. (2021); Cen et al. (2022); Xiao (2022); Wu et al. (2022); Lan (2023); Zhong and Zhang (2023); Liu et al. (2023a); Sherman et al. (2023) and references therein.

In **value-based RL**, the focus shifts to the approximation of the value function, and policy updates are driven by the estimated value function. A plethora of provable value-based algorithms exists, spanning tabular RL (Jin et al., 2018), linear RL, and beyond. These works mainly explore efficient RL through the lens of sample complexity, with less consideration for representation complexity, which is the focus of our work.

**Policy-based RL**: the learner directly approximates the optimal policy $π^∗$ . The function class $F$ takes the form $F=F_1×···×F_h$ with $F_h⊂{π_h : S→ ∆(A)}$ for any $h∈[H]$. 
**Value-based RL**: the learner utilizes the function class $F=F_1×···×F_H$ to capture the optimal value function $Q^∗$, where $F_h⊂{Q_h : S × A→ [0, H]}$ for any $h ∈ [H]$.


==чатик==
In reinforcement learning, both approximate value-based methods and policy-based methods are used to solve problems. Let's discuss each category and differentiate between those with and without policy baselines and those using policy gradients.

### Approximate Value-Based Methods:

1. **Without Policy Baselines:**
   - **Q-Learning:**
     - **Description:** Q-Learning is a model-free, off-policy algorithm that aims to learn the optimal Q function.
     - **Update Rule:** $Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$

2. **With Policy Baselines:**
   - **Deep Q Networks (DQN):**
     - **Description:** DQN is an extension of Q-Learning that uses deep neural networks to approximate the Q function.
     - **Update Rule:** Utilizes experience replay and target networks for stability.

### Policy-Based Methods:

1. **Without Policy Baselines:**
   - **REINFORCE:**
     - **Description:** REINFORCE is a policy-based algorithm that directly optimizes the policy parameters to maximize expected cumulative rewards.
     - **Update Rule:** $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$

2. **With Policy Baselines:**
   - **Actor-Critic Methods:**
     - **Description:** Actor-Critic methods combine policy-based (actor) and value-based (critic) approaches.
     - **Update Rule:** $\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot (Q(s_t, a_t) - V(s_t)) \right]$
     - **Policy Update:** $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a|s) \cdot A(s, a)$

3. **With Policy Gradients:**
   - **Proximal Policy Optimization (PPO):**
     - **Description:** PPO is an actor-critic algorithm that aims to find a policy that maximizes expected cumulative rewards with a trust region constraint.
     - **Update Rule:** Utilizes a surrogate objective to ensure small policy updates.

### Key Differences:

- **Value-Based Methods:**
  - Approximate the value function (Q-function).
  - Use value estimates to make decisions.

- **Policy-Based Methods:**
  - Directly parameterize and optimize the policy.
  - Learn a policy that maps states to actions.

- **With/Without Policy Baselines:**
  - Policy baselines are used to reduce variance in gradient estimates.

- **Policy Gradients:**
  - Directly optimize the policy parameters using gradient information.

The choice between these methods often depends on the specific problem, the characteristics of the environment, and the requirements of the learning task. Hybrid approaches, such as actor-critic methods, aim to leverage the strengths of both value-based and policy-based methods.

Policy-based methods in reinforcement learning focus on directly learning a policy, a mapping from states to actions, without explicitly estimating or representing the value function. The main idea behind policy-based methods is to optimize the parameters of a policy in order to find an optimal or near-optimal strategy for interacting with the environment.

**Main idea**
Here are the key components and the main idea of policy-based methods:

1. **Policy Parameterization:**
   - The policy is typically parameterized by a set of parameters, often denoted as $\theta$.
   - The policy function is denoted as $\pi(a|s, \theta)$, representing the probability of taking action $a$ in state $s$ with parameters $\theta$.

2. **Objective Function:**
   - The objective is to find the parameters $\theta$ that maximize the expected cumulative rewards. The objective function is often denoted as $J(\theta)$.
   - The objective function is formulated as the expected sum of rewards, weighted by the probabilities of the actions taken under the policy.

$$J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \,|\, \pi_\theta \right]$$

   - $\gamma$ is the discount factor, and $R_{t+1}$ is the reward obtained at time step $t+1$.

3. **Optimization:**
   - Policy-based methods employ optimization algorithms, such as gradient ascent, to iteratively update the policy parameters in the direction that increases the expected cumulative rewards.
	$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$
   - $\alpha$ is the learning rate, and $\nabla_\theta J(\theta)$ is the gradient of the objective function with respect to the policy parameters.

4. **Stochastic Policies:**
   - Policies are often stochastic, meaning they output probability distributions over actions rather than deterministic actions.
   - This stochasticity can facilitate exploration and improve the robustness of the learning process.

5. **Exploration-Exploitation Trade-off:**
   - Policy-based methods naturally handle the exploration-exploitation trade-off by adjusting the probabilities of different actions in response to the observed rewards.

6. **Policy Gradients:**
   - Policy gradients are a common technique used in policy-based methods. The gradient of the objective function is computed with respect to the policy parameters, and this gradient is used to update the policy in the direction of higher expected rewards.

Policy-based methods have some advantages, such as their ability to handle continuous action spaces and stochastic policies. However, they may suffer from high variance in gradient estimates, and finding a good policy can require more samples compared to value-based methods. Examples of policy-based algorithms include REINFORCE, PPO (Proximal Policy Optimization), and TRPO (Trust Region Policy Optimization).

### List of methods ###
Approximate value-based and policy-based methods without policy baselines, but with policy gradient typically fall under the category of reinforcement learning algorithms that use parameterized policies and optimize them through gradient ascent. Here's a list of such methods:

### Approximate Value-Based Methods:

1. **Deep Q-Network (DQN):**
   - **Type:** Value-based
   - **Objective:** Uses a neural network to approximate the action-value function (\(Q\)) and employs techniques like experience replay and target networks for stability.

2. **Double Q-Learning:**
   - **Type:** Value-based
   - **Objective:** Extends Q-learning by addressing overestimation bias in action values through the use of two separate networks.

### Policy-Based Methods without Policy Baselines, but with Policy Gradient:

1. **Vanilla Policy Gradient (REINFORCE):**
   - **Type:** Policy-based
   - **Objective:** Directly optimizes the policy parameters using the gradient of the expected cumulative reward with respect to the policy parameters.

2. **Trust Region Policy Optimization (TRPO):**
   - **Type:** Policy-based
   - **Objective:** Optimizes policies while ensuring a trust region to prevent large policy updates.

3. **Proximal Policy Optimization (PPO):**
   - **Type:** Policy-based
   - **Objective:** Optimizes policies in a more stable manner by enforcing a constraint on the policy update.

4. **Actor-Critic Methods without Baselines:**
   - **Type:** Hybrid (combines value-based and policy-based)
   - **Objective:** Combines the advantages of both value-based and policy-based approaches by having a value function (critic) to reduce variance.

5. **Deterministic Policy Gradient (DPG):**
   - **Type:** Policy-based
   - **Objective:** Extends policy gradient methods to deterministic policies, suitable for continuous action spaces.

6. **Soft Actor-Critic (SAC):**
   - **Type:** Actor-Critic
   - **Objective:** Incorporates entropy regularization to encourage more exploratory behavior in the learned policy.

These methods utilize deep neural networks to approximate either the action-value function or the policy directly. They often leverage gradient information to update the network parameters and improve the performance of the learned policies. Note that some methods may include variants or improvements that involve policy baselines or other techniques for stability and faster convergence.
