==чатик??== кто вкинул, поясните, плиз откуда это

The Cross-Entropy Method is an optimisation technique often used in reinforcement learning for solving optimisation problems. It is particularly effective when dealing with high-dimensional or continuous spaces.

#### 1. **Tabular Cross-Entropy Method:**

**Objective:**
- Solve optimisation problems by iteratively updating a probability distribution over the solution space.

**Procedure:**
1. **Initialisation:**
   - Initialise a probability distribution over the solution space. In the tabular case, this could be a discrete probability distribution over states and actions.

2. **Sampling:**
   - Sample a set of solutions (states/actions) from the current distribution.

3. **Evaluation:**
   - Evaluate the performance of each sampled solution using a user-defined objective function (e.g., cumulative reward).

4. **Selection:**
   - Select the top-performing solutions based on the objective function.

5. **Update:**
   - Update the probability distribution to favour the selected solutions. This is typically done by adjusting parameters to increase the likelihood of selecting good solutions.

6. **Convergence Check:**
   - Check for convergence, and if not converged, repeat steps 2-5.

**Example:**
- In the context of reinforcement learning, the Cross-Entropy Method might be used to optimise a policy in a discrete action space. The distribution could be over actions, and the objective function could be the cumulative reward.

#### 2. **Approximate Cross-Entropy Method:**

**Objective:**
- Extend the Cross-Entropy Method to handle continuous or high-dimensional solution spaces using function approximation.

**Procedure:**
1. **Initialisation:**
   - Initialise a parameterised policy or function approximator (e.g., a neural network).
     a neural network is employed as a function approximator for representing the policy. The neural network takes the state of the environment as input and outputs a probability distribution over possible actions. This is done to handle problems with continuous or high-dimensional action spaces, where it's impractical to explicitly enumerate and store all possible actions.

2. **Sampling:**
   - Sample a set of solutions (states/actions) from the current policy.

3. **Evaluation:**
   - Evaluate the performance of each sampled solution using a user-defined objective function.

4. **Selection:**
   - Select the top-performing solutions based on the objective function.

5. **Update:**
   - Update the parameters of the policy or function approximator to increase the likelihood of selecting good solutions.

6. **Convergence Check:**
   - Check for convergence, and if not converged, repeat steps 2-5.

**Example:**

- Let's say you're training a neural network to control a robotic arm. The network takes the current position and velocity of the arm as input and outputs probabilities for various joint movements. The Cross-Entropy Method, in this case, will guide the training of the neural network to favour joint movements that result in the desired task achievement, such as reaching a target location.

**Key Points:**

- The neural network acts as a policy approximator, allowing the Cross-Entropy Method to be applied to problems with continuous or high-dimensional action spaces.
- The network's parameters are adjusted to increase the likelihood of selecting actions that lead to higher rewards.
- This method is particularly beneficial when the action space is too large to be explicitly enumerated.

In summary, the use of a neural network in the Cross-Entropy Method enables the handling of complex problems with continuous action spaces, allowing for more versatile and scalable applications in reinforcement learning.

**Example:**
- In a continuous control task, the Cross-Entropy Method could be applied with a neural network representing a policy. The network's parameters are adjusted to maximise the expected cumulative reward.

**Key Points:**
- The Cross-Entropy Method is useful for optimisation in both tabular and approximate cases.
- It can be applied to various reinforcement learning problems, especially those involving policy optimisation.
- The method is particularly effective when dealing with high-dimensional or continuous solution spaces.

Understanding the Cross-Entropy Method involves grasping its iterative process, the role of sampling, evaluation based on an objective function, and the update mechanism for improving the current solution distribution.

--------------------------------------------------------------------

**Cross-Entropy method: tabular case**

![[Pasted image 20240121002114.png]]

![[Pasted image 20240121002128.png]]

![[Pasted image 20240121002142.png]]

![[Pasted image 20240121002150.png]]

![[Pasted image 20240121002158.png]]

......

![[Pasted image 20240121002221.png]]

![[Pasted image 20240121002235.png]]

![[Pasted image 20240121002338.png]]

![[Pasted image 20240121002354.png]]

Проблема в том, что эта штука может застревать в локальном максимуме (т. е. выбрать определенную оптимальную политику и больше не смещаться с нее)

Также если использовать данную задачу в больших окружениях (environments), таких как игры, например, то таблица станет здоровенной, так как состояний очень много. 

![[Pasted image 20240121003039.png]]

Можно сжимать картинку с помощью какого-то апроксиматора
В случае картинки это будет сверточная нейронная сеть. Можем брать на вход картинку и предсказывать вероятности действий из нынешнего состояния, например.

![[Pasted image 20240121003446.png]]

Суть та же, после игры выбираем элитные траектории и на них дообучаемся с помощью кросс энтропией.

Но и тут может быть проблема с тем, что пространство действий не дискретное..
Что, если пространство у нас непрерывное?

![[Pasted image 20240121003944.png]]

![[Pasted image 20240121004123.png]]

На самом деле ничего сильно не поменялось, просто теперь у нас задача регрессии.







