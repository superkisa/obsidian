==надо перевести некоторые моменты с физтеховского на человеческий==
[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки со страницы 403]]:
![[Pasted image 20240121195227.png]]
Мне нечего добавить, если вы уже разобрались с 6-м вопросом, то тут должно быть все ясно. Сравнение уравнений DDPG с DQN:
![[Pasted image 20240121195242.png]]
Код:
![[Pasted image 20240121195303.png]]
![[Pasted image 20240121195314.png]]
![[Pasted image 20240124232332.png]]
![[Pasted image 20240124232430.png]]
Код сети:
![[Pasted image 20240124232536.png]]
![[Pasted image 20240124232630.png]]
![[Pasted image 20240124232658.png]]
Оптимизация:
![[Pasted image 20240124232746.png]]
![[Pasted image 20240124232908.png]]
В DDPG мы формируем детерминированные жадные политики, которые в идеальных условиях принимают состояние и возвращают оптимальное для него действие. Но когда политика не обучена, возвращаемые действия будут неточны, хоть и детерминированы. Агентам приходится искать баланс между использованием имеющихся и сбором (исследованием) новых знаний. Но, так как агент DDPG формирует детерминированную политику, сбор данных будет нестратегическим. Представьте, что ваш агент упрямится и всегда выбирает одно и то же действие. Чтобы этого избежать, мы должны исследовать среду без использования политики. Поэтому в DDPG мы внедряем гауссовский шум в выбранные политикой действия.
Вы узнали об исследовании в нескольких агентах DRL. В NFQ, DQN и т. д. мы используем стратегии исследования, основанные на Q-значениях: получаем значения действий в заданном состоянии с помощью изученной Q-функции и исследуем на основе этих значений. В REINFORCE, VPG и т. д. мы применяем стохастические политики, поэтому этап исследования получается стратегическим: поскольку политика стохастическая (у нее есть элемент случайности), она сама занимается исследованием. В DDPG агент исследует среду, примешивая к действиям внешний шум, используя нестратегические методы исследования.
Тот самый шум:
![[Pasted image 20240124233319.png]]
![[Pasted image 20240124233355.png]]
###### TD3 - это двойной DDPG
со страницы 410:
![[Pasted image 20240121195501.png]]
Тут что-то на физтеховском:
![[Pasted image 20240121195527.png]]
Дальше на змеином:
![[Pasted image 20240124233757.png]]
![[Pasted image 20240124233834.png]]
![[Pasted image 20240124233857.png]]
![[Pasted image 20240124233931.png]]
![[Pasted image 20240124234003.png]]
Как вы помните, чтобы улучшить этап исследования в DDPG, мы внедряем гауссовский шум в действия, которые передаются среде. В TD3 эта идея получила развитие: мы добавляем шум к действиям, которые используются не только на этапе исследования, но и при вычислении целей. Формирование политики с шумными (искаженными) целями — это своеобразная регуляризация, так как теперь сеть вынуждена обобщать похожие действия. До этого сеть политик могла сходиться на неправильных действиях, ошибочно оцененных Q-функцией на ранних стадиях обучения. Новый метод позволяет это предотвратить. Шум распространяет значение по более широкому диапазону действий, чем прежде. 
![[Pasted image 20240124234133.png]]
Код:
![[Pasted image 20240124234214.png]]
![[Pasted image 20240124234241.png]]
![[Pasted image 20240124234304.png]]
![[Pasted image 20240124234338.png]]
![[Pasted image 20240124234416.png]]
![[Pasted image 20240124234439.png]]
Последняя оптимизация, которая отличает TD3 от DDPG, — откладывание обновлений сети политик и целевых сетей, благодаря чему динамическая сеть Q-функции обновляется чаще других. Задержка остальных обновлений приносит пользу, так как динамическая Q-функция показывает резкий перепад значений в начале обучения. Замедление политики так, чтобы она начинала обновляться только после нескольких обновлений функции ценности, позволяет последней сойтись на более точных значениях, прежде чем мы начнем использовать ее для формирования политики. Сети политик и целевые сети нужно обновлять в два раза реже, чем динамическую сеть Q-функции.
Еще один интересный аспект обновления политик — то, что мы должны применить один из потоков динамической модели значений, чтобы получить спрогнозированное Q-значение для действия, генерируемого политикой. В TD3 мы используем один из двух потоков — причем всегда один и тот же.

Дальше описание алгоритма SAC, который вроде не упоминался........