==чатик==


1. **State Value Function (V-function):**
   - The state value function, denoted as $V(s)$, quantifies the expected return an agent can achieve from a given state $s$, while adhering to a specific policy. Mathematically, it is expressed as the sum of the expected future rewards, discounted by a factor $\gamma$, over all possible future states:

     $$ V(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s\right] $$

   Where:
   - $\mathbb{E}_{\pi}$ denotes the expectation under policy $\pi$.
   - $R_{t+1}$ is the reward obtained at time step $t+1$.
   - $\gamma$ is the discount factor determining the importance of future rewards.

2. **State-Action Value Function (Q-function):**
   - The state-action value function, denoted as $Q(s, a)$, quantifies the expected return an agent can achieve from a given state $s$, upon executing a specific action $a$, and subsequently following a predefined policy. Mathematically, it is expressed as:

     $$ Q(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a\right] $$

   Where:
   - $\mathbb{E}_{\pi}$ denotes the expectation under policy $\pi$.
   - $R_{t+1}$ is the reward obtained at time step $t+1$.
   - $S_0 = s$ and $A_0 = a$ denote the initial state and action.

**Interpretation:**
In the context of a stochastic environment, these functions provide a formalised representation of the expected cumulative rewards associated with particular states and state-action pairs. By optimising these functions, an agent aims to derive an optimal policy that maximises its long-term reward in the given environment. The discount factor $\gamma$ accounts for the diminishing significance of future rewards, reflecting the agent's preference for more immediate outcomes.


**Value-Based Policy Iteration:**

1. **Initialisation:**
   - Commence the algorithmic process by initialising the value function, denoted as $V(s)$, arbitrarily for each state $s$. This function represents the expected cumulative reward from following the current policy.

2. **Policy Evaluation:**
   - Conduct policy evaluation by iteratively updating the value function based on the expected cumulative rewards obtained from executing actions according to the current policy. This involves employing the Bellman equation:

     $$ V(s) \leftarrow \max_a \left( \sum_{s', r} P(s', r \mid s, a) \cdot \left( r + \gamma \cdot V(s') \right) \right) $$

   Where:
   - $P(s', r \mid s, a)$ represents the transition dynamics, indicating the probability of transitioning to state $s'$ with reward $r$ given the current state $s$ and action $a$.
   - $\gamma$ denotes the discount factor.

3. **Policy Improvement:**
   - Subsequently, refine the policy by selecting actions that maximize the expected cumulative reward, leveraging the updated value function. This involves choosing actions that lead to states with higher estimated values.

4. **Iteration:**
   - Repeat the process of policy evaluation and improvement iteratively until convergence. This convergence occurs when the value function and policy reach stability, indicating an optimal solution.

**Conceptual Framework:**
   - The algorithm combines two key steps: evaluating the effectiveness of the current policy and refining the policy based on this evaluation. Through iterative updates to the value function and policy, the agent converges towards an optimal strategy for maximizing cumulative rewards.

**Outcome:**
   - As the algorithm progresses through multiple iterations, the value function converges, and the policy refines to an optimal state. The agent learns to make decisions that maximize its long-term rewards within the given environment.

In essence, value-based policy iteration is a systematic process wherein an agent refines its policy through iterative evaluation and improvement, ultimately converging to an optimal strategy for navigating a given environment. The Bellman equation serves as a foundational tool in updating the value function, capturing the expected rewards of state-action pairs under the current policy.

-----------------------------------------------------------
![[Pasted image 20240121125431.png]]

![[Pasted image 20240121125453.png]]

![[Pasted image 20240121125505.png]]

![[Pasted image 20240121125516.png]]

![[Pasted image 20240121125600.png]]

![[Pasted image 20240121125617.png]]

![[Pasted image 20240121125630.png]]

![[Pasted image 20240121125650.png]]