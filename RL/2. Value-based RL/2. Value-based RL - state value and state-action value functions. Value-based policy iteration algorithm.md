==чатик==


1. **State Value Function (V-function):**
   - The state value function, denoted as $V(s)$, quantifies the expected return an agent can achieve from a given state $s$, while adhering to a specific policy. Mathematically, it is expressed as the sum of the expected future rewards, discounted by a factor $\gamma$, over all possible future states:

     $$ V(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s\right] $$

   Where:
   - $\mathbb{E}_{\pi}$ denotes the expectation under policy $\pi$.
   - $R_{t+1}$ is the reward obtained at time step $t+1$.
   - $\gamma$ is the discount factor determining the importance of future rewards.

2. **State-Action Value Function (Q-function):**
   - The state-action value function, denoted as $Q(s, a)$, quantifies the expected return an agent can achieve from a given state $s$, upon executing a specific action $a$, and subsequently following a predefined policy. Mathematically, it is expressed as:

     $$ Q(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a\right] $$

   Where:
   - $\mathbb{E}_{\pi}$ denotes the expectation under policy $\pi$.
   - $R_{t+1}$ is the reward obtained at time step $t+1$.
   - $S_0 = s$ and $A_0 = a$ denote the initial state and action.

**Interpretation:**
In the context of a stochastic environment, these functions provide a formalised representation of the expected cumulative rewards associated with particular states and state-action pairs. By optimising these functions, an agent aims to derive an optimal policy that maximises its long-term reward in the given environment. The discount factor $\gamma$ accounts for the diminishing significance of future rewards, reflecting the agent's preference for more immediate outcomes.

==

The connection between the state-value function (V-function) and the state-action value function (Q-function) is established through the following relationship, known as the Bellman equation for Q-values. This equation expresses the Q-value of a state-action pair in terms of the expected immediate reward and the value of the next state.

For a given state s and action a, the Bellman equation for Q-values is defined as:

$$ Q(s, a) = R(s, a) + \gamma \cdot \sum_{s'} P(s' \mid s, a) \cdot V(s') $$

Here:
- Q(s, a) is the Q-value for taking action a in state s,
- R(s, a) is the immediate reward for taking action a in state s,
- $\gamma$ is the discount factor,
- $P(s' \mid s, a)$ is the probability of transitioning to state s' from state s by taking action a,
- V(s') is the state-value function, representing the expected cumulative reward from state s' onward.

The state-value function is related to the Q-function through the following expression:

$$ V(s) = \max_a Q(s, a)$$

This equation states that the value of a state is equal to the maximum Q-value across all possible actions in that state.

In summary, the V-function represents the expected cumulative reward from a given state, while the Q-function represents the expected cumulative reward from a given state-action pair. The connection is established by expressing Q-values in terms of state-values and immediate rewards using the Bellman equation.


**Value-Based Policy Iteration:**

1. **Initialisation:**
   - Commence the algorithmic process by initialising the value function, denoted as $V(s)$, arbitrarily for each state $s$. This function represents the expected cumulative reward from following the current policy.

2. **Policy Evaluation:**
   - Conduct policy evaluation by iteratively updating the value function based on the expected cumulative rewards obtained from executing actions according to the current policy. This involves employing the Bellman equation:

     $$ V(s) \leftarrow \max_a \left( \sum_{s', r} P(s', r \mid s, a) \cdot \left( r + \gamma \cdot V(s') \right) \right) $$

   Where:
   - $P(s', r \mid s, a)$ represents the transition dynamics, indicating the probability of transitioning to state $s'$ with reward $r$ given the current state $s$ and action $a$.
   - $\gamma$ denotes the discount factor.

3. **Policy Improvement:**
   - Subsequently, refine the policy by selecting actions that maximize the expected cumulative reward, leveraging the updated value function. This involves choosing actions that lead to states with higher estimated values.

4. **Iteration:**
   - Repeat the process of policy evaluation and improvement iteratively until convergence. This convergence occurs when the value function and policy reach stability, indicating an optimal solution.

**Conceptual Framework:**
   - The algorithm combines two key steps: evaluating the effectiveness of the current policy and refining the policy based on this evaluation. Through iterative updates to the value function and policy, the agent converges towards an optimal strategy for maximizing cumulative rewards.

**Outcome:**
   - As the algorithm progresses through multiple iterations, the value function converges, and the policy refines to an optimal state. The agent learns to make decisions that maximize its long-term rewards within the given environment.

In essence, value-based policy iteration is a systematic process wherein an agent refines its policy through iterative evaluation and improvement, ultimately converging to an optimal strategy for navigating a given environment. The Bellman equation serves as a foundational tool in updating the value function, capturing the expected rewards of state-action pairs under the current policy.

-----------------------------------------------------------

**Value-based RL - state value and state-action value functions**
![[Pasted image 20240122043604.png]]

![[Pasted image 20240121125431.png]]

![[Pasted image 20240121125600.png]]

Интуитивно, V_пи(s) - это полезность нашего состояния(s). т. е. сколько награды в среднем я получу, если из состояния s я начну играть политикой пи.

![[Pasted image 20240121125630.png]]
![[Pasted image 20240122045920.png]]
![[Pasted image 20240121125650.png]]
Очень просто о V и Q:
![[Pasted image 20240123203336.png]]
**Value-based policy iteration algorithm**

Перед тем, как приступать к изучению этого раздела посмотрите 5 билет - Bellman Equations

![[Pasted image 20240121155036.png]]

![[Pasted image 20240121155051.png]]

2 шага:

1) До "бесконечности" улучшаем V. Потом переходим к Policy Improvement
2) Делаем одно улучшение политики и возвращаемся к 1 пункту

![[Pasted image 20240121155623.png]]

[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | По книжке, со страницы 99]], гораздо подробнее это расписано. Сначала объясняется как считать V-функцию - формула:
![[Pasted image 20240122065006.png]]
код:
![[Pasted image 20240122065116.png]]
Затем как улучшать политику, зная V во всех состояниях - формула:
![[Pasted image 20240122065329.png]]
код:
![[Pasted image 20240122065445.png]]
И вишенкой на торте - код policy iteration:
![[Pasted image 20240122065611.png]]
