[3-я лекция](https://youtu.be/aGsLzQla3nk?si=ImcSphYejsIy3Eou)

![[Pasted image 20240121225024.png]]
![[Pasted image 20240121205237.png]]
![[Pasted image 20240121205251.png]]
![[Pasted image 20240121205303.png]]

[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 206, про 
###### SARSA:
![[Pasted image 20240123075331.png]]
![[Pasted image 20240121200346.png]]
![[Pasted image 20240121200321.png]]
Код агента SARSA:
![[Pasted image 20240123075923.png]]
![[Pasted image 20240123075957.png]]
![[Pasted image 20240123080030.png]]
![[Pasted image 20240123080109.png]]
Уравнения:
![[Pasted image 20240123080227.png]]
###### EV-SARSA 
есть только этот слайд из лекции:
![[Pasted image 20240121205251.png]]
Хотя в [[BartoSutton.pdf| Барто-Саттоне]] на странице 133, есть про EV-SARSA:
![[Pasted image 20240123225838.png]]
![[Pasted image 20240123225903.png]]
![[Pasted image 20240123225927.png]]
Кажется я познал суть - если из [[BartoSutton.pdf| книжки Барто-Саттона]] вытянуть в одно место все три метода, 
Q-learning:
![[Pasted image 20240123234317.png]]
SARSA:
![[Pasted image 20240123234408.png]]
Expected SARSA:
![[Pasted image 20240123234025.png]]
то я вижу такую разницу:
 - в Q-learning как цель, к которой итеративно мы идем, формируется из *максимального значения $Q$* в следующем состоянии *из всех возможных действий*;
 - в SARSA в цель, вкладывается значение $Q$ из следующего состояния и какого-то, *рандомно $\epsilon$-greedy-выбранного* действия в соответствии с политикой;
 - в Expected SARSA же агент берет не какое-то конкретное $Q$, выбранное по какому-то из вышеописанных методов, а берет *матожидание $Q$ всех действий следующего состояния*. Т.е. перебираются все действия, из них берется $Q$ и суммируется с вероятностями в качестве весов этих действий из действующей политики.

###### Q-learning (и on-policy, off-policy)
уже есть в [[4. Between them. Monte Carlo vs TD update questions. Q-leaning algorithm#Q-leaning algorithm| 4-м вопросе]]

###### DQN
Про DQN главы 8 и 9 книги, там отдельная "Война и мир", но я постараюсь изложить кратко.
Суть всего действа в том, что во всех предыдущих методах, мы каким-то хитровыдуманным способом улучшали дискретные табличные значения $Q$, которые зависят от состояния и выбранного дальнейшего действия:
![[Pasted image 20240124054848.png]]
Так вот, имея на руках такой инструмент, как нейросети, которые являются универсальными аппроксиматорами любых функций, конечно же возникла идея моделировать Q-функцию с помощью нейросетей непрерывно. На картинке показана аппроксимация $V$, но сути это не меняет:
![[Pasted image 20240124055243.png]]
Возникает конечно же вопрос, а как мы будем обучать нейросеть, ведь для её обучения нам нужны известные значения, из которых мы будем рассчитывать лосс и с помощью него формировать градиенты для настройки параметров сети. В качестве лосса напрашивается сразу MSE, а в качестве цели (известных значений) решили попробовать ту цель, к которой шёл Q-learning:
![[Pasted image 20240124055831.png]]
И такой подход, который в книге называется NFQ (вся 8-я глава про него), дал неплохие результаты в простых агентах и средах. К примеру, для среды с тележкой и перевернутым маятником, Q-функцию аппроксимировали в таком виде:
![[Pasted image 20240124062534.png]]
Т.е. на вход НС подается состояние среды, а на выходе мы получаем значения $Q$ для каждого возможного действия. Здесь регрессия из 4-х входов, и на 2 выхода.

Но конечно же такой метод плохо работал в более сложных средах, т.к. у него есть проблемы:
- цель, к которой нейросеть пытается подстроиться - постоянно изменяется. Ведь мы берем Q следующего состояния, к которому стремимся, предсказанные этой же сетью. А это очень плохо для обучения нейросети, т.к. все методы и оптимизаторы созданы из расчета, что мы имеем постоянные, фиксированные данные для обучения;![[Pasted image 20240124061414.png]]
- также есть проблема с тем, что между обучающими данными есть корреляция, а по условиям обучения нейросети, данные должны иметь свойство IID.

Так вот это была присказка, теперь будет сказка о DQN:
~~Жил был дед Саттон с братом Барто и внучком Владимиром Мнихом (изобретателем DQN)~~
Подумали люди, и придумали как решить эти проблемы может не полностью, но хоть частично. Проблема с тем, что цель к которой учится нейросеть постоянно убегает, как свой хвост собаки, которая пытается его цапнуть - проблема решается применением зафиксированной копии нейросети:
![[Pasted image 20240124061328.png]]
При этом лосс-функция практически не меняется, только что нам нужно использовать два набора параметров нейросети:
![[Pasted image 20240124061534.png]]
одна НС фиксированная, и одна динамическая, которая учится на постоянных предсказаниях фиксированной НС. Кусочек кода, в котором как раз момент с двумя НС:
![[Pasted image 20240124061907.png]]
А вопрос с корреляцией данных решается размером нейросети. Т.е. чем больше нейросеть, тем больше у неё способностей генерировать разрозненные данные. Более сложные взаимосвязи можно аппроксимировать, и соответственно мы все ближе к свойству IID. 

Но здесь кроется еще один момент в том, что проходя по обучению в указанном в уравнениях порядке, мы всегда идем по какой-то из траекторий состояний. Соответственно, мы все равно имеем риск получать скоррелированные данные. Этот вопрос можно решить с помощью введения в процесс обучения *буфера воспроизведения опыта*, который был у нас в домашке. Т.е. мы собираем большой буфер данных во время этапа исследования агента. И затем формируем батчи для обучения нейросети из этого буфера рандомно. Таким образом мы учим нейросеть не на данных полученных последовательным проходом по траекториям, а хорошо перемешанным данным из различных траекторий, что еще более приближает наши данные к свойству IID.
![[Pasted image 20240124064024.png]]
У этого способа конечно же есть еще и свои подводные камни, т.к., возможно, что мы много учим сеть на редких состояниях, или далеких от цели, и в целом, довольно бесполезных, и тут всплывает отдельная тема с оценкой приоритетов и полезности воспроизводимого опыта (о чем в книге с 346-й страницы и думаю, нам это не пригодится на экзамене).

Если кратко - то так, думаю, этого будет достаточно...

Далее идет сказка о DDQN, по аналогии с двойным Q-learning, но это уже совсем другая история со все более красивыми градиентами...
![[Pasted image 20240124065930.png]]