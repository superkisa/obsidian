==Евгений, занимаюсь этим вопросом, думаю, до конца==
[Лекция 3](https://youtu.be/aGsLzQla3nk?si=trRG_jvP8_JuaLnL)
![[Pasted image 20240122203023.png]]
![[Pasted image 20240122203044.png]]
Более подробно рассмотрено это все в презенташке к несуществующей лекции 9: ![[MSAI_RL_lect09_exploration.pdf]]
но вообще тема раскрывается в [вебинаре 9](https://youtu.be/cR5fpgjjk0w?si=rL5M7D8Tje6FzgQD)

[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 119, целая глава про Exploration-exploitation trade-off, с подробнейшим объяснением, хотя вроде объяснять особо нечего).
Стр. 128, код для жадной эксплуатации без исследования вообще:
![[Pasted image 20240122185942.png]]
Суть в том, что агент всегда выбирает тот вариант, который дал какой-то положительный реворд. При этом он не проверяет другие варианты, а они могут быть и лучше. 

Стр. 130, код для рандомного исследования (не лучшая стратегия для исследования):
![[Pasted image 20240122190053.png]]
При этом агент просто рандомно выбирает действие, и возможно когда-то исследует всю среду, но это будет довольно долго и не эффективно.
Здесь важно понять, что использовать полученные знания агент может только одним способом - жадно - выбирая тот вариант, который даст наибольшую предполагаемую награду. Но стратегий исследования может быть очень много разных, и случайное исследование - не лучшая стратегия.

Стр. 132, код для эпсилон-жадной стратегии (уже есть trade-off):
![[Pasted image 20240122190223.png]]
Сначала агент действует больше рандомно чем жадно, и делает ставку на исследование, а когда уже "наберется опыта" - тогда плавно переходит к стратегии жадной стратегии, что логично, и неплохо работает.
Далее есть примеры с линейным или экспоненциальным затуханием эпсилон - думаю, это понятно, что эпсилон можно изменять, в процессе, и таким образом изменять соотношение эксплуатации-исследования - достаточно просто знать что они есть, код не особо сложнее.

Есть еще вид исследования - оптимистичная инициализация - когда значения Q-функции инициализируются не нулями, а единицами (*почему единицами? - потому что мы знаем максимальную награду среды здесь в этом примере, но в общем случае нечестно её знать, и об этом будет в методе UCB*), и тогда получается, что агент при исследовании начинает не с того, что выбирает что-то из "самых плохих" вариантов, а наоборот считает, что все варианты прекрасны на первый взгляд, но в процессе исследования, значения Q-функции начинают уменьшаться, и становятся более реальными. Таким образом, применяется жадная стратегия, но из-за ненулевой инициализации агент имеет шанс проверить все варианты, пока не убедится, что какие-то из них хуже чем другие.
![[Pasted image 20240122191539.png]]

Далее со страницы 141 описывается стратегия построенная на функции softmax - её нет в вопросе, но, в лекции была, и для общего понимания, думаю, это важно. Т.е. вероятность выбора агентом какого-то действия теперь зависит от разницы в величинах Q-функции, и более высокие значения, получают более высокую вероятность (плюс, еще параметр "температуры"):
![[Pasted image 20240122194101.png]]
код:
![[Pasted image 20240122194232.png]]
![[Pasted image 20240122194319.png]]

UCB - upper confidence bound strategy - стратегия верхнего доверительного интервала (стр. 143):
![[Pasted image 20240122195654.png]]
![[Pasted image 20240122195822.png]]
![[Pasted image 20240122195849.png]]

Thompson sampling, стр. 145:
![[Pasted image 20240122200822.png]]
![[Pasted image 20240122200849.png]]
![[Pasted image 20240122200918.png]]
![[Pasted image 20240122200938.png]]
Я не до конца понял, но кажется, что вместо того чтобы жадно выбрать действие с максимальной Q мы делаем вокруг этой Q сэмплы из нормального распределения, указывая сэмплеру, что центр распределения находится в известном Q, а ширина распределения постепенно уменьшается. Таким образом мы делаем предположение, что Q имеет нормальное распределение (это конечно же не всегда так, и автор уточняет, что в реальности обычно это бета-расределение) и выбираем действие рандомно, учитывая это распределение как априорное знание. За счет того, что ширина распределения со временем уменьшается, то получается, что те действия, которые мы часто выбирали, имеют меньшую ширину распределения, и соответственно меньший разброс при семплировании, что дает больше жадной эксплуатации, чем исследования. А те действия, которые реже выбирались, имеют более широкое распределение - и соответственно при семплировании агент больше исследует это действие, пока не добьется лучшего изучения среды.
Из презентации приведенной выше, итог по Томпсону:
![[Pasted image 20240122211429.png]]

И последнее, насколько я понял, упоминание "heuristics for exploration" - это про то, что надо перечислить, вышеописанные стратегии исследования, которые могут использоваться агентом, и их характерные черты.