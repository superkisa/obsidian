==Евгений, занимаюсь этим вопросом, думаю, до конца==
[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 119, целая глава про Exploration-exploitation trade-off, с подробнейшим объяснением, хотя вроде объяснять особо нечего).
Стр. 128, код для жадной эксплуатации без исследования вообще:
![[Pasted image 20240122185942.png]]
Суть в том, что агент всегда выбирает тот вариант, который дал какой-то положительный реворд. При этом он не проверяет другие варианты, а они могут быть и лучше. 

Стр. 130, код для рандомного исследования (не лучшая стратегия для исследования):
![[Pasted image 20240122190053.png]]
При этом агент просто рандомно выбирает действие, и возможно когда-то исследует всю среду, но это будет довольно долго и не эффективно.

Стр. 132, код для эпсилон-жадной стратегии (уже есть trade-off):
![[Pasted image 20240122190223.png]]
Сначала агент действует больше рандомно чем жадно, и делает ставку на исследование, а когда уже "наберется опыта" - тогда плавно переходит к стратегии жадной стратегии, что логично, и неплохо работает.
Далее есть примеры с линейным или экспоненциальным затуханием эпсилон - думаю, это понятно, что эпсилон можно изменять, в процессе, и таким образом изменять соотношение эксплуатации-исследования - достаточно просто знать что они есть, код не особо сложнее.

Есть еще вид исследования - оптимистичная инициализация - когда значения Q-функции инициализируются не нулями, а единицами, и тогда получается, что агент при исследовании начинает не с того, что выбирает что-то из "самых плохих" вариантов, а наоборот считает, что все варианты прекрасны на первый взгляд, но в процессе исследования, значения Q-функции начинают уменьшаться, и становятся более реальными. Таким образом, применяется жадная стратегия, но из-за ненулевой инициализации агент имеет шанс проверить все варианты, пока не убедится, что какие-то из них хуже чем другие.
![[Pasted image 20240122191539.png]]

UCB - upper confidence bound strategy - 