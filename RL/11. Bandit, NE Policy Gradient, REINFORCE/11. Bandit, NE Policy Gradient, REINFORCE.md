###### Бандиты
[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 122, про бандитов:
![[Pasted image 20240124204236.png]]
Представляем себя возле стойки с игровыми автоматами, и понимаем, что мы можем понять где мы можем выиграть, только попробовав дернуть за ручку каждый автомат. 
![[Pasted image 20240124204358.png]]
В общем это одна из сред, в которых учатся наши агенты.

###### Policy Gradient
==Кто-нибудь смог разгадать, что значит NE в вопросе?==
Если забить на загадочные NE, то [[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | из книжки]] со страницы 364, про метод градиента политик:
![[Pasted image 20240124205958.png]]
Смысл в том, что для решения задач РЛ мы уже использовали различные методы для оценки ценностей состояний, с учетом действий или без учета. Но стратегические методы градиента политик идут другим путем, и сразу оптимизируют политику как функцию. Без учета ценностей (хотя будет еще и с учетом позже, но это другое). 
Метод градиента политик имеет преимущество в том, что политика представлена как непрерывная обучаемая функция. Это удобно для непрерывных и многомерных пространств действий. Также это хорошо для стохастических политик в частично наблюдаемых средах. Напомню, что в задачах РЛ бывает часто неполная информация о среде, ограниченная возможностями наблюдений. 
Интересно, что не смотря на стохастичность политики, она вполне сходится к детерминированному результату. Во многих средах также, невозможно, сложно, или имеет мало смысла оценивать функции ценности состояний, и в этих случаях полученная политика гораздо важнее.
Далее приводится пример описания нейросети, которая оптимизирует политику, с дискретными действиями. Полезный пример, для понимания. Он очень похож на DQN тем, что на вход сети такой же структуры, мы подаем также состояние из наблюдения. Но на выходе мы получаем не Q-значения для разных состояний, а логиты вероятностей действий из смоделированной политики:
![[Pasted image 20240124211955.png]]
![[Pasted image 20240124212026.png]]
![[Pasted image 20240124212114.png]]
![[Pasted image 20240124212147.png]]
Вывод основной формулы метода градиента (у нас в слайдах она выглядит немного по-другому, но смысл остается тем же):
![[Pasted image 20240124212348.png]]
![[Pasted image 20240124212417.png]]
Так ну с этим **ВСЕ ПОНЯТНО** идем дальше.
###### REINFORCE
Далее идет описание алгоритма REINFORCE:
![[Pasted image 20240124212723.png]]
Код, обязательный к разбору:
![[Pasted image 20240124213627.png]]
![[Pasted image 20240124213721.png]]
![[Pasted image 20240124213759.png]]
![[Pasted image 20240124213830.png]]
![[Pasted image 20240124213855.png]]
Алгоритм REINFORCE хорошо работает в простых средах и дает гарантии сходимости. Но, так как для вычисления градиента используется полная выгода Монте-Карло, у этого алгоритма возникает проблема с дисперсией. ==Далее идет описание методов борьбы с этой проблемой, известных как базовый градиент политик (vanilla policy gradient) или REINFORCE с направляющей (REINFORCE with baseline). Наверное они нам не нужны?==

