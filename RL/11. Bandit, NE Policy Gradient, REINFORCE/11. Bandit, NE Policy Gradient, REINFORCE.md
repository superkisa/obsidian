 
 
 ==надо перевести некоторые моменты с физтеховского на человеческий==

###### Бандиты
[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 122, про бандитов:
![[Pasted image 20240124204236.png]]
Представляем себя возле стойки с игровыми автоматами, и понимаем, что мы можем понять где мы можем выиграть, только попробовав дернуть за ручку каждый автомат. 
![[Pasted image 20240124204358.png]]
В общем это одна из сред, в которых учатся наши агенты.

###### Policy Gradient
==Кто-нибудь смог разгадать, что значит NE в вопросе?==
Если забить на загадочные NE, то [[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | из книжки]] со страницы 364, про метод градиента политик:
![[Pasted image 20240124205958.png]]
Смысл в том, что для решения задач РЛ мы уже использовали различные методы для оценки ценностей состояний, с учетом действий или без учета. Но стратегические методы градиента политик идут другим путем, и сразу оптимизируют политику как функцию. Без учета ценностей (хотя будет еще и с учетом позже, но это другое). 
Метод градиента политик имеет преимущество в том, что политика представлена как непрерывная обучаемая функция. Это удобно для непрерывных и многомерных пространств действий. Также это хорошо для стохастических политик в частично наблюдаемых средах. Напомню, что в задачах РЛ бывает часто неполная информация о среде, ограниченная возможностями наблюдений. 
Интересно, что не смотря на стохастичность политики, она вполне сходится к детерминированному результату. Во многих средах также, невозможно, сложно, или имеет мало смысла оценивать функции ценности состояний, и в этих случаях полученная политика гораздо важнее.
Далее приводится пример описания нейросети, которая оптимизирует политику, с дискретными действиями. Полезный пример, для понимания. Он очень похож на DQN тем, что на вход сети такой же структуры, мы подаем также состояние из наблюдения. Но на выходе мы получаем не Q-значения для разных состояний, а логиты вероятностей действий из смоделированной политики:
![[Pasted image 20240124211955.png]]
![[Pasted image 20240124212026.png]]
![[Pasted image 20240124212114.png]]
![[Pasted image 20240124212147.png]]
Вывод основной формулы метода градиента (у нас в слайдах она выглядит немного по-другому, но смысл остается тем же):
![[Pasted image 20240124212348.png]]
![[Pasted image 20240124212417.png]]
Так ну с этим **ВСЕ ПОНЯТНО** идем дальше.
###### REINFORCE
Саттон стр 379:
![[Pasted image 20240126083338.png]]
Далее идет описание алгоритма REINFORCE:
![[Pasted image 20240124212723.png]]
Код, обязательный к разбору:
![[Pasted image 20240124213627.png]]
![[Pasted image 20240124213721.png]]
![[Pasted image 20240124213759.png]]
![[Pasted image 20240124213830.png]]
![[Pasted image 20240124213855.png]]
Алгоритм REINFORCE хорошо работает в простых средах и дает гарантии сходимости. Но, так как для вычисления градиента используется полная выгода Монте-Карло (в дальнейшем описано развитие этого алгоритма с ожидаемой выгодой), у этого алгоритма возникает проблема с дисперсией. ==Далее идет описание методов борьбы с этой проблемой, известных как базовый градиент политик VPG (vanilla policy gradient) или REINFORCE с направляющей (REINFORCE with baseline) - в вопросе нет, но в презах был. Наверное они нам не нужны?==


==чатик про policy gradient==
The main idea behind policy gradient methods in reinforcement learning is to directly optimize the policy of an agent to maximize expected cumulative rewards. Instead of estimating the value of each action or state, policy gradient methods work with the policy itself. Here's the main idea broken down:

### 1. Policy as a Strategy:

- **Policy:** In reinforcement learning, a **policy** is like a strategy or a set of rules that guides the agent's decision-making in an environment. It defines the probability distribution over actions given a certain state.

### 2. Objective: Maximize Expected Rewards:

- **Goal:** The primary goal in reinforcement learning is to find a policy that maximizes the expected cumulative rewards over time.

### 3. Policy Parameterization:

- **Parameterization:** Policies are often parameterized by a set of weights or parameters. These parameters determine the probabilities of taking different actions in various states.

### 4. Gradient Ascent:

- **Optimization:** Policy gradient methods use a gradient ascent approach. They iteratively adjust the policy parameters in the direction that increases the expected cumulative rewards.

- **Gradient:** The gradient of the expected cumulative reward with respect to the policy parameters indicates how the policy should be adjusted.

### 5. Learning from Experience:

- **Experience:** The agent interacts with the environment, takes actions according to its current policy, receives rewards, and learns from this experience.

- **Update:** After each interaction, the policy parameters are updated to increase the probability of actions that resulted in higher rewards.

### 6. Exploration-Exploitation Trade-Off:

- **Trade-Off:** Policy gradient methods inherently balance exploration (trying new actions to discover their effects) and exploitation (favoring known good actions to maximize rewards).

### 7. Stochastic Policies:

- **Stochasticity:** Policies are often stochastic, meaning they provide probabilities for each action. This stochasticity allows for exploration and accounts for uncertainty in the environment.

### 8. Reinforcement Signal:

- **Reinforcement Signal:** The feedback signal for updating the policy comes from the rewards obtained by the agent. The objective is to increase the likelihood of actions that lead to higher rewards.

### 9. Types of Policy Gradient Methods:

- **Vanilla Policy Gradient:** Basic form of policy gradient optimization.
  
- **Trust Region Policy Optimization (TRPO):** Adds constraints to control the size of policy updates.

- **Proximal Policy Optimization (PPO):** An extension of TRPO, making it more sample-efficient.

### 10. Application to Complex Problems:

- **Flexibility:** Policy gradient methods are particularly useful in problems with high-dimensional action spaces or when the optimal action is not clear-cut.

- **Deep Reinforcement Learning:** They are often employed in conjunction with deep neural networks, forming deep policy gradient methods.

The main idea is to directly learn a policy that, over time, guides the agent to take actions that lead to higher expected rewards. Policy gradient methods have been successful in a wide range of applications, including robotic control, game playing, and natural language processing.
