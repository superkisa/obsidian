[[Аббревиатуры]]
[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Книжка "Грокаем глубокое обучение с подкреплением"]]
[[Miguel_Morales_-_Grokking_Deep_Reinforcement_Learning-Manning_Publications_2020.pdf | Оригинал на английском]] (конечно же лучше качеством и без ошибок переводчиков)
[[BartoSutton.pdf | Книжка Барто-Саттона]], классика RL, сложная, но крутая
[[Обучение_с_подкреплением_Введение_2_е_издание_2020_Ричард_С_Саттон.pdf | Перевод на русский тож есть]]

[[Theoretical minimum]]:
1. Markov decision process and its properties. Reward, discounted reward.
2. What is a Q-function and a Value-function? Relationship between them.
3. How can RL be applied to NLP or CV tasks?
4. What is an exception to exploration tradeoff?
5. Value-based vs. Policy based methods (general idea)
6. What is the difference between model-based and model-free RL?

Exam program:
1. [[1. Cross-entropy method (tabular and approximate case) | Cross-entropy method (tabular and approximate case).]]
2. [[2. Value-based RL - state value and state-action value functions. Value-based policy iteration algorithm | Value-based RL: state value and state-action value functions. Value-based policy iteration algorithm]].
3. [[3. Value iteration algorithm | Value iteration algorithm]].
4. [[4. Between them, Monte Carlo vs TD update questions. Q-leaning algorithm| Between them. Monte Carlo vs TD update questions. Q-leaning algorithm]]
5. [[5. Model-free RL - Bellman-expectation equality, algorithms | Model-free RL: Bellman-expectation equality, algorithms]].
6. [[6. SARSA, Expected value SARSA, Q-Learning, DQN, training details. On-policy vs Off-policy| SARSA, Expected value SARSA, Q-Learning, DQN, training details. On-policy vs. Off-policy]]
7. [[7. Policy-based methods. SARSA algorithm | Policy-based methods. SARSA algorithm]]
8. [[8. Approximate Value-Based and Policy-Based Methods (without policy baselines, with policy gradient) | Approximate Value-Based and Policy-Based Methods (without policy baselines, with policy gradient)]].
9. [[9. RL Advanced Policy Gradient - actor-critic algorithm | RL Advanced Policy Gradient: actor-critic algorithm]].
10. [[10. RL Solvers and Deviation-Soft ActorCritic, A3C, GAE | RL Solvers and Deviation-Soft ActorCritic, A3C, GAE]].
11. [[11. Bandit, NE Policy Gradient, REINFORCE | Bandit, NE Policy Gradient, REINFORCE]]
12. [[12. DDPG, TD3 in RL algorithms | DDPG, TD3 in RL algorithms]].
13. [[13. Sampling. Exploration. Exploration strategies - greedy, UCB, Thompson sampling, heuristics for exploration | Sampling. Exploration. Exploration strategies: greedy, UCB, Thompson sampling, heuristics for exploration]].
14. [[14. Planning - Monte-Carlo tree search, metrics for exploration | Planning: Monte-Carlo tree search, metrics for exploration.]]
15. [[15. Example of usage exploration algorithms in recommendation systems | Example of usage exploration algorithms in recommendation systems]].
16. [[16. Convergence of policy-based and value-based iterations. Difference between them | Convergence of policy-based and value-based iterations. Difference between them.]]


