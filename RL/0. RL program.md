

Exam program:
1. [[1. Cross-entropy method (tabular and approximate case) | Cross-entropy method (tabular and approximate case).]]
2. [[2. Value-based RL - state value and state-action value functions. Value-based policy iteration algorithm | Value-based RL - state value and state-action value functions. Value-based policy iteration algorithm]].
3. Value iteration algorithm.
4. Between them. Monte Carlo vs TD update questions. Q-leaning algorithm
5. Model-free RL: Bellman-expectation equality, algorithms.
6. SARSA, Expected Q-Learning, DQN, training details. On-policy vs. Off-policy
7. Policy-based methods. SARSA algorithm
8. Approximate Value-Based and Policy-Based Methods (without policy baselines, with policy gradient).
9. RL Advanced Policy Gradient: actor-critic algorithm.
10. RL Solvers and Deviation-Soft ActorCritic, A3C, GAE.
11. Bandit, NE Policy Gradient, REINFORCE
12. DDPG, TD3 in RL algorithms.
13. Sampling. Exploration. Exploration strategies: greedy, UCB, Thompson sampling, heuristics for exploration.
14. Planning: Monte-Carlo tree search, metrics for exploration.

>[!info]- То что было:
>Theoretical minimum:  
>- RL problem statement (environment, agent, etc., abstraсtions). Markov  
>decision process and its properties. Reward, discounted reward.  
>- What is a Q-function and a Value-function? Relationship between them.  
>- How can RL be applied to NLP or CV tasks?  
>- What is an exploration-exploitation tradeoff?  
>- What is the difference between model-based and model-free RL?  
>- Value-based vs. Policy based methods (general idea)  
>Program:  
>1. [[1. Cross-entropy method (tabular and approximate case)]].  
2. [[2. Value-based RL state value and state-action value functions. Relationship between them. Bellman expectation and optimality equations]]
3. [[3. Value iteration algorithm]]  
4. [[4. Policy iteration algorithm]].  
5. [[5. Model-free RL Monte-Carlo vs. TD updates. Q-Learning algorithm]].  
6. [[6. SARSA, Expected-Value SARSA algorithms. On-policy vs. Off-policy methods]].  
7. Approximate Q-Learning. DQN, training details.  
8. Policy-based methods. REINFORCE algorithm (without  
baselines, with derivation).  
9. Baselines in Policy Gradient. Actor-critic algorithm.  
10. Advanced Policy Gradient: A2C, A3C, GAE.  
11. RL for NLP and CV. Self-critical sequence training algorithm.  
12. Exploration in RL. Exploration strategies: eps-greedy, UCB, Thompson  
sampling. Metrics for exploration.  
13. DDPG, TD3 algorithms.  
14. Planning; Monte-Carlo tree search.