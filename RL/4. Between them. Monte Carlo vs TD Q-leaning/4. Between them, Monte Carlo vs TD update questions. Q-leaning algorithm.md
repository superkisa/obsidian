[3-я лекция](https://youtu.be/aGsLzQla3nk?si=ImcSphYejsIy3Eou)

==вопросы==

![[Pasted image 20240121204648.png]]
![[Pasted image 20240121204701.png]]
![[Pasted image 20240121204712.png]]
![[Pasted image 20240121204728.png]]
![[Pasted image 20240121204745.png]]
![[Pasted image 20240121204758.png]]
![[Pasted image 20240121204608.png]]

[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 157, это гораздо подробнее описано, но немного в другой последовательности. Эти методы рассматриваются сначала для итерационного вычисления значений Value и переход к Q-learning делается позже. Но для нас сути это не меняет, т.к. нам надо же не конкретные формулы запомнить (хотя если хотите, они есть на слайдах тоже), а понять смысл, плюсы-минусы методов.
###### Суть метода Монте-Карло:
![[Pasted image 20240123035144.png]]
![[Pasted image 20240123035415.png]]
![[Pasted image 20240123040231.png]]
![[Pasted image 20240123040449.png]]
![[Pasted image 20240123040629.png]]
![[Pasted image 20240123040700.png]]
![[Pasted image 20240123040729.png]]
Если кратко - то генерятся много траекторий, от одной политики. Среда стохастическая, поэтому при одной политике получается много траекторий. Затем рассчитывается V ожидаемое для каждого эпизода.
Еще есть заморочка с тем, как считать V в тех состояниях, в которых уже были. Можно использовать то значение, которое уже вычислено в первый визит, или пересчитывать каждый визит:
![[Pasted image 20240123043312.png]]
Далее идет код функций, которые нужны для дальнейшего кода методов.
Вычисление всех альфа заранее:
![[Pasted image 20240123043503.png]]
Создание траектории из политики, по запросу:
![[Pasted image 20240123043545.png]]
Функция, в которой полностью описан метод Монте-Карло:
![[Pasted image 20240123043653.png]]
![[Pasted image 20240123043726.png]]
![[Pasted image 20240123043822.png]]
![[Pasted image 20240123043857.png]]

С Монте-Карло на этом все, переходим к 
###### Методу временных разностей (temporal-difference, TD)
![[Pasted image 20240123050445.png]]
![[Pasted image 20240123050643.png]]
![[Pasted image 20240123051341.png]]
Уравнения:
![[Pasted image 20240123051429.png]]
![[Pasted image 20240123051507.png]]
![[Pasted image 20240123051602.png]]
Код:
![[Pasted image 20240123053254.png]]
![[Pasted image 20240123053321.png]]
![[Pasted image 20240123053504.png]]
Далее выделены отличия TD от MC:
![[Pasted image 20240123053656.png]]
![[Pasted image 20240123053729.png]]
![[Pasted image 20240123053841.png]]
![[Pasted image 20240123053855.png]]

###### Перейдем теперь к "Between them"
Суть в том, что методы МС и TD - это две крайности. Метод МС прогоняет агента через все действия в политике до завершения эпизода. А в методе TD другая крайность - агент взаимодействует со средой только один раз. Оказывается что между MC и TD есть целый спектр алгоритмов, в которых мы выполняем бутстрап не по одному действию, а по двум, трем, четырем, и т.д. Вместо одного шага, как в случае с TD, или целого эпизода, как в MC, мы можем вычислять функции ценности с использованием $n$ шагов, абстрагируясь от $n$. Этот способ называется *$n$-шаговым методом на основе TD*: он выполняет бутстрап на протяжении $n$ шагов. Интересно, что промежуточное значение $n$ часто работает лучше, чем любое из крайних значений.
![[Pasted image 20240123055145.png]]
Код:
![[Pasted image 20240123055232.png]]
![[Pasted image 20240123055303.png]]
![[Pasted image 20240123055351.png]]
![[Pasted image 20240123055411.png]]
![[Pasted image 20240123055442.png]]
Далее идет объяснение метода *прогнозирования прямого (и обратного тоже) обзора* TD($\lambda$), но, думаю он нам не нужен. Суть его в том чтобы не выбирать $n$ вручную, а объединить все $n$-шаговые и одношаговые обновления с экспоненциально затухающим коэффициентом $\lambda$.
Отдельно еще добавляется механизм, который называется *следами приемлемости (eligibility traces)*. Но вроде это уже совсем лишнее для нашего курса.
Итог по MC и TD:
![[Pasted image 20240123060626.png]]
![[Pasted image 20240123203543.png]]
###### Q-leaning algorithm 
В книге этот алгоритм идет вместе с SARSA, поэтому, встречается сравнение. Начинаю со стр. 208:
![[Pasted image 20240123073005.png]]
![[Pasted image 20240123073019.png]]
![[Pasted image 20240123073120.png]]
![[Pasted image 20240123073140.png]]
В последнем уравнении собственно все видно, что Q-learning строится на итеративном обновлении текущего Q-значения на основании максимума Q-значения в следующем состоянии. 
Код агента Q-learning:
![[Pasted image 20240123073311.png]]
![[Pasted image 20240123073330.png]]
![[Pasted image 20240123073359.png]]
![[Pasted image 20240123073426.png]]
В результате применения этого алгоритма получаем оптимальную политику, оптимальные Q-значения, и V-значения.
