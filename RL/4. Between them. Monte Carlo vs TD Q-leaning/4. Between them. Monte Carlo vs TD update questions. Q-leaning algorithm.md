==Евгений, занимаюсь этим вопросом==

[3-я лекция](https://youtu.be/aGsLzQla3nk?si=ImcSphYejsIy3Eou)
![[Pasted image 20240121204648.png]]
![[Pasted image 20240121204701.png]]
![[Pasted image 20240121204712.png]]
![[Pasted image 20240121204728.png]]
![[Pasted image 20240121204745.png]]
![[Pasted image 20240121204758.png]]
![[Pasted image 20240121204608.png]]

[[Грокаем_глубокое_обучение_с_подкреплением_NdMAJD.pdf | Из книжки]] со страницы 157, это гораздо подробнее описано, но немного в другой последовательности. Эти методы рассматриваются сначала для итерационного вычисления значений Value и переход к Q-learning делается позже. Но для нас сути это не меняет, т.к. нам надо же не конкретные формулы запомнить (хотя если хотите, они есть на слайдах же), а понять смысл, плюсы-минусы методов.
Суть метода Монте-Карло:
![[Pasted image 20240123035144.png]]
![[Pasted image 20240123035415.png]]
![[Pasted image 20240123040231.png]]
![[Pasted image 20240123040449.png]]
![[Pasted image 20240123040629.png]]
![[Pasted image 20240123040700.png]]
![[Pasted image 20240123040729.png]]
Если кратко - то генерятся много траекторий, от одной политики. Среда стохастическая, поэтому при одной политике получается много траекторий. Затем рассчитывается V ожидаемое для каждого эпизода.
Еще есть заморочка с тем, как считать V в тех состояниях, в которых уже были. Можно использовать то значение, которое уже вычислено в первый визит, или пересчитывать каждый визит:
![[Pasted image 20240123043312.png]]
Далее идет код функций, которые нужны для дальнейшего кода методов.
Вычисление всех альфа заранее:
![[Pasted image 20240123043503.png]]
Создание траектории из политики, по запросу:
![[Pasted image 20240123043545.png]]
Функция, в которой полностью описан метод Монте-Карло:
![[Pasted image 20240123043653.png]]
![[Pasted image 20240123043726.png]]
![[Pasted image 20240123043822.png]]
![[Pasted image 20240123043857.png]]
