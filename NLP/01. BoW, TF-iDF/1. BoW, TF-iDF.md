==чатик==
### Bag of Words (BoW) Model:

The Bag of Words (BoW) model is a common and simple representation of text in Natural Language Processing (NLP). It treats a document as an unordered set of words, disregarding grammar and word order but keeping track of the frequency of each word. Here's how the BoW model works:

1. **Tokenization:** The first step is to break down a document or a piece of text into individual words or tokens.

2. **Vocabulary Creation:** Create a vocabulary, which is a list of all unique words present in the entire corpus (collection of documents).

3. **Word Counting:** Represent each document as a vector, where each element corresponds to a word from the vocabulary. The value of each element is the count of how many times the word appears in the document.

4. **Sparse Representation:** Since most documents use only a small subset of the vocabulary, the resulting vectors are usually sparse, containing many zero elements.

The BoW model is simple and efficient, but it ignores the order and structure of the words in a document. It is widely used in text classification, sentiment analysis, and information retrieval.

==из лекции==
The longer the sentence => the higher its norm

**Problems:**
- No information about words order
- Word vectors are huge and very sparse
- Word vectors are not normalised
- Same words can take different forms


### TF-IDF (Term Frequency-Inverse Document Frequency):

TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents, often called a corpus. It is composed of two parts:

1. **Term Frequency (TF):** Measures how often a term (word) appears in a document. It is calculated as the ratio of the number of times a term occurs in a document to the total number of words in that document. 

$$ \text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d} $$

2. **Inverse Document Frequency (IDF):** Measures the importance of a term across a collection of documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.

   $$ \text{IDF}(t, D) = \log\left(\frac{\text{Total number of documents in the corpus } |D|}{\text{Number of documents containing term } t + 1}\right) $$

The TF-IDF score for a term in a document is then calculated by multiplying the term's TF and IDF values:

$$ \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D) $$

In NLP, TF-IDF is used to determine the importance of words in a document relative to a larger corpus. It is commonly used for document retrieval, information retrieval, and text mining tasks. Words with higher TF-IDF scores are considered more important to the document.
