==чатик==
### Bag of Words (BoW) Model:

The Bag of Words (BoW) model is a common and simple representation of text in Natural Language Processing (NLP). It treats a document as an unordered set of words, disregarding grammar and word order but keeping track of the frequency of each word. Here's how the BoW model works:

1. **Tokenization:** The first step is to break down a document or a piece of text into individual words or tokens.

2. **Vocabulary Creation:** Create a vocabulary, which is a list of all unique words present in the entire corpus (collection of documents).

3. **Word Counting:** Represent each document as a vector, where each element corresponds to a word from the vocabulary. The value of each element is the count of how many times the word appears in the document.

4. **Sparse Representation:** Since most documents use only a small subset of the vocabulary, the resulting vectors are usually sparse, containing many zero elements.

The BoW model is simple and efficient, but it ignores the order and structure of the words in a document. It is widely used in text classification, sentiment analysis, and information retrieval.

"Bag of words" (мешок слов) - это простая и распространенная модель представления текстовых данных в машинном обучении и обработке естественного языка. Эта модель преобразует текст в неупорядоченный набор слов, игнорируя порядок слов в предложении, и фокусируется только на их частоте в тексте. 

Процесс создания "мешка слов" включает в себя несколько шагов:

1. **Токенизация:** Текст разбивается на отдельные слова или токены. Токены могут быть просто словами, или более сложными единицами, такими как биграммы (пары слов) или триграммы (тройки слов).

2. **Построение словаря:** Создается словарь, который содержит все уникальные слова (или токены), встречающиеся в текстах. Каждое слово получает уникальный идентификатор.

3. **Построение векторов признаков:** Для каждого текста строится вектор, который представляет его в терминах частоты появления слов из словаря. Каждая позиция в векторе соответствует слову из словаря, а значение в этой позиции указывает на количество раз, которое это слово встречается в тексте.

Пример:

Предположим, у нас есть два текста:

Текст 1: "Я люблю машинное обучение."
Текст 2: "Машинное обучение интересно."

Шаги построения "мешка слов":

1. Токенизация:
   - Текст 1: ["Я", "люблю", "машинное", "обучение"]
   - Текст 2: ["Машинное", "обучение", "интересно"]

2. Построение словаря:
   - {"Я", "люблю", "машинное", "обучение", "интересно"}

3. Построение векторов признаков:
   - Текст 1: [1, 1, 1, 1, 0]
   - Текст 2: [0, 0, 1, 1, 1]

Таким образом, каждый текст представлен в виде вектора, который можно использовать в алгоритмах машинного обучения. Однако, важно отметить, что эта модель теряет информацию о порядке слов в тексте и не учитывает семантические отношения между ними.

[Ссылка вики "Мешок слов"](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%88%D0%BE%D0%BA_%D1%81%D0%BB%D0%BE%D0%B2)
Однако, частотность слов не всегда являются наилучшей характеристикой текста. Распространенные слова, такие как «и», «в», «но», почти всегда имеют наивысшую частотность в тексте. Таким образом, высокая частотность появления слова не обязательно означает, что это слово является более важным по смыслу. Популярным способом решения этой проблемы является мера TF-IDF, которая нормализует частотность слова в документе делением на частотность этого слова во всей коллекции документов.

==из лекции==
The longer the sentence => the higher its norm

**Problems:**
- No information about words order
- Word vectors are huge and very sparse
- Word vectors are not normalised
- Same words can take different forms


### TF-IDF (Term Frequency-Inverse Document Frequency):

TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents, often called a corpus. It is composed of two parts:

1. **Term Frequency (TF):** Measures how often a term (word) appears in a document. It is calculated as the ratio of the number of times a term occurs in a document to the total number of words in that document. 

$$ \text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d} $$

2. **Inverse Document Frequency (IDF):** Measures the importance of a term across a collection of documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.

   $$ \text{IDF}(t, D) = \log\left(\frac{\text{Total number of documents in the corpus } |D|}{\text{Number of documents containing term } t + 1}\right) $$

The TF-IDF score for a term in a document is then calculated by multiplying the term's TF and IDF values:

$$ \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D) $$

In NLP, TF-IDF is used to determine the importance of words in a document relative to a larger corpus. It is commonly used for document retrieval, information retrieval, and text mining tasks. Words with higher TF-IDF scores are considered more important to the document.
