 
https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html
https://www.youtube.com/watch?v=L8HKweZIOmg&t=760s

<u><b>We have:</b></u>
- an <u>input sequence</u> $x_1, x_2, \dots, x_m$ ,
- an <u>output sequence</u> $y_1, y_2, \dots, y_n$ (note that their lengths can be different).
<u><b>The task</u></b>: to find the <u>target sequence</u> that is <u>the most probable given the input</u>; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x)$.
So,  we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta)$.

<u>Encoder-decoder</u> is the standard modeling paradigm for sequence-to-sequence tasks. This framework consists of two components:
- <u>encoder</u> - reads source sequence and produces its representation;
- <u>decoder</u> - uses source representation from the encoder to generate the target sequence.

![[Pasted image 20240115005026.png]]

Sequence-to-sequence tasks can be modeled as <u>Conditional Language Models (CLM)</u>.

<u>High-level pipeline:</u>
- feed source and previously generated target words into a network;
- get vector representation of context (both source and previous target) from the networks decoder;
- from this vector representation, predict a probability distribution for the next token.
![[Pasted image 20240115010043.png]]
Vector representation of a text has some dimensionality $d$, but we need a vector of size $|V|$ (probabilities for $|V|$ tokens/classes). To get a $|V|$-sized vector from a $d$-sized, we can use a linear layer. Once we have a $|V|$-sized vector, all is left is to apply the softmax operation to convert the raw numbers into token probabilities.

![[Pasted image 20240115153212.png]]
- Encoder RNN: 
	- reads the source sentence;
	- final state is used as the initial state of the decoder RNN
- Decoder RNN:
	- 