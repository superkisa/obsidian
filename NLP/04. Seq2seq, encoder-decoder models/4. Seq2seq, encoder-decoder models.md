 
https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html

- https://www.youtube.com/watch?v=YCzL96nL7j0&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=16&t=921s
- chttps://www.youtube.com/watch?v=L8HKweZIOmg&t=760s

<u><b>We have:</b></u>
- an <u>input sequence</u> $x_1, x_2, \dots, x_m$ ,
- an <u>output sequence</u> $y_1, y_2, \dots, y_n$ (note that their lengths can be different).
<u><b>The task</u></b>: to find the <u>target sequence</u> that is <u>the most probable given the input</u>; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x)$.
So,  we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta)$.

<u>Encoder-decoder</u> is the standard modeling paradigm for sequence-to-sequence tasks. This framework consists of two components:
- <u>encoder</u> - reads source sequence and produces its representation;
- <u>decoder</u> - uses source representation from the encoder to generate the target sequence.

![[Pasted image 20240115005026.png]]

Sequence-to-sequence tasks can be modeled as <u>Conditional Language Models (CLM)</u>.

<u>High-level pipeline:</u>
- feed source and previously generated target words into a network;
- get vector representation of context (both source and previous target) from the networks decoder;
- from this vector representation, predict a probability distribution for the next token.
![[Pasted image 20240115010043.png]]
Vector representation of a text has some dimensionality $d$, but we need a vector of size $|V|$ (probabilities for $|V|$ tokens/classes). To get a $|V|$-sized vector from a $d$-sized, we can use a linear layer. Once we have a $|V|$-sized vector, all is left is to apply the softmax operation to convert the raw numbers into token probabilities.

![[Pasted image 20240115153212.png]]
- Encoder RNN: 
	- reads the source sentence;
	- final state is used as the initial state of the decoder RNN
- Decoder RNN:
	- generates the target sentence based on encoder output

#### Long-Short-Term Mamory
![[Pasted image 20240115215544.png]]
- <font color="#008ECC">Sigmoid activation function</font> $f(x)=\frac{e^x}{e^x+1}$
	 ![[Pasted image 20240115214814.png]]
- <font color="#EB9605">Tanh activation function</font> $f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
	![[Pasted image 20240115215114.png]]

#### Sequence-to-Sequence (seq2seq)
- <u><b>Embedding Layer</u></b>
	![[Pasted image 20240115225511.png]]

- <u><b>Encoder</u></b>
	![[Pasted image 20240115225910.png]]
	- In practice to have **more Weights and Biases** to fit the model to our data, often <u><b>additional LSTM cells are added to the input</u></b>.
		![[Pasted image 20240115230357.png]]
		<u>Here</u>: 2 embedding values for the word "let's" are used as the input values for 2 different LSTM cells and these  LSTM cells have their own, seperate, sets of Weights and Baises. When we unroll them for the word "go", the original LSTM cell reuses it's set of Weights and Baises and the new LSTM cell reuses it's separate set of Weights and Baises (<u><b>they go parallrel</u></b>).
	- To add more Weights and Biases to fit our model to our data, people add <u><b>additional layers of LSTMs</u></b>.
		

![[Pasted image 20240115223418.png]]
