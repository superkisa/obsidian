[Long Short-Term Memory (LSTM), Clearly Explained](https://www.youtube.com/watch?v=YCzL96nL7j0&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=16&t=921s)
[Seq2Seq Encoder-Decoder NN, Clearly Explained](https://www.youtube.com/watch?v=L8HKweZIOmg&t=760s)

[Источник](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html):

<u><b>We have:</b></u>
- an <u>input sequence</u> $x_1, x_2, \dots, x_m$ ,
- an <u>output sequence</u> $y_1, y_2, \dots, y_n$ (note that their lengths can be different).
<u><b>The task</u></b>: to find the <u>target sequence</u> that is <u>the most probable given the input</u>; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x)$.
So,  we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta)$.

<u>Encoder-decoder</u> is the standard modeling paradigm for sequence-to-sequence tasks. This framework consists of two components:
- <u>encoder</u> - reads source sequence and produces its representation;
- <u>decoder</u> - uses source representation from the encoder to generate the target sequence.

![[Pasted image 20240115005026.png]]

Sequence-to-sequence tasks can be modeled as <u>Conditional Language Models (CLM)</u>.

<u>High-level pipeline:</u>
- feed source and previously generated target words into a network;
- get vector representation of context (both source and previous target) from the networks decoder;
- from this vector representation, predict a probability distribution for the next token.
![[Pasted image 20240115010043.png]]
Vector representation of a text has some dimensionality $d$, but we need a vector of size $|V|$ (probabilities for $|V|$ tokens/classes). To get a $|V|$-sized vector from a $d$-sized, we can use a linear layer. Once we have a $|V|$-sized vector, all is left is to apply the softmax operation to convert the raw numbers into token probabilities.

![[Pasted image 20240115153212.png]]
- Encoder RNN: 
	- reads the source sentence;
	- final state is used as the initial state of the decoder RNN
- Decoder RNN:
	- generates the target sentence based on encoder output

#### Long-Short-Term Memory
![[Pasted image 20240115215544.png]]
- <font color="#008ECC">Sigmoid activation function</font> $f(x)=\frac{e^x}{e^x+1}$
	 ![[Pasted image 20240115214814.png]]
- <font color="#EB9605">Tanh activation function</font> $f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
	![[Pasted image 20240115215114.png]]

#### Sequence-to-Sequence (seq2seq)
- <u><b>Embedding Layer</u></b>
	![[Pasted image 20240115225511.png]]

- <u><b>Encoder</u></b>
	![[Pasted image 20240115225910.png]]
	- In practice to have **more Weights and Biases** to fit the model to our data, often <u><b>additional LSTM cells are added to the input</u></b>.
		![[Pasted image 20240115232500.png]]
		<u>Here</u>: 2 embedding values for the word "let's" are used as the input values for 2 different LSTM cells and these  LSTM cells have their own, seperate, sets of Weights and Baises. When we unroll them for the word "go", the original LSTM cell reuses it's set of Weights and Baises and the new LSTM cell reuses it's separate set of Weights and Baises (<u><b>they go parallrel</u></b>).
	- To add more Weights and Biases to fit our model to our data, people add <u><b>additional layers of LSTMs</u></b>.
		![[Pasted image 20240115232900.png]]
		<u>Here</u>: the output values (the short term memories or the hidden states) from the unrolled LSTM units in the first layer are used as the inputs to the unrolles LSTM units in the second layer. 
		Just like how both embedding values are used as inputs to both LSTM cells in the first layer, both outputs (**the short-term memories** or **hidden states**) from the each cell are used as inputs to both LSTM cells in the second layer.
	The last long and short term memories (the cell and hidden states) from both layers of the LSTM cells in the Encoder are called the <u><b>Context Vector</u></b>.

- <u><b>Decoder</u></b>
	We connect the long and short term memories (the cell and hidden states) that from the Context Vector to a new set of LSTMs that have 2 layers, and each layer has 2 cells. The Context Vector is used to initiallixe the long and short-term memories (the cell and hidden states) in the LSTMs in the Decoder. The input of LSTM cells in the first layer comes from an Embedding Layer (which creates embedding values for words in target sequens).
	Decoder does math in the 2 layers of LSTMs each with 2 LSTM cells and the 2 output values (the short term memories, or hidden states) from the top layer of LSTM cells are transformed by a Fully Connected Layer (2 inputs and N outputs for each token in a dictionary) $\Longrightarrow$ Softmax function that picks up the word.
	The Decoder doesn't stop until it outputs an EOS token 
	![[Pasted image 20240116000838.png]]
	<u>Sum up</u>: Comtext vector, created bu both layers of the Encoder'sunrolled LSTM cells, are used to initialixe the LSTMs in the Decoder. The input to the LSTMs comes from the output Word Embedding leyer that starts with EOS, but subsequantly uses whatever word was predicted by the Output Layer. Decoder will  keep predicting words until it predicts the EOS token, or hits some maximum output length.

![[Pasted image 20240115223418.png]]

<font size=20 color='red'>ТУТ ПРО ОБУЧЕНИЕ </font>
[Seq2Seq Encoder-Decoder NN, Clearly Explained](https://youtu.be/L8HKweZIOmg?si=GS_B4PlMjEbX6cEg&t=804)
<font size=20 color='red'>ДОКОНСПЕКТИТЬ (УСТАЛА)</font>

