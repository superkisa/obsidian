Статья с иллюстрациями на [англ](https://jalammar.github.io/illustrated-bert/) и на [рус](https://habr.com/ru/articles/487358/). Переносить сюда не вижу особого смысла.

BERT’s state-of-the-art performance is based on two things. First, novel pre-training tasks called [**Masked Language Model(MLM)**](https://jalammar.github.io/illustrated-bert/#:~:text=R%2Drated%20BERT.-,Masked%20Language%20Model,-%E2%80%9CWe%E2%80%99ll%20use%20transformer) and **Next Sentense Prediction (NSP)**. Second, a lot of data and compute power to train BERT.
