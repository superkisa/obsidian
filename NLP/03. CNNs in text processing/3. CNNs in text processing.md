https://lena-voita.github.io/nlp_course/text_classification.html#nn_models_cnn

<font color="green" size=30>Проверить тому, кто смотрел лекцию</font>

CNN in text preprossesing using, for example, for tasks of finding some patterns througth the text (the task of classifying positive/negative sentiment of the text). 
Following the intuition above, <u>we want to detect some patterns, but we don't care much where exactly these patterns are</u>. This behavior is implemented with <u><b>two layers</u></b>:
- **convolution**: finds matches with patterns (as the cat head we saw above);
- **pooling**: aggregates these matches over positions (either locally or globally).
![[Pasted image 20240114201322.png]]
Differently from images, texts have only one dimension: here a *convolution is one-dimensional.
![[cnn_filter_reads_text.gif]]

#### Convolution is a Linear Operation Applied to Each Window
![[Pasted image 20240114203337.png]]
A convolution is a linear layer (followed by a non-linearity) applied to each input window.
- $(x_1, ... , x_n)$ - representations of the input words, $x_i\in \mathbb{R}^d$;
- $d$ (input channels) - size of an input embedding;
- $k$ (kernel size) - the length of a convolution window (on the illustration, $k=3$);
- $m$ (output channels) - number of convolution filters (i.e., number of channels produced by the convolution).
Then a convolution is a linear layer $W\in\mathbb{R}^{(k\cdot d)\times m}$. For a $k$-sized window $(x_i, ... , x_{i+k-1})$, the convolution takes the concatenation of these vectors
$$u_i = [x_i, \dots x_{i+k-1}]\in\mathbb{R}^{k\cdot d}$$
and multiplies by the convolution matrix:
$$F_i = u_i \times W.$$
Each Filter Extracts a Feature. A filter takes vector representations in a current window and transforms them linearly into a single feature. For a window $u_i = [x_i, \dots x_{i+k-1}]\in\mathbb{R}^{k\cdot d}$ a filter $f\in\mathbb{R}^{k\cdot d}$ computes dot product: $F_i^{(f)} = (f, u_i)$. The number $F_i^{(f)}$ (the extracted "feature") is a result of applying the filter $f$ to the window $(x_i, \dots x_{i+k-1})$.
![[several_filters_read.gif]]
![[Pasted image 20240115000347.png]]

#### Pooling Operation
A pooling layer <u>summarises the features in some region</u>. Pooling layers are used to reduce the input dimension, and, therefore, to <u>reduce the number of parameters used by the network</u>.

- <u><b>Max-pooling</u></b> takes maximum over each dimension, i.e. takes the maximum value of each feature.
![[Pasted image 20240115001001.png]]

- <u><b>Mean-pooling</u></b> works similarly but computes mean over each feature instead of maximum.

Pooling also has the <u>stride parameter</u>, and the most common approach is to use pooling with <u>non-overlapping windows</u>.
![[Pasted image 20240115001853.png]]

- <u><b>Pooling</u></b> is applied over features in each window independently.

- <u><b>Global pooling</u></b> performs over the whole input. For texts, global pooling is often used to get a single vector representing the whole text; such global pooling is called max-over-time pooling, where the "time" axis goes from the first input token to the last.
![[Pasted image 20240115002234.png]]

#### Convolutional Neural Networks for Text Classification

![[Pasted image 20240115002752.png]]
Specifically, after the convolution, we use <u>global-over-time pooling</u>. This is the key operation: it allows to compress a text into a single vector. The model itself can be different, but at some point it has to use the global pooling to compress input in a single vector.


![[Pasted image 20240115002925.png]]
Instead of picking one kernel size for your convolution, you can use <u>several convolutions with different kernel sizes</u>. The recipe is simple: 
- apply each convolution to the data, 
- add non-linearity and global pooling after each of them, 
- concatenate the results (on the illustration, non-linearity is omitted for simplicity). 
$\Longrightarrow$ get vector representation of the data which is used for classification.


![[Pasted image 20240115003446.png]]
Instead of one layer, you can <u>stack several blocks convolution+pooling on top of each other</u>. After several blocks, you can apply another convolution, but with global pooling this time.
Such multi-layered convolutions can be useful when <u>your texts are very long</u>; for example, if your model is character-level (as opposed to word-level).