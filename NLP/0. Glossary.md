**[Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#:~:text=Attention%3A%20A%20High%2DLevel%20View)** - at different steps, let a model "focus" on different parts of the input.
**Corpus** - a set of all texts available at the training stage
**Stop words** - used everywhere but are useless, i.e. articles (do not affect the meaning). these are normally exluded from the text at pre-processing
**[Token](https://huggingface.co/docs/transformers/v4.36.1/en/glossary#token)** - is a smallest piece of information that cannot be split further / a part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a punctuation symbol (элементарная единица текста, которую модель рассматривает в процессе обработки. Токены могут представлять собой отдельные слова, подслова (например, n-граммы или морфемы), символы или другие лингвистические элементы, в зависимости от выбранного уровня разбиения текста).
**[Tokenization](https://huggingface.co/docs/transformers/tokenizer_summary)** - extracting tokens from corpus of texts
**Word embeddings** - in simple words, are numerical representations of words that capture their meanings based on how they relate to other words in a given context. Imagine each word as a point in a multi-dimensional space, where words with similar meanings are closer to each other. These embeddings allow computers to understand and work with the semantic relationships between words, enabling more accurate language processing in tasks like language translation, sentiment analysis, and information retrieval.
- Word embeddings are numerical representations of words in a continuous vector space, designed to capture semantic relationships between words. Unlike traditional methods that represent words as discrete symbols, word embeddings place words in a high-dimensional space where the proximity of vectors reflects the semantic similarity of words.

**Perplexity score:** Perplexity is a measure used in natural language processing (NLP) to evaluate the performance of a language model. It provides an indication of how well the model predicts a sequence of words.

Here's a breakdown of perplexity:

1. **Definition:**
   - Perplexity is a statistical measure that quantifies how well a probability distribution or probability model predicts a sample. In the context of language modeling, perplexity measures how well a language model predicts a given sequence of words.

2. **Calculation:**
   - Perplexity is calculated using the probability assigned by the language model to a test dataset. For a given sequence of words \(W\), the perplexity (\(PP\)) is calculated as \(PP(W) = P(w_1, w_2, ..., w_N)^{-1/N}\), where \(N\) is the number of words in the sequence.

3. **Interpretation:**
   - Lower perplexity scores indicate better performance. A lower perplexity suggests that the model assigns higher probabilities to the actual words in the sequence, indicating that it is more certain about its predictions.

4. **Relation to Probability:**
   - Perplexity is inversely proportional to the probability of the test dataset. Higher probabilities lead to lower perplexity scores.

5. **Use in Language Modeling:**
   - In the context of language modeling, perplexity is often used during the training and evaluation of models. During training, the goal is to minimize perplexity by adjusting the model parameters to improve the likelihood of generating the training data. In evaluation, perplexity serves as a measure of how well the model generalizes to unseen data.

6. **Example:**
   - If a language model has a perplexity of 50 on a test set, it implies that, on average, the model is as uncertain as if it had to choose uniformly among 50 equiprobable words at each position in the sequence.

In summary, perplexity is a valuable metric for assessing the effectiveness of language models. It helps quantify how well a model captures the underlying patterns and distributions in a given sequence of words. Lower perplexity values are indicative of better language models.