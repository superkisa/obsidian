**[Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#:~:text=Attention%3A%20A%20High%2DLevel%20View)** - at different steps, let a model "focus" on different parts of the input.
**Corpus** - a set of all texts available at the training stage
**Stop words** - used everywhere but are useless, i.e. articles (do not affect the meaning). these are normally exluded from the text at pre-processing
**[Token](https://huggingface.co/docs/transformers/v4.36.1/en/glossary#token)** - is a smallest piece of information that cannot be split further / a part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a punctuation symbol (элементарная единица текста, которую модель рассматривает в процессе обработки. Токены могут представлять собой отдельные слова, подслова (например, n-граммы или морфемы), символы или другие лингвистические элементы, в зависимости от выбранного уровня разбиения текста).
**[Tokenization](https://huggingface.co/docs/transformers/tokenizer_summary)** - extracting tokens from corpus of texts
**Word embeddings** - in simple words, are numerical representations of words that capture their meanings based on how they relate to other words in a given context. Imagine each word as a point in a multi-dimensional space, where words with similar meanings are closer to each other. These embeddings allow computers to understand and work with the semantic relationships between words, enabling more accurate language processing in tasks like language translation, sentiment analysis, and information retrieval.
- Word embeddings are numerical representations of words in a continuous vector space, designed to capture semantic relationships between words. Unlike traditional methods that represent words as discrete symbols, word embeddings place words in a high-dimensional space where the proximity of vectors reflects the semantic similarity of words.
