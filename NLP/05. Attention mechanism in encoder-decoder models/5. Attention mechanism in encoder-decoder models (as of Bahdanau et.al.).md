[Seq2Seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
[Attention for Neural Networks, Clearly Explained](https://www.youtube.com/watch?v=PSs6nxngL6k)

==забираю вопрос, делаю, Евгений==

# Attention

## The Problem of Fixed Encoder Representation

Problem: Fixed source representation is suboptimal: (i) for the encoder, it is hard to compress the sentence; (ii) for the decoder, different information may be relevant at different steps.

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/bottleneck-min.png)

In the models we looked at so far, the encoder compressed the whole source sentence into a single vector. This can be very hard - the number of possible source sentences (hence, their meanings) is infinite. When the encoder is forced to put all information into a single vector, it is likely to forget something.

