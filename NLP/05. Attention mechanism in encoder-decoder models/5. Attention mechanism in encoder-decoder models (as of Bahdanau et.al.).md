![Attention for Neural Networks, Clearly Explained](https://www.youtube.com/watch?v=PSs6nxngL6k)

Source: [Seq2Seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#:~:text=to%20worse%20quality.-,Attention,-The%20Problem%20of):
# Attention

## The Problem of Fixed Encoder Representation

Problem: Fixed source representation is suboptimal: (i) for the encoder, it is hard to compress the sentence; (ii) for the decoder, different information may be relevant at different steps.

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/bottleneck-min.png)

In the models we looked at so far, the encoder compressed the whole source sentence into a single vector. This can be very hard - the number of possible source sentences (hence, their meanings) is infinite. When the encoder is forced to put all information into a single vector, it is likely to forget something.

Not only it is hard for the encoder to put all information into a single vector - this is also hard for the decoder. The decoder sees only one representation of source. However, at each generation step, different parts of source can be more useful than others. But in the current setting, the decoder has to extract relevant information from the same fixed representation - hardly an easy thing to do.

## Attention: A High-Level View

Attention was introduced in the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) to address the fixed representation problem.
[Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#:~:text=fixed%20representation%20problem.-,Attention,-%3A%20At%20different%20steps): At different steps, let a model "focus" on different parts of the input.

An attention mechanism is a part of a neural network. At each decoder step, it decides which source parts are more important. In this setting, the encoder does not have to compress the whole source into a single vector - it gives representations for all source tokens (for example, all RNN states instead of the last one).

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/general_scheme-min.png)

At each decoder step, attention

- receives attention input: a decoder state $\color{#b01277}{h_t}$ and all encoder states $\color{#7fb32d}{s_1}$,  $\color{#7fb32d}{s_2}$, ...,  $\color{#7fb32d}{s_m}$;
- computes attention scores  
    For each encoder state  $\color{#7fb32d}{s_k}$, attention computes its "relevance" for this decoder state $\color{#b01277}{h_t}$. Formally, it applies an attention function which receives one decoder state and one encoder state and returns a scalar value $\color{#307cc2}{score}(\color{#b01277}{h_t}\color{black}{,}\color{#7fb32d}{s_k}\color{#307cc2})$;
- computes attention weights: a probability distribution - softmax applied to attention scores;
- computes attention output: the weighted sum of encoder states with attention weights.

The general computation scheme is shown below.

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/computation_scheme-min.png)
** Note: Everything is differentiable - learned end-to-end!**

The main idea that a network can learn which input parts are more important at each step. Since everything here is differentiable (attention function, softmax, and all the rest), a model with attention can be trained end-to-end. You don't need to specifically teach the model to pick the words you want - the model itself will learn to pick important information.

Try to notice how attention weights change from step to step - which words are the most important at each step?

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/1-min.png)
>[!info]- All the slides are here in callout. 
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/2-min.png)
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/3-min.png)
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/4-min.png)
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/5-min.png)
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/6-min.png)
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/7-min.png)
> ![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/8-min.png)

## How to Compute Attention Score?
![|400](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_score_what_is_here-min.png)
In the general pipeline above, we haven't specified how exactly we compute attention scores. You can apply any function you want - even a very complicated one. However, usually you don't need to - there are several popular and simple variants which work quite well.
![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/score_functions-min.png)

The most popular ways to compute attention scores are:

- dot-product - the simplest method;
- bilinear function (aka "Luong attention") - used in the paper [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025);
- multi-layer perceptron (aka "Bahdanau attention") - the method proposed in the [original paper](https://arxiv.org/pdf/1409.0473.pdf).
## Model Variants: Bahdanau and Luong

When talking about the early attention models, you are most likely to hear these variants:

- Bahdanau attention - from the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) by Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio (this is the paper that introduced the attention mechanism for the first time);
- Luong attention - from the paper [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) by Minh-Thang Luong, Hieu Pham, Christopher D. Manning.

These may refer to either score functions of the whole models used in these papers. In this part, we will look more closely at these two model variants.

### Bahdanau Model

- encoder: bidirectional  
    To better encode each source word, the encoder has two RNNs, forward and backward, which read input in the opposite directions. For each token, states of the two RNNs are concatenated.
- attention score: multi-layer perceptron  
    To get an attention score, apply a multi-layer perceptron (MLP) to an encoder state and a decoder state.
- attention applied: between decoder steps  
    Attention is used between decoder steps: state $\color{#b01277}{h_{t-1}}$ is used to compute attention and its output $\color{#7fb32d}{c}^{\color{#b01277}{(t)}}$, and both $\color{#b01277}{h_{t-1}}$ and $\color{#7fb32d}{c}^{\color{#b01277}{(t)}}$ are passed to the decoder at step $t$.

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/bahdanau_model-min.png)

### Luong Model

While the [paper](https://arxiv.org/abs/1508.04025) considers several model variants, the one which is usually called "Luong attention" is the following:

- encoder: unidirectional (simple)
- attention score: bilinear function
- attention applied: between decoder RNN state $t$ and prediction for this step  
    Attention is used after RNN decoder step $t$ before making a prediction. State $\color{#b01277}{h_{t}}$ used to compute attention and its output $\color{#7fb32d}{c}^{\color{#b01277}{(t)}}$. Then $\color{#b01277}{h_{t}}$ is combined with $\color{#7fb32d}{c}^{\color{#b01277}{(t)}}$ to get an updated representation $\color{#b01277}{\tilde{h}_{t}}$, which is used to get a prediction.

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/luong_model-min.png)

## Attention Learns (Nearly) Alignment

Remember the motivation for attention? At different steps, the decoder may need to focus on different source tokens, the ones which are more relevant at this step. Let's look at attention weights - which source words does the decoder use?

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/bahdanau_examples-min.png)  
The examples are from the paper [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf).

From the examples, we see that attention learned (soft) alignment between source and target words - the decoder looks at those source tokens which it is translating at the current step.

Lena: "Alignment" is a term from statistical machine translation, but in this part, its intuitive understanding as "what is translated to what" is enough.
