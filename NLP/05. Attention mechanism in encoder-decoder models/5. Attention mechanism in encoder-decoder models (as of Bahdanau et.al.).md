[Seq2Seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
[Attention for Neural Networks, Clearly Explained](https://www.youtube.com/watch?v=PSs6nxngL6k)

==забираю вопрос, делаю, Евгений==

# Attention

## The Problem of Fixed Encoder Representation

Problem: Fixed source representation is suboptimal: (i) for the encoder, it is hard to compress the sentence; (ii) for the decoder, different information may be relevant at different steps.

![](https://lena-voita.github.io/resources/lectures/seq2seq/attention/bottleneck-min.png)

In the models we looked at so far, the encoder compressed the whole source sentence into a single vector. This can be very hard - the number of possible source sentences (hence, their meanings) is infinite. When the encoder is forced to put all information into a single vector, it is likely to forget something.

Not only it is hard for the encoder to put all information into a single vector - this is also hard for the decoder. The decoder sees only one representation of source. However, at each generation step, different parts of source can be more useful than others. But in the current setting, the decoder has to extract relevant information from the same fixed representation - hardly an easy thing to do.

[Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#:~:text=fixed%20representation%20problem.-,Attention,-%3A%20At%20different%20steps): At different steps, let a model "focus" on different parts of the input.

