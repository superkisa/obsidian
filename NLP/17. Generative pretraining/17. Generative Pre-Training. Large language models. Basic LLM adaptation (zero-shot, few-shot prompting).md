==в процессе, Игорь==

Генеративная предтренинговая подготовка. Большие языковые модели. Базовая адаптация LLM (zero-shot, few-shot prompting)



**Generative** **pre**-**trained** transformers (GPT) are a type of large language **model** (LLM) and a prominent framework for **generative** artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, **pre**-**trained** on large data sets of unlabelled text, and.
==Это типа только один из типов LLM, а какие еще бывают?== 
https://habr.com/ru/articles/599673/:
Откуда взялась GPT и какую задачу она решает
Языковое моделирование — это предсказание следующего слова (или куска слова) с учётом предыдущего контекста.
Подбор модификаций текста называется **«Prompt Engineering»**. Такая простая идея позволяет решать практически неограниченное количество задач. Именно поэтому многие считают GPT-3 подобием сильного искусственного интеллекта.

**Токенизация текста**
это библиотеки от [Hugging Face](https://huggingface.co/): `transformers`, `tokenizers`, `datasets`
процедура токенизации — преобразование текста в последовательность чисел
можно разбивать слова на наиболее общие части и представлять их полные версии как комбинации этих кусков (см. картинку). Такой способ токенизации называется BPE (Byte Pair Encoding). Но даже это иногда не самый оптимальный выбор. Чтобы сжать словарь ещё сильнее для обучения GPT OpenAI использовали byte-level BPE токенизацию. Эта модификация BPE работает не с текстом, а напрямую с его байтовым представлением.

Архитектура GPT
при генерации продолжения текста с помощью GPT происходит следующее:
1. Входной текст токенизируется в последовательность чисел (токенов).
    2. Список токенов проходит через Embedding Layer (линейный слой) и превращается в список эмбеддингов (очень похоже на word2vec).
    3. К каждому эмбеддингу прибавляется positional embedding
    4. Далее список эмбеддингов начинает своё путешествие через несколько одинаковых блоков (**Transformer Decoder Block**)
    5. **После того как список эмбеддингов пройдёт через последний блок, эмбеддинг, соответствующий последнему токену матрично умножается на всё тот же входной, но уже транспонированный Embedding Layer и после применения SoftMax получается распределение вероятностей следующего токена.**
    6. Из этого распределения выбираем следующий токен (например с помощью функции **argmax**).
    7. Добавляем этот токен к входному тексту и повторяем шаги 1-6.

![[Pasted image 20240117200341.png]]


### Transformer Decoder Block
Вот так выглядит главная структурная часть GPT: self-attention, нормализация, feed-forward и residual connections.
![[Pasted image 20240117200800.png]]

**Методы генерации текста**
языковая модель выдаёт распределение вероятностей следующего токена

Greedy Search

Самый простой способ — это **аргмаксная генерация** (greedy search), когда мы каждый раз выбираем токен, у которого максимальная вероятность - самый простой подход

Beam search
Чуть более сложный и качественный способ сэмплирования — это **beam search**. В этом случае на каждом шаге мы выбираем не только один самый вероятный токен, а сразу несколько (`beam-size`), и дальше продолжаем поиск для каждого из выбранных токенов. Таким образом мы разветвляем пути генерации, получая несколько вариантов сгенерированного текста. В итоге можно выбрать тот вариант, у которого лучшая перплексити (уверенность модели в реалистичности текста).
![[Pasted image 20240117202127.png]]





 **Файнтюнинг GPT**
Обучающий текст нарезается на случайные куски, которые составляются в последовательности из 1024 (2048 у GPT-3) токенов, разделяясь специальным `<|endoftext|>` символом. Во время обучения, модель учится предсказывать (классифицировать) каждый токен в последовательности один за другим при помощи [CrossEntropy Loss](https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e).

Так как входная последовательность всегда заполнена до конца, padding не используется. Но во время инференса, длина входного текста может быть произвольной, поэтому надо явно указывать чем паддить оставшиеся позиции. По дефолту используется тот же `<|endoftext|>`. 

В кастомных версиях GPT вышесказанное может модифицироваться. Например, в ruGPT3 гораздо больше специальных токенов: `<s>`, `<\s>`, `<pad>`, `<unk>`.



**Basic LLM adaptation (zero-shot, few-shot prompting)**
На высоком уровне обучение модели LLM включает в себя три этапа, т. е. сбор данных, обучение и оценку.

- **Сбор данных (Data Collection)** Первым шагом является сбор данных, которые будут использоваться для обучения модели. Данные могут быть собраны из различных источников, таких как Википедия, новостные статьи, книги, веб-сайты и т. д.
    
- **Обучение (Training)**: Затем данные проходят через обучающий конвейер, где они очищаются и предварительно обрабатываются перед тем, как поступить в модель для обучения. Процесс обучения обычно занимает много времени и требует больших вычислительных мощностей.
    
- **Оценка (Evaluation)**: Последний шаг — оценить производительность модели, чтобы увидеть, насколько хорошо она справляется с различными задачами, такими как ответы на вопросы, обобщение, перевод и т. д.
https://devopsgu.ru/blog/posts/chto-takoe-llm/#llm

**zero-shot**
позволяет моделям находить решения в условиях малых или отсутствующих данных, обучаясь на базе аналогий и векторных представлений.
Zero-shot обучение, или обучение «без примеров», — это когда ИИ учится выполнять новые задачи, не обучаясь на специфических примерах для этих задач. По сути, это машина, которая может учиться на лету!

В отличие от этого, supervised (обучение с учителем) и unsupervised (обучение без учителя) подходы требуют большого количества данных для обучения.

У этой статьи есть [google colab версия](https://colab.research.google.com/drive/1sD_hQJOi3CrHn7Ba-XuKkHRToxDRRSof?usp=sharing), где можно сразу в интерактивном режиме запустить все примеры.
Чтобы разобраться в принципе работы zero-shot подхода, нужно понять две вещи: перенос знаний между задачами и векторное представление с семантическим пространством.

Перенос знаний между задачами означает, что ИИ, обученный на одной задаче, способен применить полученные знания для решения другой, схожей задачи.
Векторное представление и семантическое пространство играют ключевую роль в zero-shot обучении. Это способ кодирования информации о предмете или явлении в виде вектора (набора чисел) в многомерном пространстве. Близость векторов в этом пространстве показывает сходство между объектами. ИИ использует эти векторные представления для анализа и принятия решений.

Преимущества:
- позволяет модели лучше справляться с недостатком данных для обучения, особенно когда речь идет о редких объектах или языках. 
- Способствует гибкости ИИ, так как модель не привязана к конкретным примерам и может адаптироваться к новым задачам.

Недостатки:
- Может страдать от неточности из-за отсутствия прямого опыта с объектами или языками, что в конечном итоге влияет на качество работы модели.
- Сложность синхронизации знаний между областями

**Few**-**shot** **prompting**" переводится на русский как "**Промптинг** с несколькими примерами или дословно несколько выстрелов". Большие языковые модели (LLM) продемонстрировали впечатляющие возможности в области **промптинга** без примеров (Zero-**Shot** **промптинг**) формулирования запросов, однако они всё ещё имеют ограничения при выполнении более сложных задач в рамках данного метода.

Ссылки:

[**Generative Pre-Training Language Models with Auxiliary Conditional Summaries**](https://docs.yandex.ru/docs/view?tm=1705499515&tld=ru&lang=en&name=report50.pdf&text=Generative%20pre%20training%20on%20large%20language%20models&url=https%3A%2F%2Fweb.stanford.edu%2Fclass%2Farchive%2Fcs%2Fcs224n%2Fcs224n.1204%2Freports%2Fcustom%2Freport50.pdf&lr=29397&mime=pdf&l10n=ru&sign=50db5878a282f0f1d0276a63a34359e5&keyno=0&nosw=1&serpParams=tm%3D1705499515%26tld%3Dru%26lang%3Den%26name%3Dreport50.pdf%26text%3DGenerative%2Bpre%2Btraining%2Bon%2Blarge%2Blanguage%2Bmodels%26url%3Dhttps%253A%2F%2Fweb.stanford.edu%2Fclass%2Farchive%2Fcs%2Fcs224n%2Fcs224n.1204%2Freports%2Fcustom%2Freport50.pdf%26lr%3D29397%26mime%3Dpdf%26l10n%3Dru%26sign%3D50db5878a282f0f1d0276a63a34359e5%26keyno%3D0%26nosw%3D1)


DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation
https://aclanthology.org/2020.acl-demos.30.pdf


### Сравнение поколений GPT

#### GPT-1

**GPT — Generative Pretraining of Transformers.** Первая версия этой модели состояла из 12 слоёв и была обучена на 7000 книг. Как языковая модель она работала не очень хорошо (длинные тексты генерировались плохо), но при файнтюнинге на отдельных задачах эта модель выбила несколько SOTA результатов. Собственно, статья была про то, что usupervised language modeling pretraining улучшает качество дальнейшего файнтюнинга. Максимальный размер контекста у GPT-1 — 512 токенов.

#### GPT-2

SOTA результаты первой GPT держались недолго, так как появился BERT. В OpenAI психанули и решили значительно прокачать свою модель. Во-первых, они её сделали в 10 раз больше: 48 слоёв ~ 1.5B параметров. А во вторых, обучили на невероятно большом объёме данных — к книгам добавили 8 миллионов сайтов (с хорошим рейтингом на реддите). Суммарно получилось 40 гб текста. Архитектурно модель изменилась не сильно — только немного переместили слои нормализации. В итоге оказалось, что GPT-2 настолько стала лучше, что научилась писать длинные связные тексты и даже решать при помощи prompt engineering множество новых задач! Максимальный размер контекста у GPT-2 — 1024 токенов.

#### GPT-3

Опять же модель сделали в 10 раз больше (175B параметров) и обучили на ещё большем количестве данных (570GB текста). Из архитектурных изменений — только немного оптимизировали attention. После такого апгрейда модель стала настолько крутой, что научилась писать рабочий программный код (так появился [CODEX](https://openai.com/blog/openai-codex/)) и решать ещё больше почти сверхъестественных задач ([воскрешать мёртвых](https://futurism.com/openai-dead-fiancee)). Максимальный размер контекста у GPT-3 — 2048 токенов.
# GPT для чайников: от токенизации до файнтюнинга
https://habr.com/ru/articles/599673/

**Оригинальные статьи про три поколения GPT:**

- [Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    
- [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    
- [Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)