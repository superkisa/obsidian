==в процессе, Игорь==

Генеративная предтренинговая подготовка. Большие языковые модели. Базовая адаптация LLM (zero-shot, few-shot prompting)



**Generative** **pre**-**trained** transformers (GPT) are a type of large language **model** (LLM) and a prominent framework for **generative** artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, **pre**-**trained** on large data sets of unlabelled text, and.
==Это типа только один из типов LLM, а какие еще бывают?== (ходи с нами и увидишь дивный новый мир XD)
![[Pasted image 20240117191008.png]]
https://habr.com/ru/articles/599673/:
Откуда взялась GPT и какую задачу она решает
Языковое моделирование — это предсказание следующего слова (или куска слова) с учётом предыдущего контекста.
Подбор модификаций текста называется **«Prompt Engineering»**. Такая простая идея позволяет решать практически неограниченное количество задач. Именно поэтому многие считают GPT-3 подобием сильного искусственного интеллекта.

**Токенизация текста**
это библиотеки от [Hugging Face](https://huggingface.co/): `transformers`, `tokenizers`, `datasets`
процедура токенизации — преобразование текста в последовательность чисел
можно разбивать слова на наиболее общие части и представлять их полные версии как комбинации этих кусков (см. картинку). Такой способ токенизации называется BPE (Byte Pair Encoding). Но даже это иногда не самый оптимальный выбор. Чтобы сжать словарь ещё сильнее для обучения GPT OpenAI использовали byte-level BPE токенизацию. Эта модификация BPE работает не с текстом, а напрямую с его байтовым представлением.

Архитектура GPT
при генерации продолжения текста с помощью GPT происходит следующее:
1. Входной текст токенизируется в последовательность чисел (токенов).
    2. Список токенов проходит через Embedding Layer (линейный слой) и превращается в список эмбеддингов (очень похоже на word2vec).
    3. К каждому эмбеддингу прибавляется positional embedding
    4. Далее список эмбеддингов начинает своё путешествие через несколько одинаковых блоков (**Transformer Decoder Block**)
    5. **После того как список эмбеддингов пройдёт через последний блок, эмбеддинг, соответствующий последнему токену матрично умножается на всё тот же входной, но уже транспонированный Embedding Layer и после применения SoftMax получается распределение вероятностей следующего токена.**
    6. Из этого распределения выбираем следующий токен (например с помощью функции **argmax**).
    7. Добавляем этот токен к входному тексту и повторяем шаги 1-6.

![[Pasted image 20240117200341.png]]


### Transformer Decoder Block
Вот так выглядит главная структурная часть GPT: self-attention, нормализация, feed-forward и residual connections.
![[Pasted image 20240117200800.png]]

**Методы генерации текста**
языковая модель выдаёт распределение вероятностей следующего токена

Greedy Search

Самый простой способ — это **аргмаксная генерация** (greedy search), когда мы каждый раз выбираем токен, у которого максимальная вероятность - самый простой подход

Beam search
Чуть более сложный и качественный способ сэмплирования — это **beam search**. В этом случае на каждом шаге мы выбираем не только один самый вероятный токен, а сразу несколько (`beam-size`), и дальше продолжаем поиск для каждого из выбранных токенов. Таким образом мы разветвляем пути генерации, получая несколько вариантов сгенерированного текста. В итоге можно выбрать тот вариант, у которого лучшая перплексити (уверенность модели в реалистичности текста).
![[Pasted image 20240117202127.png]]





 Файнтюнинг GPT

У этой статьи есть [google colab версия](https://colab.research.google.com/drive/1sD_hQJOi3CrHn7Ba-XuKkHRToxDRRSof?usp=sharing), где можно сразу в интерактивном режиме запустить все примеры.







Ссылки:

**Generative Pre-Training Language Models with Auxiliary Conditional Summaries**
https://docs.yandex.ru/docs/view?tm=1705499515&tld=ru&lang=en&name=report50.pdf&text=Generative%20pre%20training%20on%20large%20language%20models&url=https%3A%2F%2Fweb.stanford.edu%2Fclass%2Farchive%2Fcs%2Fcs224n%2Fcs224n.1204%2Freports%2Fcustom%2Freport50.pdf&lr=29397&mime=pdf&l10n=ru&sign=50db5878a282f0f1d0276a63a34359e5&keyno=0&nosw=1&serpParams=tm%3D1705499515%26tld%3Dru%26lang%3Den%26name%3Dreport50.pdf%26text%3DGenerative%2Bpre%2Btraining%2Bon%2Blarge%2Blanguage%2Bmodels%26url%3Dhttps%253A%2F%2Fweb.stanford.edu%2Fclass%2Farchive%2Fcs%2Fcs224n%2Fcs224n.1204%2Freports%2Fcustom%2Freport50.pdf%26lr%3D29397%26mime%3Dpdf%26l10n%3Dru%26sign%3D50db5878a282f0f1d0276a63a34359e5%26keyno%3D0%26nosw%3D1


DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation
https://aclanthology.org/2020.acl-demos.30.pdf

# GPT для чайников: от токенизации до файнтюнинга
https://habr.com/ru/articles/599673/

**Оригинальные статьи про три поколения GPT:**

- [Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    
- [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    
- [Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)