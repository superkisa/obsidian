
Передача обучения с LLM. Стратегии точной настройки – полная точная настройка, p-настройка, адаптеры


==Игорь в процессе==

The concept of Transfer learning was first proposed in 1995 by scientist [Sebastian Thrun](https://en.wikipedia.org/wiki/Sebastian_Thrun) during his research in the field of artificial intelligence

researchers faced the problem that training each new model from scratch required immense computational resources and time. 
This issue became particularly acute with the growth of the complexity of models and the tasks they were meant to solve.

Transfer learning was the answer to this problem. It allowed the use of knowledge and experience gained in solving one task to be applied to another related task. This not only reduced the time and resources needed to train a new model but also enabled models to generalize knowledge across different domains and tasks.

The concept became a fundamental breakthrough in machine learning, as it opened new paths and training methods and allowed researchers and engineers to create more complex and powerful systems.


Technically, transfer learning is divided into two main stages: pre-training, where the model undergoes basic training, and fine-tuning, where the model is further trained for a specific task. The beauty of transfer learning for many lies in the fact that the first stage is not mandatory for many companies and tasks. They can take pre-trained models and fine-tune them for their specific needs. However, we will generally discuss both stages.

![[Pasted image 20240118003743.png]]

Pre-training is the first and critically important stage for LLMs in the transfer learning process. At this stage, the model is trained on a vast corpus of textual data, including literature, news, web pages, and other sources. The process includes the following stages:

#### Tokenization and vectorization

Data conversion to machine-readable format is the initial data processing and preparation stage. In the case of text data, this process involves tokenization and vectorization. Tokenization involves splitting the text into individual words or tokens, and vectorization transforms these tokens into numerical vectors.

#### Choice of architecture

The next step is selecting the neural network architecture, a critically important phase that determines how efficiently the model will be able to process data and solve the given tasks. In the Natural Language Processing (NLP) context, transformers have become the standard due to their ability to capture long-term dependencies in text. However, other architectures like RNNs or LSTMs might also be applicable depending on specific requirements and conditions.

#### Hyperparameter tuning

At this stage, key model parameters affecting its learning and performance are set. Learning rate specifies the pace at which the model adjusts its weights based on errors made in previous iterations. Batch size refers to the number of data samples processed in a single training iteration. The number of epochs indicates how often the model will iterate over the entire training data set. These parameters are often selected through cross-validation, a statistical assessment method for optimizing model performance.

#### Optimization algorithms

This stage is critically important for the model's efficient learning. Optimization algorithms, such as Stochastic Gradient Descent (SGD) or Adam, not only adjust the model's weights but do so adaptively, considering the dynamics of the loss function. For instance, SGD updates the model's weights at each iteration with a small subset of the data, accelerating the learning process. Adam combines the best attributes of SGD and other algorithms for a more effective minimization of the loss function. Choosing the right optimization algorithm and its parameters can significantly impact the model's predictive quality and convergence speed.

### Fine-tuning for a specific task

Fine-tuning is the second stage, where the model is adapted to a specific task. This stage includes:

#### Data preparation for the specific task

Preparing data for a particular task is the initial and crucial step. Data is collected and pre-processed to be highly relevant to the specific task at hand. In NLP tasks, this may include stop-word removal, the utilization of specialized lexicons, or more advanced technologies like TF-IDF vectorization.

#### Model adaptation

The model itself also undergoes architectural modifications to handle specialized tasks better. 

This could mean replacing the last fully connected layer with a new one featuring an output neuron count that matches the number of classes in the new task. This layer is then fine-tuned on a new corresponding dataset while the remaining layers may remain "frozen" to preserve features learned from larger datasets. 

This can also include adding specialized layers for specific tasks like named entity recognition, sentiment analysis, etc. These new layers are also subsequently fine-tuned on task-specific data to make the model more efficient in solving any new issue.

#### Hyperparameter tuning for the specific task

Hyperparameter settings for specific tasks often differ from those for general training. Lower learning rates or smaller batch sizes may be needed for the model to adapt to new data more accurately.

Besides cross-validation, methods like differential evolution are often used for hyperparameter tuning in cases where the parameter space is continuous and complex and particularly useful when the loss function's gradient is unknown or incalculable.

Another method is population-based training, which dynamically adapts hyperparameters during training, allowing the model to converge faster to the optimal solution. This method is especially useful in tasks requiring quick adaptation to changing conditions, among many other methods depending on task-specificity.

#### Evaluation metrics

After retraining the model with the changes mentioned above, it's important to select the proper evaluation metrics. More specialized metrics can be used here rather than general ones like F1-score.

For instance, in text classification tasks, metrics like ROC AUC or Precision-Recall AUC may be used, which account for not just accuracy but also the model's ability to distinguish classes in imbalanced data scenarios. For machine translation tasks, relevant metrics include BLEU or TER, which evaluate translation quality at the level of individual words and phrases.

Thus, the choice of performance evaluation metrics must be adjusted to the specific task at hand to provide the most accurate and informative efficiency assessment.

## Advantages of using transfer learning in LLMs

Applying transfer learning in LLMs brings many significant advantages, making it useful for businesses and research entities.

### Customized product development

Utilizing transfer learning in LLMs allows for deep customization of algorithms for domain-specific tasks. This enables the injection of domain expertise into the model, ranging from sectors like healthcare to finance. As a result, each solution acquires unique, unattainable attributes through standard machine learning algorithms. This facilitates the deep integration of the products into existing business processes, reducing dependency on generic solutions and improving operational efficiency.

### Market entry

Transfer learning in LLMs focuses on comprehensively understanding local nuances, from syntactic and grammatical elements to cultural intricacies and social cues. This minimizes language inaccuracies and, more crucially, lowers the risk of culturally inappropriate assumptions that could be offensive or unacceptable in a new region. Consequently, market entry becomes less risky and more predictable, allowing for a focus on strategic expansion rather than operational issues.

### Cost savings

Traditional machine-learning approaches require substantial computational and time resources for training a model from scratch. Transfer learning can leverage pre-trained models, substantially reducing computational power and training time. This not only cuts down hardware and specialist man-hours but also speeds up the implementation of innovative projects. Freed resources can be reallocated to research and development rather than routine operations, turning the cost savings at the development stage into a strategic advantage for your business.

### Accuracy

Standard machine learning models frequently encounter issues of overfitting or underfitting, compromising their effectiveness in real-world business scenarios. Transfer learning in LLMs significantly enhances prediction accuracy by using pre-collected extensive data sets and domain expertise. This allows for more effective customer personalization, business process adaptation, and decision-making systems. You gain a tool that accomplishes tasks with high accuracy, positively affecting client trust and, ultimately, profitability.

### Adaptability

The industry is ever-changing, with new technologies, market demands, and customer preferences emerging. In such a dynamic environment, adaptability is a key factor. Transfer learning enables your LLMs to adapt rapidly to new conditions. Pre-trained models can be supplemented or modified for new tasks much more quickly than training models from scratch. This means your business can respond more swiftly to changes in market conditions or customer needs while maintaining high efficiency and competitiveness.

## Challenges of transfer learning in LLMs

However, like any technological approach, it has its nuances and complexities. Understanding these aspects is critical for successfully implementing transfer learning in business processes.


https://maddevs.io/blog/transfer-learning-from-large-language-models/

# Про fine-tuning моделей простыми словами
Мы берем существующую модель, в которой уже установлено большинство параметров, и нам нужно только подкрутить несколько других. Это гораздо проще, чем тренировать модель с нуля.
Ключевой момент здесь - использовать модель, которая уже хорошо справляется с определенными базовыми аспектами типа задачи, которую вы хотите решить. Например, если вы пытаетесь построить систему, которая категоризирует отзывы пользователей на товары как положительные, нейтральные или отрицательные, лучше использовать существующую модель обработки естественного языка (NLP модель), которая уже хорошо понимает человеческий язык, в качестве основы. Например, модель, обученная на Википедии, чтобы предсказывать следующее слово статьи Википедии. Затем вы можете провести дополнительную настройку с набором данных отзывов на товары, чтобы она могла определять оценку товара из текстового обзора пользователя.

во многих случаях настройка не имеет смысла. LLM с правильным промптом и некоторым контекстом будет работать гораздо лучше, чем что-либо, что вы можете реально настроить или обучить самостоятельно. Если ваша задача настолько специфична, что универсальная модель не является хорошим вариантом, тогда да, вам придется настраивать. Но попробовать инженерию промптов сначала с существующей большой моделью гораздо дешевле.

**В контексте NLP это можно рассматривать как конструирование переводчика с языка 1 на язык 3, имея переводчик с языка 1 на язык 2 из той же языковой группы**

Вот несколько сценариев, где настройка помогает:

1. Вход в новую область: если у вас есть универсальная модель, и вы хотите, чтобы она лучше работала в определенной области, например, в медицине, вы можете донастроить ее с помощью соответствующих документов, чтобы специализироваться в этой области.
    
2. Улучшение качества: если ваша цель — улучшить способность модели выполнять определенные задачи, например, писать стихи или переводить языки, донастройка может помочь достичь этого.
    
3. Настройка стиля вывода: вы можете настроить модель, чтобы корректировать ее тон, личность или уровень детализации в ее выводах, делая ее более подходящей для ваших требований.
    
4. Адаптация к меняющимся данным: по мере изменения типа данных, с которыми вы работаете, настройка модели обеспечивает ее актуальность и эффективность.


**p-tuning**
few-shot — учится составлять тексты по заданному нами принципу

Задача остаётся прежней: на базе подводки составлять новый текст, пользуясь знаниями об устройстве языка и мира, заложенными в модель.
загружаем наборы векторов, получившиеся из текстов. С этими векторами производятся некоторые манипуляции, после чего мы превращаем их обратно в итоговый текст.
Набор эмбеддингов, получившихся из подводки, априори не является самым удачным. И модель пытается отыскать более оптимальный набор, меняя эти эмбеддинги так, чтобы итоговая задача решалась всё лучше. Поскольку мы оптимизируем не тексты, а числа, то это можно делать классическим методом уменьшения ошибки — градиентным спуском.
То есть модель сама подбирает себе наиболее оптимальную подводку — потому что мы перешли на понятный ей уровень абстракции. Если совсем грубо — «заговорили» на её языке (языке эмбеддингов, а не текстов), объясняя, какого результата мы от неё ждём.

![[Pasted image 20240118010754.png]]- Давайте заменим эмбеддинги токенов подводки (на рисунке выше), которые мы во few-shot подбирали руками, на обучаемые векторы (которые будем предсказывать некоторой моделью) и будем учить только их, заморозив веса основной языковой модели.
- Моделировать эмбеддинги можно несколькими способами, авторы предложили использовать для этого LSTM и MLP:  
      
    
    ![](https://habrastorage.org/r/w1560/webt/6l/jj/iy/6ljjiysf8j-jj4pj8uafbmrekyk.png)
    
- Наши эксперименты показывают, что такая модель хорошо работает на сотнях (и больше) примеров в обучении.
- Если же примеров только десятки, и больше получить нельзя, то стоит моделировать их просто новым Embedding-слоем, без LSTM и MLP, но инициализировать веса этого слоя весами эмбеддингов pretrained языковой модели, которые соответствуют токенам из вашей подводки, составленной руками заранее (впервые предложено в [этой статье](https://arxiv.org/pdf/2104.08691.pdf)).
- Получается своего рода adversarial attack на часть входного текста в NLP-модель.

  
Мы проверили, что P-tuning отлично работает в связке с представителями семейства языковых моделей, но ничто не мешает использовать его и c другими популярными архитектурами, например с BERT или [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html).

## Преимущества P-tuning

  
Устройство P-tuning'a наделяет его следующими свойствами:  
  
![](https://habrastorage.org/r/w1560/webt/r3/p9/3r/r3p93rayel2abksrimdu6qkyuuo.png)  
  

1. P-tuning избавляет инженера от перебора подводок и **всегда обеспечивает более высокое качество**, чем few-shot (способен найти почти любую подводку, которая может быть создана руками).
2. Модель чаще всего не производит артефактов, присущих few-shot.
3. Так как параметры модели не обучаются, то для применения метода не требуется больших вычислительных ресурсов (вместо **GPU-дней** в finetuning'е получаем **GPU-часы**).
4. Модель, которая ищет более оптимальную подводку, имеет относительно **небольшое число параметров**. Поэтому для её обучения не нужны огромные массивы данных — достаточно сотен или тысяч примеров, в то время как для finetuning-a нужны десятки тысяч.
5. Так как P-tuning по своей природе влияет только на данные, поступающие на вход, то для одной и той же pretrained языковой модели можно «подключать» разные P-tuning-модели: весят они всего лишь килобайты. Это позволяет создать runtime API, в котором **зафиксирована языковая модель**, а пользователи сервиса (в данном случае разработчики) посылают запросы в виде пар «модель + текст». Благодаря API можно **быстро создавать прототипы** для новых задач.
https://habr.com/ru/companies/yandex/articles/588214/
https://huggingface.co/docs/peft/task_guides/ptuning-seq-classification



**Статья про адаптеры:**
https://arxiv.org/pdf/2304.01933v3.pdf

Успех больших языковых моделей (LLM), таких как GPT-4 и ChatGPT, привел к разработке многочисленных экономически эффективных и доступных альтернатив, которые создаются путем точной настройки LLM открытого доступа с данными, относящимися к конкретной задаче (например, ChatDoctor) или данными инструкций (например, Alpaca). Среди различных методов тонкой настройки, эффективная настройка параметров на основе адаптера (PEFT), несомненно, является одной из наиболее привлекательных тем, поскольку требует точной настройки только нескольких внешних параметров вместо всей LLMS, обеспечивая при этом сопоставимую или даже лучшую производительность.
Чтобы обеспечить возможность дальнейшего исследования PEFT-методов LLMS, в этой статье представлены LLM-адаптеры, простая в использовании платформа, которая интегрирует различные адаптеры в LLMS и может выполнять эти основанные на адаптерах PEFT-методы LLMS для различных задач. Фреймворк включает в себя современные LLM с открытым доступом, такие как LLaMA, BLOOM и GPT-J, а также широко используемые адаптеры, такие как последовательные адаптеры, параллельные адаптеры, методы обучения на основе подсказок и репараметризации.
Более того, мы проводим обширные эмпирические исследования влияния типов адаптеров, мест размещения и гиперпараметров на наилучший дизайн для каждого метода, основанного на адаптерах. Мы оцениваем эффективность адаптеров на четырнадцати наборах данных из двух различных задач рассуждения: арифметического рассуждения и рассуждения на основе здравого смысла. Результаты демонстрируют, что использование PEFT на основе адаптера в LLMS меньшего масштаба (7B) с небольшим количеством дополнительных обучаемых параметров обеспечивает сопоставимую, а в некоторых случаях и превосходящую производительность по сравнению с мощными LLMS (175B) при выводе с нулевым результатом в обеих задачах рассуждения.