**Архитектура Transformer: Структура блоков, Layer Norm, Процедура обучения**

**Структура блоков:**

1. **Энкодер и Декодер:**
    
    - Архитектура Transformer состоит из энкодера и декодера.
    - Энкодер обрабатывает входную последовательность параллельно, а декодер генерирует выходную последовательность.
2. **Самовнимание (Self-Attention):**
    
    - Основное новшество: Механизм самовнимания позволяет каждому слову сосредотачиваться на разных частях входной последовательности, эффективно улавливая зависимости.
    - Multi-head attention: Несколько голов внимания работают параллельно, обеспечивая разнообразные перспективы.
3. **Позиционное Кодирование:**
    
    - Transformer не понимает порядок входной последовательности изначально.
    - Вводится позиционное кодирование для учета информации о позиции слова в последовательности.
4. **Нейронные Сети с Передачей (Feedforward):**
    
    - Выход каждой позиции после самовнимания проходит через нейронную сеть с передачей.
5. **Резидуальные Соединения и Нормализация Слоев:**
    
    - Резидуальные соединения вокруг как слоя самовнимания, так и нейронной сети с передачей.
    - Нормализация слоев применяется перед каждым выходом подслоя.

**Нормализация Слоев (Layer Norm):** 6. **Нормализация в Transformer:**

- Нормализация слоев необходима для стабилизации обучения в глубоких сетях.
- Применяется перед активационной функцией в каждом подслое (самовнимание и нейронная сеть).

7. **Формула Нормализации Слоев (для подслоя с входом �x):** LayerNorm(�)=�−mean(�)var(�)+�⋅�+�LayerNorm(x)=var(x)+ϵ​x−mean(x)​⋅γ+β где �ϵ - небольшая константа, а �γ и �β - обучаемые параметры.

**Процедура Обучения:** 8. **Оптимизатор AdamW:**

- Обучение моделей Transformer часто проводится с использованием оптимизатора AdamW.
- AdamW сочетает преимущества адаптивных скоростей обучения (Adam) с весовым угасанием для предотвращения переобучения.

9. **Планирование Скорости Обучения:**
    
    - Планирование скорости обучения критично для эффективного обучения.
    - Фаза разогрева: Постепенное увеличение скорости обучения в начальных шагах.
    - Линейное затухание: Линейное уменьшение скорости обучения после фазы разогрева.
10. **Сглаживание Меток (Label Smoothing):**
    
    - Используется для предотвращения чрезмерной уверенности модели в предсказаниях.
    - Вместо присвоения вероятности 1 целевому классу, малая вероятность �ϵ присваивается всем классам.
11. **Функция Потерь (Loss Function):**
    
    - Кросс-энтропийная функция потерь часто используется для обучения.
    - Включает в себя сглаживание меток при вычислении.
12. **Обрезка Градиентов (Gradient Clipping):**
    
    - Предотвращает взрывание градиентов, ограничивая значения градиентов во время обратного распространения.
13. **Регуляризация:**
    
    - Применяется dropout к выходу подслоев самовнимания и нейронной сети с передачей.
14. **Оценка Модели:**
    
    - Производится оценка производительности на валидационных данных с использованием метрик, таких как перплексия или BLEU-оценка.

В итоге, архитектура Transformer использует механизмы самовнимания, нормализацию слоев и резидуальные соединения как в энкодере, так и в декодере. Процедура обучения включает оптимизацию AdamW, планирование скорости обучения, сглаживание меток и регуляризацию для достижения выдающейся производительности в различных задачах обработки естественного языка.