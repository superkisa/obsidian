[[Архитектура Transformer Структура блоков, Layer Norm, Процедура обучения | краткий ответ от чатика на русском]]

Однозначно перед этим надо кратко ознакомиться с содержимым статей из прошлого вопроса [на русском](https://habr.com/ru/articles/486358/) и [оригинал на английском](https://jalammar.github.io/illustrated-transformer/).
Но здесь нужно выделить детали.

Конкретно о структуре блоков в статьях начинается примерно с [этого](https://jalammar.github.io/illustrated-transformer/#:~:text=Self%2DAttention%20in%20Detail) места.
[Positional Encoding здесь](https://jalammar.github.io/illustrated-transformer/#:~:text=Representing%20The%20Order%20of%20The%20Sequence%20Using%20Positional%20Encoding).
Важное замечание, что изначально анонсированный Positional Encoding:
![[Pasted image 20240119194609.png]]
Преобразован в такой:
![[Pasted image 20240119194706.png]]

[Residual Connections здесь](https://jalammar.github.io/illustrated-transformer/#:~:text=generate%20it%3A-,The%20Residuals,-One%20detail%20in)
![[Pasted image 20240119194903.png]]
Там же упоминается, что используется [Layer Normalization](https://arxiv.org/abs/1607.06450) вместо Batch Normalization (важный пункт)
![[Pasted image 20240119194939.png]]
>[!info]- Кратко от ==чатика== о сути метода: 
>Статья "Layer Normalization" авторства Джимми Лей Ба, Джейми Райана Кироса и Джеффри И. Хинтона рассматривает проблему вычислительной сложности обучения глубоких нейронных сетей. Один из способов сокращения времени обучения заключается в нормализации активности нейронов. В работе представлен метод нормализации под названием "batch normalization", который использует распределение суммарного входа в нейрон на мини-пакете обучающих примеров для вычисления среднего и дисперсии, которые затем используются для нормализации входа на каждом обучающем примере. Однако эффект batch normalization зависит от размера мини-пакета, и не очевидно, как его применять к рекуррентным нейронным сетям.
>В статье предлагается трансформировать batch normalization в "layer normalization", вычисляя среднее и дисперсию для нормализации из всех суммарных входов в нейроны слоя на одном обучающем примере. Как и в batch normalization, каждому нейрону присваивается собственный адаптивный сдвиг и масштаб, которые применяются после нормализации, но перед нелинейностью. В отличие от batch normalization, layer normalization выполняет точно такие же вычисления во время обучения и тестирования. Также легко применять к рекуррентным нейронным сетям, вычисляя статистику нормализации отдельно для каждого временного шага. Layer normalization эффективно стабилизирует динамику скрытого состояния в рекуррентных сетях, и в эмпирических исследованиях показано, что она существенно сокращает время обучения по сравнению с ранее опубликованными методами.

Что происходит в части [декодера](https://jalammar.github.io/illustrated-transformer/#:~:text=something%20like%20this%3A-,The%20Decoder%20Side,-Now%20that%20we%E2%80%99ve)
Последний слой на [выходе декодера](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20Final%20Linear%20and%20Softmax%20Layer)

Очень кратко о тренировке трансформера [в тех же статьях](https://jalammar.github.io/illustrated-transformer/#:~:text=an%20output%20word.-,Recap%20Of%20Training,-Now%20that%20we%E2%80%99ve)
Как лосс-функции применяются наши родные и любимые [кросс-энтропия](https://colah.github.io/posts/2015-09-Visual-Information/) и [KL-divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) (очень советую углубиться в статьи, но сделать это на пенсии)
Из оригинальной статьи [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf):
![[Pasted image 20240117215650.png]]
![[Pasted image 20240117215738.png]]
![[Pasted image 20240117215758.png]]

>[!info]- Кратко от ==чатика== о Label Smoothing: 
>Label smoothing (сглаживание меток) - это техника регуляризации, которая используется в процессе обучения нейронных сетей, в том числе в моделях трансформера для машинного перевода. Эта техника предполагает, что вместо использования точных меток для обучающих примеров, когда входные данные соответствуют одному классу, используется распределение вероятностей по всем классам, включая целевой класс.
>Например, при обучении модели машинного перевода, где каждому слову в предложении соответствует метка, label smoothing может быть применено следующим образом. Вместо того, чтобы установить истинную метку (1) для целевого слова и нули для всех остальных слов, применяется распределение вероятностей. Это означает, что небольшая вероятность распределяется по всем словам, включая целевое, исключая точное значение 1.
>Таким образом, если у нас есть 3 класса (слова), и слово "кошка" является целевым, вместо меток [0, 0, 1], мы можем использовать, например, [0.1, 0.1, 0.8]. Это позволяет модели быть менее уверенной в своих предсказаниях и уменьшает переобучение.
>Label smoothing может помочь улучшить обобщение модели, сделав ее менее склонной к переобучению на обучающем наборе данных. Однако важно выбирать правильное значение для параметра сглаживания, чтобы избежать избыточного размытия меток и сохранить релевантность обучающей информации.

Подробно об оптимизаторах Adam и AdamW в [этой статье](https://towardsdatascience.com/why-adamw-matters-736223f31b5d) (VPN). Но очень много инфы, поэтому,
>[!info]- кратко от ==чатика== об этой статье: 
>Эта статья обсуждает проблему обобщения моделей, обученных с использованием адаптивных оптимизаторов, таких как Adam, в сравнении с оптимизатором стохастического градиентного спуска (SGD) с моментумом. Авторы предлагают улучшенную версию оптимизатора Adam, названную AdamW, которая обеспечивает лучшее обобщение моделей и способна конкурировать с SGD, при этом обучаясь значительно быстрее.
>Статья объясняет, как работает оптимизатор Adam, включая его способность адаптировать размер шага для каждого веса индивидуально. Также рассматривается проблема обобщения моделей, обученных с использованием L2-регуляризации (весового усиления), и показывается, что эффективность L2-регуляризации снижается для адаптивных алгоритмов типа Adam по сравнению с SGD.
>Авторы предлагают решение этой проблемы в виде улучшенной версии Adam, названной AdamW, которая существенно улучшает обобщение моделей по сравнению со стандартным Adam. 
>В третьей части статьи рассматривается алгоритм оптимизации Adam, с фокусом на проблеме, связанной с L2-регуляризацией (весовым усилением) в Adam. Авторы отмечают, что обычная реализация L2-регуляризации в Adam неэффективна, так как она влияет не только на градиенты функции потерь, но также и на регуляризационный член. Это приводит к тому, что веса, имеющие большие и быстро изменяющиеся градиенты, регуляризуются меньше, чем веса с маленькими и медленно изменяющимися градиентами.
>Для решения этой проблемы авторы предлагают улучшенную версию Adam, названную AdamW. В AdamW регуляризационный член применяется только после коррекции параметров шага для каждого веса. Таким образом, член регуляризации не включается в подсчеты скользящих средних градиента и его квадрата (m и v), и становится пропорциональным самому весу.
>
>Далее об отличиях оптимизаторов:
>
>Основное отличие между методами Adam и AdamW заключается в обработке регуляризационного термина (весового усиления). В обычном Adam регуляризация влияет на скользящие средние градиента и его квадрата (m и v), что может привести к неэффективной L2-регуляризации и ухудшению обобщения моделей.
>В методе AdamW авторы предлагают применять регуляризацию только после коррекции параметров шага для каждого веса. Это означает, что регуляризационный член не включается в подсчеты скользящих средних, и его влияние становится пропорциональным самому весу. Этот подход позволяет улучшить обобщение моделей, и эксперименты показывают, что модели, обученные с использованием AdamW, демонстрируют лучшие результаты по сравнению с теми, которые обучены с обычным Adam. Это позволяет AdamW конкурировать с оптимизатором стохастического градиентного спуска с моментумом. Таким образом, предложенная модификация AdamW может стать более привлекательным выбором при обучении нейронных сетей, исключая необходимость частого переключения между оптимизаторами типа SGD и Adam.

>[!info]- Кратко от ==чатика== о сути learning rate scheduling:
>Планирование скорости обучения (learning rate scheduling) - это техника в обучении глубоких нейронных сетей, включая трансформерные модели, которая заключается в изменении скорости обучения в течение процесса обучения. Скорость обучения определяет размер шага, с которым модель обновляет веса в процессе градиентного спуска. Использование правильной скорости обучения является ключевым аспектом успешного обучения моделей.
>
> Основные стратегии планирования скорости обучения включают:
>1. **Фиксированный график (Fixed Schedule):** Скорость обучения остается постоянной на протяжении всего процесса обучения. Это простой и понятный метод, но может быть неэффективным, особенно при долгих обучениях.
>2. **Экспоненциальное затухание (Exponential Decay):** Скорость обучения уменьшается экспоненциально с течением времени или эпох. Этот метод часто используется для уменьшения скорости обучения по мере того, как модель приближается к оптимальным весам.
>3. **Планирование по эпохам (Step-wise Schedule):** Скорость обучения уменьшается после определенного числа эпох или итераций обучения. Это позволяет более гибко регулировать скорость обучения в зависимости от процесса обучения.
> 4. **Планирование на основе производительности (Performance-based Schedule):** Скорость обучения изменяется в ответ на производительность модели на валидационном наборе данных. Например, если производительность перестает улучшаться, скорость обучения может быть уменьшена.
> 5. **Циклическое планирование (Cyclical Schedule):** Скорость обучения циклически изменяется между верхним и нижним порогами. Этот метод может помочь модели избегать локальных минимумов.
> 
>В контексте трансформерных моделей, которые часто обучаются на больших объемах данных, планирование скорости обучения играет важную роль. Это позволяет модели лучше сходиться, избежать переобучения и достигнуть более высокого качества обобщения.