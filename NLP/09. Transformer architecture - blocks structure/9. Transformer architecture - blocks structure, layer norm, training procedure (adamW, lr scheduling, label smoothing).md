Однозначно перед этим надо кратко ознакомиться с содержимым статей из прошлого вопроса [на русском](https://habr.com/ru/articles/486358/) и [оригинал на английском](https://jalammar.github.io/illustrated-transformer/).
Но здесь нужно выделить детали.
==занимаюсь, Евгений==

Конкретно о структуре блоков в статьях начинается примерно с [этого](https://jalammar.github.io/illustrated-transformer/#:~:text=Self%2DAttention%20in%20Detail) места.
[Positional Encoding здесь](https://jalammar.github.io/illustrated-transformer/#:~:text=Representing%20The%20Order%20of%20The%20Sequence%20Using%20Positional%20Encoding).
[Residual Connections здесь](https://jalammar.github.io/illustrated-transformer/#:~:text=generate%20it%3A-,The%20Residuals,-One%20detail%20in)
Там же упоминается, что используется [Layer Normalization](https://arxiv.org/abs/1607.06450). Кратко от ==чатика== о сути процесса:
