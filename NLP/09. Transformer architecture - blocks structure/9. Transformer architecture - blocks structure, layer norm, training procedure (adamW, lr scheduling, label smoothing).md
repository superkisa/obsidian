Однозначно перед этим надо кратко ознакомиться с содержимым статей из прошлого вопроса [на русском](https://habr.com/ru/articles/486358/) и [оригинал на английском](https://jalammar.github.io/illustrated-transformer/).
Но здесь нужно выделить детали.
==занимаюсь, Евгений==

Конкретно о структуре блоков в статьях начинается примерно с [этого](https://jalammar.github.io/illustrated-transformer/#:~:text=Self%2DAttention%20in%20Detail) места.
[Positional Encoding здесь](https://jalammar.github.io/illustrated-transformer/#:~:text=Representing%20The%20Order%20of%20The%20Sequence%20Using%20Positional%20Encoding).
[Residual Connections здесь](https://jalammar.github.io/illustrated-transformer/#:~:text=generate%20it%3A-,The%20Residuals,-One%20detail%20in)
Там же упоминается, что используется [Layer Normalization](https://arxiv.org/abs/1607.06450) вместо Batch Normalization (важный пункт)
>[!info]- Кратко от ==чатика== о сути метода: 
>Статья "Layer Normalization" авторства Джимми Лей Ба, Джейми Райана Кироса и Джеффри И. Хинтона рассматривает проблему вычислительной сложности обучения глубоких нейронных сетей. Один из способов сокращения времени обучения заключается в нормализации активности нейронов. В работе представлен метод нормализации под названием "batch normalization", который использует распределение суммарного входа в нейрон на мини-пакете обучающих примеров для вычисления среднего и дисперсии, которые затем используются для нормализации входа на каждом обучающем примере. Однако эффект batch normalization зависит от размера мини-пакета, и не очевидно, как его применять к рекуррентным нейронным сетям.
>В статье предлагается трансформировать batch normalization в "layer normalization", вычисляя среднее и дисперсию для нормализации из всех суммарных входов в нейроны слоя на одном обучающем примере. Как и в batch normalization, каждому нейрону присваивается собственный адаптивный сдвиг и масштаб, которые применяются после нормализации, но перед нелинейностью. В отличие от batch normalization, layer normalization выполняет точно такие же вычисления во время обучения и тестирования. Также легко применять к рекуррентным нейронным сетям, вычисляя статистику нормализации отдельно для каждого временного шага. Layer normalization эффективно стабилизирует динамику скрытого состояния в рекуррентных сетях, и в эмпирических исследованиях показано, что она существенно сокращает время обучения по сравнению с ранее опубликованными методами.

Что происходит в части [декодера](https://jalammar.github.io/illustrated-transformer/#:~:text=something%20like%20this%3A-,The%20Decoder%20Side,-Now%20that%20we%E2%80%99ve)
Последний слой на [выходе декодера](https://jalammar.github.io/illustrated-transformer/#:~:text=The%20Final%20Linear%20and%20Softmax%20Layer)

Очень кратко о тренировке трансформера [в тех же статьях](https://jalammar.github.io/illustrated-transformer/#:~:text=an%20output%20word.-,Recap%20Of%20Training,-Now%20that%20we%E2%80%99ve)
Как лосс-функции применяются наши родные и любимые [кросс-энтропия](https://colah.github.io/posts/2015-09-Visual-Information/) и [KL-divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) (очень советую углубиться в статьи, но на пенсии)