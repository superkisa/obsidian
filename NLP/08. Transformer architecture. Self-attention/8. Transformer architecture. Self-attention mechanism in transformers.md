Не вижу особого смысла переносить сюда полную, не перегруженную и красивую инфу из статей [на русском](https://habr.com/ru/articles/486358/) и [оригинал на английском](https://jalammar.github.io/illustrated-transformer/)
Более подробно все в следующем вопросе

https://www.youtube.com/watch?v=zxQyTK8quyY
>[!info]- Кратко от ==чатика== про отличие self-attention от общего понятия attention:
> Attention и Self-Attention (или внимание и самовнимание) - это основные компоненты трансформеров, используемых для обработки последовательностей данных. Рассмотрим разницу между ними.
>1. **Attention (Внимание):**
>   - Attention в общем виде - это механизм, который позволяет модели "фокусироваться" на определенных частях входных данных при выполнении задач обработки последовательностей.
>   - В контексте трансформеров, механизм Attention используется для выделения важных частей входной последовательности при выполнении операций кодирования и декодирования.
>2. **Self-Attention (Самовнимание):**
>   - Self-Attention представляет собой частный случай механизма Attention, где входные данные используются дважды: один раз в роли запросов (queries), ключей (keys) и значений (values).
>   - Это означает, что каждый элемент входной последовательности может взаимодействовать с другими элементами, выделяя их важность. Таким образом, модель может учесть контекст из всех элементов последовательности при обработке каждого элемента.
>Таким образом, Self-Attention - это конкретный случай Attention, где входные и выходные последовательности совпадают, и каждый элемент может "обратить внимание" на остальные элементы в этой же последовательности.
>В трансформерах Self-Attention используется для обработки входных последовательностей в кодирующем слое (encoder), где каждое слово может обращать внимание на другие слова в предложении. Это позволяет модели улавливать долгосрочные зависимости и обрабатывать контекст более эффективно.

![[Screen Shot 2024-01-19 at 19.09.28.png]]

![[Pasted image 20240119212510.png]]
Вормап:
![[Pasted image 20240117181728.png]]
![[Pasted image 20240117181807.png]]
![[Pasted image 20240117181823.png]]

==из лекций== и это именно то, что мы видели в статье))
![[Screen Shot 2024-01-18 at 11.50.49.png]]

![[Screen Shot 2024-01-18 at 12.00.37.png]]

![[Screen Shot 2024-01-18 at 12.03.44.png]]

![[Screen Shot 2024-01-18 at 12.04.20.png]]

![[Screen Shot 2024-01-18 at 12.06.09.png]]

![[Screen Shot 2024-01-18 at 12.06.36.png]]

![[Screen Shot 2024-01-18 at 12.07.31.png]]

![[Screen Shot 2024-01-18 at 17.15.48.png]]