Берем [отличнейшую статью Аламмара](https://jalammar.github.io/illustrated-gpt2) (с [переводом](https://habr.com/ru/post/490842/)) и видим там:
 - что такое [Neural language modeling](https://jalammar.github.io/illustrated-gpt2/#:~:text=a%20language%20model%3F-,What%20is%20a%20Language%20Model,-In%20The%20Illustrated) ![[Pasted image 20240119201438.png]]
 - перечисление архитектур![[Pasted image 20240118185535.png]]![[Pasted image 20240118185607.png]]
 от себя добавлю, что там еще и все виды RNN подходят.
 - множество инфы про внутреннее устройство этих архитектур![[Pasted image 20240119201513.png]] ![](https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif) ![[Pasted image 20240119201720.png]] ![[Pasted image 20240119201735.png]] ![[Pasted image 20240119201756.png]] 
 - кстати здесь же, для меня более понятное объяснение механизмов [Self-Attention](https://jalammar.github.io/illustrated-gpt2/#:~:text=Self%2DAttention%20Recap). Это еще не все, далее [еще подробнее, все по действиям и молекулам](https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention)
 - но вот про метрики там нет, но у нас уже было про метрики в [[6. Machine translation. Problem setup, training and inference procedures. Quality metrics for MT#Quality metrics for MT| конце 6-го вопроса]] здесь все также.


==чатик==
**Neural Language Modeling: Problem Setup and Key Architectures**

**Problem Setup:**

- **Objective:** The goal of neural language modeling is to teach a model to predict the probability distribution of the next word in a sequence of words, given the context of the preceding words.
- **Training Data:** Large datasets of text are used for training, where the model learns to capture the relationships and patterns in language.

**Key Architectures (GPT-based or Analogous):**

1. **GPT (Generative Pretrained Transformer):**
    
    - **Architecture:** GPT is a transformer-based model that uses a stack of attention-based layers for capturing contextual information.
    - **Pretraining:** GPT is pretrained on a diverse corpus to learn general language patterns without a specific task.
    - **Fine-Tuning:** The pretrained model can be fine-tuned on specific tasks, making it adaptable to various applications.
2. **GPT-2 and GPT-3:**
    
    - **Scale:** GPT-2 and GPT-3 are larger versions of GPT, with significantly more parameters.
    - **Capabilities:** They demonstrate improved performance on a range of language tasks and showcase the benefits of scale in language modeling.
3. **BERT (Bidirectional Encoder Representations from Transformers):**
    
    - **Architecture:** BERT uses a bidirectional transformer, allowing it to consider context from both left and right directions.
    - **Masked Language Model (MLM):** BERT is pretrained using a masked language model objective, where some words are randomly masked, and the model learns to predict them based on context.
    - **Fine-Tuning:** Like GPT, BERT can be fine-tuned for specific downstream tasks.
4. **XLNet:**
    
    - **Permutation Language Model:** XLNet combines the strengths of autoregressive (like GPT) and autoencoding (like BERT) language models.
    - **Permutation Language Model Objective:** It introduces a permutation language model objective, allowing the model to capture bidirectional dependencies without the limitations of standard autoregressive or autoencoding approaches.

**Quality Metrics for Language Modeling:**

1. **Perplexity:**
    
    - **Definition:** Perplexity measures how well the model predicts a sequence of words.
    - **Lower is Better:** A lower perplexity indicates better performance, as it means the model is more certain about predicting the next word.
2. **Bits Per Character (BPC):**
    
    - **Definition:** BPC measures the average number of bits needed to encode each character in the sequence.
    - **Lower is Better:** Similar to perplexity, lower BPC values indicate better language modeling performance.
3. **Human Evaluation:**
    
    - **Comparison to Human Writing:** Qualitative evaluation involves assessing how natural the generated text is compared to human-written text.
    - **Fluency and Coherence:** Human evaluators judge the fluency and coherence of generated language.
4. **Downstream Task Performance:**
    
    - **Transferability:** The model's performance on downstream tasks (e.g., sentiment analysis, text classification) can serve as a practical measure of language understanding and generation capabilities.
5. **ROUGE and BLEU Scores:**
    
    - **Applicability to Text Generation Tasks:** In tasks involving text generation (e.g., summarization), ROUGE and BLEU scores can be used to evaluate the quality of generated sequences compared to reference summaries.

Neural language modeling has advanced significantly with transformer-based architectures like GPT and BERT, and evaluating the quality of language models involves a combination of quantitative metrics like perplexity and qualitative assessments such as human evaluation.