
https://lena-voita.github.io/nlp_course/word_embeddings.html

<font color="green">Доделать:</font>
	* Word embeddings (зацени ниже, [[#Word Embeddings | норм]]?)
	*  Word2vec properties
<font color="red"> This text is red </font>

#### Word Embeddings

![](https://lena-voita.github.io/resources/lectures/word_emb/word_repr_intro-min.png)

The way machine learning models "see" data is different from how we (humans) do. For example, we can easily understand the text "I saw a cat", but our models can not - they need vectors of features. Such vectors, or word embeddings, are representations of words which can be fed into your model.

##### How it works: Look-up Table (Vocabulary)

In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance. For each vocabulary word, a look-up table contains its embedding. This embedding can be found using the word index in the vocabulary (i.e., you to look up the embedding in the table using word index).
![](https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif)
To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary contains a special token UNK. Alternatively, unknown tokens can be ignored or assigned a zero vector.
![[Pasted image 20240115204117.png|400]]

#### W2V basics

<b><u>Word2Vec</u></b> is an iterative method. Its main idea is as follows:
- take a huge text corpus;
- go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window);
- for the central word, compute probabilities of context words;
- adjust the vectors to increase these probabilities.

![[Pasted image 20240114154000.png]]

Calculating $P(\omega_{t+j}|\omega_t , \theta)$
($\theta$ is for all variables to be optimized)

![[Pasted image 20240114155642.png]]
For each word $\omega$ we will have two vectors:
- $v_{\omega}$ when it is a central word;
- $u_{\omega}$ when it is a context word.
Then for the central word $c$ ($c$ - central) and the context word $o$ ($o$ - outside word) probability of the context word is
![[Pasted image 20240114154927.png]]

The objective function (aka loss function or cost function) $J(\theta)$ is the average negative log-likelihood:
![[Pasted image 20240114160210.png]]
For each position $t=1,…,T$ in a text corpus, Word2Vec predicts context words within a m-sized window given the central word $\omega_t$.

By making an update to minimize $J_{t, j}(\theta)$, we force the parameters to increase similarity (dot product) of $v_{cat}$ and $u_{cute}$ and, at the same time, to decrease similarity between $v_{cat}$ and $u_{\omega}$ for all other words $\omega$ in the vocabulary.

![[Pasted image 20240114162848.png]]

#### Negative Sampling

At the current step we consider context vectors <u>not for all words</u>, but only <u>for the current target (cute) and several randomly chosen words</u>.
![[Pasted image 20240114164224.png]]
now we decrease similarity between $u_{cat}$ and context vectors <u>not for all</u> words, but only with a <u>subset of K "negative" examples</u>.
Since we have a *large corpus*, on average over all updates we will update *each vector sufficient number of times*, and the vectors will still be able to learn the relationships between words quite well.

The new <u><b>loss function</u></b> is
$$ J_{t,j}(\theta)=
    -\log\sigma({u_{cute}^T}{v_{cat}}) -
    \sum\limits_{w\in \{w_{i_1},\dots, w_{i_K}\}}\log\sigma({-{u_w^T}{v_{cat}}}) $$
$\omega_{i_1}, ..., \omega_{i_k}$ are K negative examples chosen at this step.
<u>The sigmoid function</u>$$\sigma(x)=\frac{1}{1+e^{-x}}$$

#### Skip-Gram and CBOW
<u>Skip-Gram</u> is the model we considered so far: it predicts context words given the central word. Skip-Gram with negative sampling is the most popular approach.

<u>CBOW (Continuous Bag-of-Words)</u> predicts the central word from the sum of context vectors. This simple sum of word vectors is called "bag of words", which gives the name for the model.

![[Pasted image 20240114184535.png]]