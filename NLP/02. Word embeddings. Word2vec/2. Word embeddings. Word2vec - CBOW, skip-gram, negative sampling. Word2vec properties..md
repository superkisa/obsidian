
https://lena-voita.github.io/nlp_course/word_embeddings.html

**Word2Vec** is an iterative method. Its main idea is as follows:
- take a huge text corpus;
- go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window);
- for the central word, compute probabilities of context words;
- adjust the vectors to increase these probabilities.

![[Pasted image 20240114154000.png]]

Calculating $P(\omega_{t+j}|\omega_t , \theta)$
For each word $\omega$ we will have two vectors:
- $v_{\omega}$ when it is a central word;
- $u_{\omega}$ when it is a context word.
Then for the central word $c$ ($c$ - central) and the context word $o$ ($o$ - outside word) probability of the context word is
![[Pasted image 20240114154927.png]]