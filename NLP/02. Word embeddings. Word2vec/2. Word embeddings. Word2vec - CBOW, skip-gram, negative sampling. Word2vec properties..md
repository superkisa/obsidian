
[Источник](https://lena-voita.github.io/nlp_course/word_embeddings.html)

<font color="green" size=30>Доделать:</font>
* Word embeddings (зацените ниже, [[#Word Embeddings | норм]]?)
* Word2vec properties (зацените ниже, [[#Word2vec properties| норм]]?)


#### Word Embeddings

![](https://lena-voita.github.io/resources/lectures/word_emb/word_repr_intro-min.png)

The way machine learning models "see" data is different from how we (humans) do. For example, we can easily understand the text "I saw a cat", but our models can not - they need vectors of features. Such vectors, or word embeddings, are representations of words which can be fed into your model.

##### How it works: Look-up Table (Vocabulary)

In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance. For each vocabulary word, a look-up table contains its embedding. This embedding can be found using the word index in the vocabulary (i.e., you to look up the embedding in the table using word index).
![](https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif)
To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary contains a special token UNK. Alternatively, unknown tokens can be ignored or assigned a zero vector.
![[Pasted image 20240115204117.png|400]]
Из лекции:
![[Pasted image 20240117180232.png]]
![[Pasted image 20240117180308.png]]
![[Pasted image 20240117180330.png]]


#### Word2Vec basics

<b><u>Word2Vec</u></b> is an iterative method. Its main idea is as follows:
- take a huge text corpus;
- go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window);
- for the central word, compute probabilities of context words;
- adjust the vectors to increase these probabilities.

![[Pasted image 20240114154000.png]]

Calculating $P(\omega_{t+j}|\omega_t , \theta)$
($\theta$ is for all variables to be optimized)

![[Pasted image 20240114155642.png]]
For each word $\omega$ we will have two vectors:
- $v_{\omega}$ when it is a central word;
- $u_{\omega}$ when it is a context word.
Then for the central word $c$ ($c$ - central) and the context word $o$ ($o$ - outside word) probability of the context word is
![[Pasted image 20240114154927.png]]

The objective function (aka loss function or cost function) $J(\theta)$ is the average negative log-likelihood:
![[Pasted image 20240114160210.png]]
For each position $t=1,…,T$ in a text corpus, Word2Vec predicts context words within a m-sized window given the central word $\omega_t$.

By making an update to minimize $J_{t, j}(\theta)$, we force the parameters to increase similarity (dot product) of $v_{cat}$ and $u_{cute}$ and, at the same time, to decrease similarity between $v_{cat}$ and $u_{\omega}$ for all other words $\omega$ in the vocabulary.

![[Pasted image 20240114162848.png|800]]

#### Negative Sampling

At the current step we consider context vectors <u>not for all words</u>, but only <u>for the current target (cute) and several randomly chosen words</u>.
![[Pasted image 20240114164224.png]]
now we decrease similarity between $u_{cat}$ and context vectors <u>not for all</u> words, but only with a <u>subset of K "negative" examples</u>.
Since we have a *large corpus*, on average over all updates we will update *each vector sufficient number of times*, and the vectors will still be able to learn the relationships between words quite well.

The new <u><b>loss function</u></b> is
$$ J_{t,j}(\theta)=
    -\log\sigma({u_{cute}^T}{v_{cat}}) -
    \sum\limits_{w\in \{w_{i_1},\dots, w_{i_K}\}}\log\sigma({-{u_w^T}{v_{cat}}}) $$
$\omega_{i_1}, ..., \omega_{i_k}$ are K negative examples chosen at this step.
<u>The sigmoid function</u>$$\sigma(x)=\frac{1}{1+e^{-x}}$$
Из лекций:
![[Pasted image 20240117180637.png]]
![[Pasted image 20240117180651.png]]

#### Skip-Gram and CBOW
<u>Skip-Gram</u> is the model we considered so far: it predicts context words given the central word. Skip-Gram with negative sampling is the most popular approach.

<u>CBOW (Continuous Bag-of-Words)</u> predicts the central word from the sum of context vectors. This simple sum of word vectors is called "bag of words", which gives the name for the model.

![[Pasted image 20240114184535.png]]
![[Pasted image 20240117180557.png]]

## Word2vec properties
==композиция чатика, гугла и своих представлений, проверить, обсудить==
* Word2Vec представляет слова в виде векторов в многомерном пространстве, где семантически близкие слова имеют близкие по метрике векторы.
* Размерность векторов (embedding size) является параметром, который можно настроить в процессе обучения и влияет на количество компонент в векторе, представляющем слово.
* Способность учесть контекстуальную информацию делает его мощным для понимания смысла слов в различных контекстах.
* Word2Vec не учитывает контекст на уровне предложений. Глубина учета контекста зависит от ширины окна, применяющегося при обучении модели.
* Векторы слов могут не учитывать полностью сложные лингвистические и семантические отношения в предложениях, многозначность слов.
* Оба метода (Skip-gram и CBOW) не учитывают порядок слов в предложении, что может быть критичным в некоторых задачах.

**Word Embeddings:**

Word embeddings are numerical representations of words in a continuous vector space, designed to capture semantic relationships between words. Unlike traditional methods that represent words as discrete symbols, word embeddings place words in a high-dimensional space where the proximity of vectors reflects the semantic similarity of words.

**Word2Vec:**

Word2Vec is a popular word embedding technique introduced by Tomas Mikolov and his colleagues at Google. It utilizes neural networks to learn distributed representations of words based on their contextual usage.

**CBOW (Continuous Bag of Words):**

CBOW is one of the architectures employed by Word2Vec. In CBOW, the model predicts the target word based on the context words surrounding it. It takes the average of the word vectors in the context window to predict the target word. CBOW is computationally efficient and works well for frequent words.

**Skip-Gram:**

Contrary to CBOW, Skip-Gram predicts context words given the target word. It considers each context-target pair individually and aims to maximize the probability of predicting context words. Skip-Gram is particularly effective for capturing semantic relationships and performs well for infrequent words.

**Negative Sampling:**

Training Word2Vec models on large datasets can be computationally intensive. Negative sampling is a technique used to address this challenge. Instead of adjusting all non-target words' weights during training, negative sampling randomly selects a small number of negative samples (words not in the context) and updates their weights. This reduces computational complexity while maintaining the quality of learned word embeddings.

**Word2Vec Properties:**

1. **Semantic Similarity:**
    
    - Words with similar meanings have similar vector representations. The model captures semantic relationships, allowing for similarity comparisons between words.
2. **Analogies:**
    
    - Word2Vec embeddings often exhibit analogy relationships. For example, the vector for "king" minus "man" plus "woman" results in a vector close to "queen." This demonstrates the model's ability to capture linguistic regularities.
3. **Vector Arithmetic:**
    
    - Vector operations like addition and subtraction on word vectors produce meaningful results. For instance, "king" - "man" + "woman" results in a vector close to "queen."
4. **Contextual Information:**
    
    - Word2Vec leverages contextual information to understand and represent word meanings. The model learns to associate words based on their surrounding context, capturing nuances in semantics.

In summary, Word2Vec, through CBOW, Skip-Gram, and the use of negative sampling, provides a powerful framework for generating word embeddings. The resulting embeddings possess properties such as semantic similarity, analogy relationships, and the ability to perform vector arithmetic, making them valuable for a variety of natural language processing tasks.

Word2Vec technique step-by-step:
**1. **Corpus Preparation:**

- Gather a large corpus of text data. This could be a collection of documents, articles, or any sizable text dataset.

**2. **Text Preprocessing:**

- Clean and preprocess the text data by removing stop words, punctuation, and any irrelevant information. Convert words to lowercase to ensure uniformity.

**3. **Build Vocabulary:**

- Create a vocabulary of unique words present in the corpus. Assign a unique index to each word.

**4. **Context-Target Pair Creation:**

- For each word in the corpus, create context-target pairs. The target word is the word of interest, and the context words are the words surrounding it within a specified window size.

**5. **One-Hot Encoding:**

- Represent each word in the context and target pairs using one-hot encoding. This converts words into binary vectors with a length equal to the vocabulary size, with only one '1' indicating the position of the word in the vocabulary.

**6. **Architecture Selection (Skip-Gram):**

- Choose the Skip-Gram architecture, where the model aims to predict context words given the target word.

**7. **Neural Network Setup:**

- Create a neural network with an input layer representing the one-hot encoded target word and an output layer with softmax activation, representing the probabilities of context words.

**8. **Embedding Layer:**

- Introduce an embedding layer with weights connecting the input and output layers. These weights are the word embeddings that the model learns during training.

**9. **Training Setup:**

- Define a loss function, often cross-entropy loss, to measure the difference between predicted and actual context word probabilities. Use backpropagation and gradient descent to minimize this loss.

**10. **Training Process:**

- Iterate through the corpus multiple times, updating the weights of the neural network to minimize the loss. The embeddings in the embedding layer are adjusted during this process.

**11. **Vector Space Representation:**

- Once training is complete, the learned weights in the embedding layer represent the continuous vector space embeddings of words. These vectors capture semantic relationships between words based on their contextual usage.

**12. **Evaluation (Optional):**

- Evaluate the quality of word embeddings using intrinsic or extrinsic evaluation tasks. Intrinsic tasks may include similarity or analogy tests, while extrinsic tasks involve using embeddings for downstream NLP applications.

Вормап:
![[Pasted image 20240117180903.png]]
![[Pasted image 20240117181008.png]]
![[Pasted image 20240117181020.png]]
