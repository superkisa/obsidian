
https://lena-voita.github.io/nlp_course/word_embeddings.html


## Word2Vec

#### W2V basics

<b><u>Word2Vec</u></b> is an iterative method. Its main idea is as follows:
- take a huge text corpus;
- go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window);
- for the central word, compute probabilities of context words;
- adjust the vectors to increase these probabilities.

![[Pasted image 20240114154000.png]]

Calculating $P(\omega_{t+j}|\omega_t , \theta)$
($\theta$ is for all variables to be optimized)

![[Pasted image 20240114155642.png]]
For each word $\omega$ we will have two vectors:
- $v_{\omega}$ when it is a central word;
- $u_{\omega}$ when it is a context word.
Then for the central word $c$ ($c$ - central) and the context word $o$ ($o$ - outside word) probability of the context word is
![[Pasted image 20240114154927.png]]

The objective function (aka loss function or cost function) $J(\theta)$ is the average negative log-likelihood:
![[Pasted image 20240114160210.png]]
For each position $t=1,…,T$ in a text corpus, Word2Vec predicts context words within a m-sized window given the central word $\omega_t$.
![[Pasted image 20240114162848.png]]

#### Negative Sampling

At the current step we consider context vectors <u>not for all words</u>, but only <u>for the current target (cute) and several randomly chosen words</u>.
![[Pasted image 20240114164224.png]]
now we decrease similarity between $u_{cat}$ and context vectors <u>not for all</u> words, but only with a <u>subset of K "negative" examples</u>.
Since we have a large corpus, on average over all updates we will update each vector sufficient number of times, and the vectors will still be able to learn the relationships between words quite well.

The new <u>loss function</u> is
$$ J_{t,j}(\theta)=
    -\log\sigma({u_{cute}^T}{v_{cat}}) -
    \sum\limits_{w\in \{w_{i_1},\dots, w_{i_K}\}}\log\sigma({-{u_w^T}{v_{cat}}}) $$
    