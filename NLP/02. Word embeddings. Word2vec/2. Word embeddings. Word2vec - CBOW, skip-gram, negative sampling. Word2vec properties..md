
https://lena-voita.github.io/nlp_course/word_embeddings.html

#### Word2Vec
**Word2Vec** is an iterative method. Its main idea is as follows:
- take a huge text corpus;
- go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window);
- for the central word, compute probabilities of context words;
- adjust the vectors to increase these probabilities.

![[Pasted image 20240114154000.png]]

Calculating $P(\omega_{t+j}|\omega_t , \theta)$
($\theta$ is for all variables to be optimized)

![[Pasted image 20240114155642.png]]
For each word $\omega$ we will have two vectors:
- $v_{\omega}$ when it is a central word;
- $u_{\omega}$ when it is a context word.
Then for the central word $c$ ($c$ - central) and the context word $o$ ($o$ - outside word) probability of the context word is
![[Pasted image 20240114154927.png]]

The objective function (aka loss function or cost function) $J(\theta)$ is the average negative log-likelihood:
![[Pasted image 20240114160210.png]]
For each position $t=1,…,T$ in a text corpus, Word2Vec predicts context words within a m-sized window given the central word $\omega_t$.