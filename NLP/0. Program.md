[[1. BoW, TF-iDF]]
[[2. Word embeddings. Word2vec - CBOW, skip-gram, negative sampling. Word2vec properties.]]
[[3. CNNs in text processing]]
[[4. Seq2seq, encoder-decoder models]]
[[5. Attention mechanism in encoder-decoder models (as of Bahdanau et.al.)]]
[[6. Machine translation. Problem setup, training and inference procedures. Quality metrics for MT]]
[[7. Unsupervised translation approach]]
[[8. Transformer architecture. Self-attention mechanism in transformers]]
[[9. Transformer architecture - blocks structure, layer norm, training procedure (adamW, lr scheduling, label smoothing)]]
[[10. BERT pretraining. MLM and NSP objectives]]
[[11. BERT finetuning. Transfer learning paradigm. Finetuning strategies]]
[[12. Discriminative transformer models (ROBERTA, ELECTRA, ALBERT)]]
[[13. Question-answering systems]]
[[14. Self-critical sequence training (SCST, RL-related approach in NLP)]]
[[15. Neural language modeling. Problem setup, key architectures (GPT-based or analogous). Quality metrics for LM]]
[[16. Transfer learning paradigm. T5, instruction-tuning (T0)]]
[[17. Generative Pre-Training. Large language models. Basic LLM adaptation (zero-shot, few-shot prompting)]]
[[NLP/18. Transfer learning with LLMs/18. Transfer learning with LLMs. Finetuning strategies - full finetuning, p-tuning, adapters| 18. Transfer learning with LLMs. Finetuning strategies - full finetuning, p-tuning, adapters]]
[[19. Learning from human feedback for LLMs. ChatGPT alignment (RLHF)]]
  
[[0. Glossary | Glossary]]
[[0. Шпора от чатика | Шпора от чатика]]


| Model Name  | Architecture                | Key Features                                                                                      | Encoder | Decoder | Tasks It Solves                                       | G/D | Loss Function                        | Training Method                                                      |
|-------------|-----------------------------|--------------------------------------------------------------------------------------------------|---------|---------|-------------------------------------------------------|-----|--------------------------------------|---------------------------------------------------------------------|
| Word2Vec    | Skip-gram or CBOW           | Distributed word representations, semantic similarity, analogy completion                        | -       | -       | Word embeddings, similarity, analogy tasks             | G   | CrossEntropy Loss                                  | Unsupervised pretraining on large corpora                           |
| Seq2Seq     | Encoder-Decoder (RNN, LSTM)  | Sequential modeling, attention mechanism, parallel text generation                               | Encoder | Decoder | Machine Translation, Summarization, Text Generation   | G   | CrossEntropy Loss                   | Supervised learning with parallel sequences                         |
| BiDAF       | BiDAF Architecture           | Bi-directional attention flow, dynamic passage-query interactions, contextual embeddings        | BiDAF   | -       | Question Answering, Reading Comprehension              | D   | CrossEntropy Loss                   | Supervised learning with SQuAD dataset                              |
| Transformer | Transformer Architecture     | Attention mechanism, parallel processing, scalability, position embeddings                      | Encoder | Decoder | Various NLP tasks (e.g., translation, classification) | Both (depending on usage)  | Various (e.g., CrossEntropy, Mean Squared Error) | Unsupervised pretraining on large corpora                           |
| BERT        | Transformer Architecture     | Bidirectional context understanding, attention mechanism, contextual embeddings                   | Encoder | -       | Sentence classification, NER, QA, language understanding | D   | Masked Language Model (MLM)         | Supervised fine-tuning on task-specific datasets                   |
| RoBERTa      | Transformer Architecture     | Improved training strategies, removal of NSP, dynamic masking, robust pretraining                 | Encoder | -       | Similar to BERT, improved training strategies          | D   | Masked Language Model (MLM)         | Supervised fine-tuning on task-specific datasets                   |
| ALBERT      | Transformer Architecture     | Parameter reduction techniques, improved efficiency, comparable performance to BERT               | Encoder | -       | Similar to BERT with parameter reduction              | D   | Masked Language Model (MLM)         | Supervised fine-tuning on task-specific datasets                   |
| Electra      | Transformer Architecture     | Token replacement with plausible alternatives, adversarial training paradigm, efficient training | Encoder | -       | Various NLP tasks with a focus on efficiency          | D   | Adversarial Loss, Discriminative Loss | Pretraining with a generator-discriminator architecture             |
| GPT         | Transformer Architecture     | Autoregressive language modeling, generative capabilities, sequential context understanding    | -       | Decoder | Text generation, language modeling                    | G   | Autoregressive Loss (e.g., CrossEntropy) | Unsupervised pretraining on large corpora                           |
| T5          | Transformer Architecture     | Unified text-to-text framework, flexible task representation, diverse NLP tasks                   | Both    | Both    | Text-to-text tasks (e.g., summarization, translation) | Both (generative)          | Various (customized task-specific objectives) | Unsupervised pretraining on large corpora                           |
| T0          | Not widely recognized       | N/A                                                                                              | -       | -       | N/A                                                   | N/A | N/A                                  | N/A                                                                 |

