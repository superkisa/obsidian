[[1. BoW, TF-iDF]]
[[2. Word embeddings. Word2vec - CBOW, skip-gram, negative sampling. Word2vec properties.]]
[[3. CNNs in text processing]]
[[4. Seq2seq, encoder-decoder models]]
[[5. Attention mechanism in encoder-decoder models (as of Bahdanau et.al.)]]
[[6. Machine translation. Problem setup, training / inference procedures. Quality metrics for MT]]
[[7. Unsupervised translation approach]]
[[8. Transformer architecture. Self-attention mechanism in transformers]]
[[9. Transformer architecture - blocks structure, layer norm, training procedure (adamW, lr scheduling, label smoothing)]]
[[10. BERT pretraining. MLM and NSP objectives]]
[[11. BERT finetuning. Transfer learning paradigm. Finetuning strategies]]
[[12. Discriminative transformer models (ROBERTA, ELECTRA, ALBERT)]]
[[13. Question-answering systems]]
[[14. Self-critical sequence training (SCST, RL-related approach in NLP)]]
[[15. Neural language modeling. Problem setup, key architectures (GPT-based or analogous). Quality metrics for LM]]
[[16. Transfer learning paradigm. T5, instruction-tuning (T0)]]
[[17. Generative pretraining. Large language models. Basic LLM adaptation (zero-shot, few-shot prompting)]]
[[18. Transfer learning with LLMs. Finetuning strategies: full finetuning, p-tuning, adapters]]
[[19. Learning from human feedback for LLMs. ChatGPT alignment (RLHF)]]