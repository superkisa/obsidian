Анализ статьи:
>[!info]- Systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a “baseline” to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure.
>Системы построены с использованием нового подхода к оптимизации, который мы называем самокритичным последовательным обучением (SCST). SCST — это форма популярного алгоритма REINFORCE, который вместо оценки «базового уровня» для нормализации вознаграждений и уменьшения дисперсии использует результаты собственного алгоритма вывода во время тестирования для нормализации получаемых вознаграждений. Используя этот подход, можно избежать оценки сигнала вознаграждения (как это должны делать методы актер-критик) и оценки нормализации (как обычно делают алгоритмы REINFORCE), в то же время гармонизируя модель относительно ее процедуры вывода во время тестирования.

Информация от чатика:
Self-critical sequence training (SCST) is an optimization technique used in the field of machine learning, particularly within the realm of reinforcement learning (RL) and natural language processing (NLP). SCST is designed to train models directly on non-differentiable metrics, which are often the actual performance metrics one cares about in practice.

The technique was introduced in the context of image captioning, where the goal is to generate descriptive text for a given image. Traditional training methods for such tasks rely on maximizing the likelihood of the next correct word given the previous words and the image, which often leads to exposure bias and discrepancies between the training objective and the evaluation metrics.

SCST addresses these issues by utilizing the REINFORCE algorithm, a popular method in RL, but with a twist. Instead of estimating a separate baseline to normalize the rewards and reduce variance, SCST uses the model's own output at test time to normalize the rewards. This is done by comparing the reward of a sampled sequence (using the model's current parameters) with the reward of the "greedy" sequence (the sequence chosen by the model using a greedy decoding strategy). The difference between these two rewards is used to adjust the model parameters, encouraging the model to favor sequences that perform better according to the evaluation metric.

Here's a high-level overview of how SCST works:

- The model generates two sequences: one sampled according to the probability distribution of the next word (sampled sequence) and another generated greedily by always choosing the word with the highest probability at each step (baseline sequence).
- Both sequences are evaluated using a non-differentiable metric (like BLEU or CIDEr for captioning tasks), which provides a reward signal.
- The reward for the sampled sequence is compared to the reward for the greedy sequence. If the sampled sequence performs better, the model is encouraged to make such choices more likely in the future.
- The model parameters are updated to minimize the loss that takes into account the difference in rewards, effectively pushing the model to improve its performance on the actual evaluation metric.

This approach has been shown to harmonize the model with its test-time behavior, leading to better performance on the evaluation metrics used for tasks like image captioning. The SCST method has set new state-of-the-art results on benchmarks like MSCOCO for image captioning, improving the performance on metrics like CIDEr [1](https://arxiv.org/abs/1612.00563).

To summarize, SCST is a reinforcement learning-based approach that improves the training of sequence models by using the model's own output as a baseline for reward normalization, directly optimizing for the desired evaluation metrics without the need for a separate baseline estimator.



[Лекция Радослава](https://www.youtube.com/watch?v=Jv_L42ghwTE)
![[Pasted image 20240117222925.png]]
![[Pasted image 20240117223107.png]]
Что такое baseline?
Decoder в жадном семплировании

[Разбор видео](https://www.youtube.com/watch?v=UnT5wTe13yc)
![[Pasted image 20240117225032.png]]
Было выбрано несколько методов компенсации exposure bias.
Лучшим методом выбран RL via Policy gradient
(Можно напрямую оптимизировать недифференцируемые показатели)
Рассматриваем проблему (подписи к картинкам) с точки зрения RL:
- Enviroment - изображения;
- Actions - слова, которые мы сгенерировали;
- Policy;
- Reward - метрики - максимально увеличить ожидаемое награждение в будущем;
## *Policy gradient*
Проблема метода - имеет тенденцию к высокой дисперсии. Поэтому добавляют baseline, для уменьшения дисперсии градиента.
(В лекции Радослав объяснял, что всё это выражение, если система уже хорошо обучена, даст 0)
![[Pasted image 20240117225752.png]]
Методика является беспристрастной
![[Pasted image 20240117225855.png]]
![[Pasted image 20240117225936.png]]
![[Pasted image 20240117225947.png]]
![[Pasted image 20240117230002.png]]
![[Pasted image 20240117230040.png]]
Идея SCST заключается в том, чтобы исходить из результатов алгоритма вывода времени тестирования
При награждении выдаём их только тем моделям, результат которых лучше уже полученного
![[Pasted image 20240117230242.png]]
Обычно используют greedy параметр
![[Pasted image 20240117230718.png]]
![[Pasted image 20240117230535.png]]

