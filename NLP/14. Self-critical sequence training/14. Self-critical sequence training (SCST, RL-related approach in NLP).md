Информация от чатика:
Self-critical sequence training (SCST) is an optimization technique used in the field of machine learning, particularly within the realm of reinforcement learning (RL) and natural language processing (NLP). SCST is designed to train models directly on non-differentiable metrics, which are often the actual performance metrics one cares about in practice.

The technique was introduced in the context of image captioning, where the goal is to generate descriptive text for a given image. Traditional training methods for such tasks rely on maximizing the likelihood of the next correct word given the previous words and the image, which often leads to exposure bias and discrepancies between the training objective and the evaluation metrics.

SCST addresses these issues by utilizing the REINFORCE algorithm, a popular method in RL, but with a twist. Instead of estimating a separate baseline to normalize the rewards and reduce variance, SCST uses the model's own output at test time to normalize the rewards. This is done by comparing the reward of a sampled sequence (using the model's current parameters) with the reward of the "greedy" sequence (the sequence chosen by the model using a greedy decoding strategy). The difference between these two rewards is used to adjust the model parameters, encouraging the model to favor sequences that perform better according to the evaluation metric.

Here's a high-level overview of how SCST works:

- The model generates two sequences: one sampled according to the probability distribution of the next word (sampled sequence) and another generated greedily by always choosing the word with the highest probability at each step (baseline sequence).
- Both sequences are evaluated using a non-differentiable metric (like BLEU or CIDEr for captioning tasks), which provides a reward signal.
- The reward for the sampled sequence is compared to the reward for the greedy sequence. If the sampled sequence performs better, the model is encouraged to make such choices more likely in the future.
- The model parameters are updated to minimize the loss that takes into account the difference in rewards, effectively pushing the model to improve its performance on the actual evaluation metric.

This approach has been shown to harmonize the model with its test-time behavior, leading to better performance on the evaluation metrics used for tasks like image captioning. The SCST method has set new state-of-the-art results on benchmarks like MSCOCO for image captioning, improving the performance on metrics like CIDEr [1](https://arxiv.org/abs/1612.00563).

To summarize, SCST is a reinforcement learning-based approach that improves the training of sequence models by using the model's own output as a baseline for reward normalization, directly optimizing for the desired evaluation metrics without the need for a separate baseline estimator.


[Лекция Радослава](https://www.youtube.com/watch?v=Jv_L42ghwTE)
![[Pasted image 20240117222925.png]]
![[Pasted image 20240117223107.png]]
Что такое baseline?
Decoder в жадном семплировании