Анализ статьи:
Systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a “baseline” to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure.
>[!info]- перевод
>Системы построены с использованием нового подхода к оптимизации, который мы называем самокритичным последовательным обучением (SCST). SCST — это форма популярного алгоритма REINFORCE, который вместо оценки «базового уровня» для нормализации вознаграждений и уменьшения дисперсии использует результаты собственного алгоритма вывода во время тестирования для нормализации получаемых вознаграждений. Используя этот подход, можно избежать оценки сигнала вознаграждения (как это должны делать методы актер-критик) и оценки нормализации (как обычно делают алгоритмы REINFORCE), в то же время гармонизируя модель относительно ее процедуры вывода во время тестирования.

Deep generative models for text are typically trained to maximize the likelihood of the next ground-truth word given the previous ground-truth word using back-propagation. This approach has been called “Teacher-Forcing”. However, this approach creates a mismatch between training and testing, since at test-time the model uses the previously generated words from the model distribution to predict the next word. This exposure bias, results in error accumulation during generation at test time, since the model has never been exposed to its own predictions.
>[!info]- перевод
>Глубокие генеративные модели для текста обычно обучаются так, чтобы максимизировать вероятность появления следующего достоверного слова с учетом предыдущего достоверного слова с использованием обратного распространения ошибки. Этот подход получил название «Принуждение учителя». Однако этот подход создает несоответствие между обучением и тестированием, поскольку во время тестирования модель использует ранее сгенерированные слова из распределения модели для прогнозирования следующего слова. Это смещение воздействия приводит к накоплению ошибок во время генерации во время тестирования, поскольку модель никогда не подвергалась воздействию собственных прогнозов.


Feeding back the model’s own predictions and slowly increasing the feedback probability p during training leads to significantly better test-time performance. Another line of work proposes “Professor-Forcing”, a technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when sampling freely from the network.
>[!info]- перевод
>Получение собственных прогнозов модели и медленное увеличение вероятности обратной связи p во время обучения приводит к значительному улучшению производительности во время тестирования. Другое направление работы предлагает «Профессор-форсинг», метод, который использует состязательное обучение, чтобы обеспечить одинаковую динамику рекуррентной сети при обучении, обусловленном истинностью предыдущих слов, и при свободной выборке из сети.

While sequence models are usually trained using the cross entropy loss, they are typically evaluated at test time using discrete and non-differentiable NLP metrics such as BLEU, ROUGE, METEOR or CIDEr. Ideally sequence models for image captioning should be trained to avoid exposure bias and directly optimize metrics for the task at hand.
Both the exposure bias and non-differentiable task metric issues can be addressed by incorporating techniques from Reinforcement Learning (RL). 
>[!info]- перевод
>Хотя модели последовательностей обычно обучаются с использованием перекрестной энтропийной потери, они обычно оцениваются во время тестирования с использованием дискретных и недифференцируемых показателей НЛП, таких как BLEU, ROUGE, METEOR или CIDEr. В идеале модели последовательностей для подписей к изображениям должны быть обучены, чтобы избежать систематической ошибки экспозиции и напрямую оптимизировать показатели для поставленной задачи.
Как систематическая ошибка воздействия, так и проблемы с недифференцируемыми метриками задач можно решить, используя методы обучения с подкреплением (RL).

The REINFORCE algorithm to directly optimize nondifferentiable, sequence-based test metrics, and overcome both issues. REINFORCE allows one to optimize the gradient of the expected reward by sampling from the model during training, and treating those samples as ground-truth labels (that are re-weighted by the reward they deliver). The major limitation of the approach is that the expected gradient computed using mini-batches under REINFORCE typically exhibit high variance, and without proper context-dependent normalization, is typically unstable.
The recent discovery that REINFORCE with proper bias correction using learned “baselines” is effective has led to a flurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference. Actor-critic methods , which instead train a second “critic” network to provide an estimate of the value of each generated word given the policy of an actor network, have also been investigated for sequence problems recently. These techniques overcome the need to sample from the policy’s (actors) action space, which can be enormous, at the expense of estimating future rewards, and training multiple networks based on one another’s outputs, which as explore, can also be unstable.


>[!info]- перевод
>Алгоритм REINFORCE позволяет напрямую оптимизировать недифференцируемые тестовые показатели на основе последовательностей и решить обе проблемы. REINFORCE позволяет оптимизировать градиент ожидаемого вознаграждения путем выборки из модели во время обучения и обработки этих выборок как меток базовой истины (которые повторно взвешиваются в зависимости от получаемого вознаграждения). Основным ограничением подхода является то, что ожидаемый градиент, рассчитанный с использованием мини-пакетов в рамках REINFORCE, обычно демонстрирует высокую дисперсию и без надлежащей контекстно-зависимой нормализации обычно нестабильен.
>Недавнее открытие того, что REINFORCE с правильной коррекцией смещения с использованием изученных «базовых линий» является эффективным, привело к шквалу работы по применению REINFORCE для решения проблем RL, обучения с учителем и вариационного вывода. Методы актор-критик, которые вместо этого обучают вторую сеть «критика» предоставлять оценку значения каждого сгенерированного слова с учетом политики сети акторов, также недавно исследовались для проблем последовательности. Эти методы устраняют необходимость выборки из пространства действий политики (актеров), которое может быть огромным, за счет оценки будущих вознаграждений и обучения нескольких сетей на основе результатов друг друга, которые, как выяснилось, также могут быть нестабильными.

 Captioning Models
In this section we describe the recurrent models that we use for caption generation.
FC models. We first encode the input image F using a deep CNN, and then embed it through a linear projection WI . Words are represented with one hot vectors that are embedded with a linear embedding E that has the same output dimension as WI . The beginning of each sentence is marked with a special BOS token, and the end with an EOS token. Under the model, words are generated and then fed back into the LSTM, with the image treated as the first word WICNN(F). The following updates for the hidden units and cells of an LSTM define the model:
>[!info]- перевод
>Модели субтитров
>В этом разделе мы описываем рекуррентные модели, которые мы используем для создания подписей.
>Модели ФК. Мы сначала кодируем входное изображение F с помощью глубокой CNN, а затем встраиваем его через линейную проекцию WI. Слова представлены одними горячими векторами, в которые встроено линейное вложение E, имеющее ту же выходную размерность, что и WI. Начало каждого предложения отмечается специальным токеном BOS, а конец — токеном EOS. В соответствии с моделью слова генерируются, а затем передаются обратно в LSTM, при этом изображение рассматривается как первое слово WICNN(F). Следующие обновления для скрытых модулей и ячеек LSTM определяют модель:
![[Pasted image 20240118113603.png]]

where φ is a maxout non-linearity with 2 units (⊗ denotes the units) and σ is the sigmoid function. We initialize $h_0$ and $c_0$ to zero. The LSTM outputs a distribution over the next word $w_t$ using the softmax function:
$w_t∼softmax(s_t)$
The hidden states and word and image embeddings have dimension 512. Let θ denote the parameters of the model. Traditionally the parameters θ are learned by maximizing the likelihood of the observed sequence. Specifically, given a target ground truth sequence ${w_1^{*}, . . . , w_T^{*}}$, the objective is to minimize the cross entropy loss (XE):
![[Pasted image 20240118114028.png]]

# Sequence Generation as an RL problem.


Captioning systems are traditionally trained using the cross entropy loss. To directly optimize NLP metrics and address the exposure bias issue, we can cast our generative models in the Reinforcement Learning terminology. Our recurrent models (LSTM) introduced above can be viewed as an “agent” that interacts with an external “environment” (words and image features). The parameters of the network, θ, define a policy pθ, that results in an “action” that is the prediction of the next word.
After each action, the agent (the LSTM) updates its internal “state” (cells and hidden states of the LSTM, attention weights etc). Upon generating the end-of-sequence (EOS) token, the agent observes a “reward” that is, for instance, the CIDEr score of the generated sentence—we denote this reward by r. The reward is computed by an evaluation metric by comparing the generated sequence to corresponding ground-truth sequences. The goal of training is to minimize the negative expected reward:

>[!info]- перевод
>Системы субтитров традиционно обучаются с использованием перекрестной потери энтропии. Чтобы напрямую оптимизировать показатели НЛП и решить проблему систематической ошибки воздействия, мы можем применить наши генеративные модели к терминологии обучения с подкреплением. Наши рекуррентные модели (LSTM), представленные выше, можно рассматривать как «агента», который взаимодействует с внешней «средой» (словами и признаками изображения). Параметры сети θ определяют политику pθ, результатом которой является «действие», то есть предсказание следующего слова.
>После каждого действия агент (LSTM) обновляет свое внутреннее «состояние» (ячейки и скрытые состояния LSTM, веса внимания и т. д.). После генерации токена конца последовательности (EOS) агент получает «награду», которая представляет собой, например, оценку CIDEr сгенерированного предложения — мы обозначаем это вознаграждение через r. Вознаграждение рассчитывается с помощью метрики оценки путем сравнения сгенерированной последовательности с соответствующими последовательностями наземных данных. Цель обучения — минимизировать отрицательное ожидаемое вознаграждение:
![[Pasted image 20240118114541.png]]

### Policy Gradient with REINFORCE. 

In order to compute the gradient $∇_θ L(θ)$, we use the REINFORCE algorithm. REINFORCE is based on the observation that the expected gradient of a nondifferentiable reward function can be computed as follows:
>[!info]- перевод
>Чтобы вычислить градиент ∇θL(θ), мы используем алгоритм REINFORCE. REINFORCE основан на наблюдении, что ожидаемый градиент недифференцируемой функции вознаграждения можно вычислить следующим образом:
![[Pasted image 20240118115137.png]]

In practice the expected gradient can be approximated using a single Monte-Carlo sample $w^s = (w^s_1 . . . w^s_T)$ from $p_θ$, for each training example in the minibatch:
![[Pasted image 20240118115250.png]]
### REINFORCE with a Baseline.
The policy gradient given by REINFORCE can be generalized to compute the reward associated with an action value relative to a reference reward or baseline b:
![[Pasted image 20240118115352.png]]
The baseline can be any arbitrary function, as long as it does not depend on the “action” $w^s$ since in this case:
>[!info]- перевод
>Базовой линией может быть любая произвольная функция, при условии, что она не зависит от «действия» $w^s$, поскольку в этом случае:

![[Pasted image 20240118115540.png]]
This shows that the baseline does not change the expected gradient, but importantly, it can reduce the the variance of the gradient estimate. For each training case, we againapproximate the expected  gradient with a single sample $w^s∼p_θ$:
Note that if $b$ is function of $θ$ or $t$, equation (6) still holds and $b(θ)$ is a valid baseline.

>[!info]- перевод
>Это показывает, что базовая линия не меняет ожидаемый градиент, но, что важно, она может уменьшить дисперсию оценки градиента. Для каждого обучающего случая мы снова аппроксимируем ожидаемый градиент с помощью одной выборки:
>Обратите внимание, что если является функцией или , уравнение (6) по-прежнему сохраняется и является допустимой базовой линией.

![[Pasted image 20240118115847.png]]

# THE MAIN IDEA
The central idea of the self-critical sequence training (SCST) approach is to baseline the REINFORCE algorithm with the reward obtained by the current model under the inference algorithm used at test time. The gradient of the negative reward of a sample $w^s$ from the model w.r.t. to the softmax activations at time-step t then becomes:
>[!info]- перевод
>Основная идея подхода самокритичного последовательного обучения (SCST) заключается в том, чтобы базировать алгоритм REINFORCE на вознаграждении, полученном текущей моделью в соответствии с алгоритмом вывода, используемым во время тестирования. Градиент отрицательного вознаграждения выборки $w^s$ из модели w.r.t. к активациям softmax на временном шаге t становится:

![[Pasted image 20240118120052.png]]
where $r(\hat{w})$ again is the reward obtained by the current model under the inference algorithm used at test time. Accordingly, samples from the model that return higher reward than $\hat{w}$ will be “pushed up”, or increased in probability, while samples which result in lower reward will be suppressed. Like MIXER, SCST has all the advantages of REINFORCE algorithms, as it directly optimizes the true, sequence-level, evaluation metric, but avoids the usual scenario of having to learn a (context-dependent) estimate of expected future rewards as a baseline. In practice we have found that SCST has much lower variance, and can be more effectively trained on mini-batches of samples using SGD.
Since the SCST baseline is based on the test-time estimate under the current model, SCST is forced to improve the performance of the model under the inference algorithm used at test time. This encourages training/test time consistency like the maximum likelihood-based approaches “Data as Demonstrator”, “Professor Forcing”, and E2E, but importantly, it can directly optimize  sequence metrics.Finally, SCST is self-critical, and so avoids all the inherent training difficulties associated with actor-critic methods, where a second “critic” network must be trained to estimate value functions, and the actor must be trained on estimated value functions rather than actual rewards. In this paper we focus on scenario of greedy decoding, where:

>[!info]- перевод
>где $r(\hat{w})$ — это вознаграждение, полученное текущей моделью в соответствии с алгоритмом вывода, используемым во время тестирования. Соответственно, выборки из модели, которые возвращают более высокое вознаграждение, чем $\hat{w}$, будут «подняты» или увеличены по вероятности, в то время как образцы, которые приводят к более низкому вознаграждению, будут подавлены. Как и MIXER, SCST обладает всеми преимуществами алгоритмов REINFORCE, поскольку он напрямую оптимизирует истинную метрику оценки на уровне последовательности, но позволяет избежать обычного сценария, когда необходимо изучить (зависящую от контекста) оценку ожидаемых будущих вознаграждений в качестве базовой линии. На практике мы обнаружили, что SCST имеет гораздо меньшую дисперсию и его можно более эффективно обучать на мини-партиях образцов с использованием SGD.
>Поскольку базовый уровень SCST основан на оценке времени тестирования в рамках текущей модели, SCST вынужден улучшать производительность модели в соответствии с алгоритмом вывода, используемым во время тестирования. Это способствует согласованности времени обучения/тестирования, как подходы, основанные на максимальном правдоподобии «Данные как демонстратор», «Принуждение профессора» и E2E, но, что важно, это может напрямую оптимизировать метрики последовательности. Наконец, SCST самокритичен и поэтому избегает всех присущие трудности обучения, связанные с методами актер-критик, когда вторая сеть «критиков» должна быть обучена оценивать функции ценности, а актер должен обучаться оценочным функциям ценности, а не фактическим вознаграждениям. В этой статье мы сосредоточимся на сценарии жадного декодирования, где:

$\hat{w}=arg max{p(w_t|h_t)}$

This choice, depicted in Figure 1, has several practical advantages. First and foremost, it minimizes the impact of baselining with the test-time inference algorithm on training time, since it requires only one additional forward pass, and trains the system to be optimized for fast, greedy decoding at test-time. This choice may also be among the best forms of SCST based on a single test-time estimate, as suppressing all samples that underperform relative to the final test-time estimate will tend to favor a very decisive policy.
The investigation of forms of SCST that incorporate margin, utilize more than 1 test-time estimate (e.g. an n-best list) to baseline, and/or more elaborate test-time inference procedures (e.g. beam search) are interesting possible directions of future work.
>[!info]- перевод
>Этот выбор, изображенный на рисунке 1, имеет несколько практических преимуществ. Прежде всего, он сводит к минимуму влияние определения базовой линии с помощью алгоритма вывода во время тестирования на время обучения, поскольку он требует только одного дополнительного прямого прохода и обучает систему оптимизации для быстрого жадного декодирования во время тестирования. Этот выбор также может быть одной из лучших форм SCST, основанных на одной оценке времени тестирования, поскольку подавление всех образцов, производительность которых ниже окончательной оценки времени тестирования, будет способствовать очень решительной политике.
>Исследование форм SCST, которые включают запас, используют более одной оценки времени тестирования (например, список n лучших) для базового уровня и/или более сложные процедуры вывода во время тестирования (например, поиск луча) представляют собой интересные возможные направления будущего. работа.

![[Pasted image 20240118120106.png]]
>[!info]- Figure 1
>Self-critical sequence training (SCST). The weight put on words of a sampled sentence from the model is determined by the difference between the reward for the sampled sentence and the reward obtained by the estimated sentence under the test-time inference procedure (greedy inference depicted). This harmonizes learning with the inference procedure, and lowers the variance of the gradients, improving the training procedure.
>[!info]- Рисунок 1
>Обучение самокритичной последовательности (SCST). Вес, придаваемый словам выборочного предложения из модели, определяется разницей между вознаграждением за выборочное предложение и вознаграждением, полученным оценочным предложением в рамках процедуры вывода во время тестирования (изображен жадный вывод). Это гармонизирует обучение с процедурой вывода и снижает дисперсию градиентов, улучшая процедуру обучения.



---

Информация от чатика:
Self-critical sequence training (SCST) is an optimization technique used in the field of machine learning, particularly within the realm of reinforcement learning (RL) and natural language processing (NLP). SCST is designed to train models directly on non-differentiable metrics, which are often the actual performance metrics one cares about in practice.

The technique was introduced in the context of image captioning, where the goal is to generate descriptive text for a given image. Traditional training methods for such tasks rely on maximizing the likelihood of the next correct word given the previous words and the image, which often leads to exposure bias and discrepancies between the training objective and the evaluation metrics.

SCST addresses these issues by utilizing the REINFORCE algorithm, a popular method in RL, but with a twist. Instead of estimating a separate baseline to normalize the rewards and reduce variance, SCST uses the model's own output at test time to normalize the rewards. This is done by comparing the reward of a sampled sequence (using the model's current parameters) with the reward of the "greedy" sequence (the sequence chosen by the model using a greedy decoding strategy). The difference between these two rewards is used to adjust the model parameters, encouraging the model to favor sequences that perform better according to the evaluation metric.

Here's a high-level overview of how SCST works:

- The model generates two sequences: one sampled according to the probability distribution of the next word (sampled sequence) and another generated greedily by always choosing the word with the highest probability at each step (baseline sequence).
- Both sequences are evaluated using a non-differentiable metric (like BLEU or CIDEr for captioning tasks), which provides a reward signal.
- The reward for the sampled sequence is compared to the reward for the greedy sequence. If the sampled sequence performs better, the model is encouraged to make such choices more likely in the future.
- The model parameters are updated to minimize the loss that takes into account the difference in rewards, effectively pushing the model to improve its performance on the actual evaluation metric.

This approach has been shown to harmonize the model with its test-time behavior, leading to better performance on the evaluation metrics used for tasks like image captioning. The SCST method has set new state-of-the-art results on benchmarks like MSCOCO for image captioning, improving the performance on metrics like CIDEr [1](https://arxiv.org/abs/1612.00563).

To summarize, SCST is a reinforcement learning-based approach that improves the training of sequence models by using the model's own output as a baseline for reward normalization, directly optimizing for the desired evaluation metrics without the need for a separate baseline estimator.



[Лекция Радослава](https://www.youtube.com/watch?v=Jv_L42ghwTE)
![[Pasted image 20240117222925.png]]
![[Pasted image 20240117223107.png]]
Что такое baseline?
Decoder в жадном семплировании

[Разбор видео](https://www.youtube.com/watch?v=UnT5wTe13yc)
![[Pasted image 20240117225032.png]]
Было выбрано несколько методов компенсации exposure bias.
Лучшим методом выбран RL via Policy gradient
(Можно напрямую оптимизировать недифференцируемые показатели)
Рассматриваем проблему (подписи к картинкам) с точки зрения RL:
- Enviroment - изображения;
- Actions - слова, которые мы сгенерировали;
- Policy;
- Reward - метрики - максимально увеличить ожидаемое награждение в будущем;
## *Policy gradient*
Проблема метода - имеет тенденцию к высокой дисперсии. Поэтому добавляют baseline, для уменьшения дисперсии градиента.
(В лекции Радослав объяснял, что всё это выражение, если система уже хорошо обучена, даст 0)
![[Pasted image 20240117225752.png]]
Методика является беспристрастной
![[Pasted image 20240117225855.png]]
![[Pasted image 20240117225936.png]]
![[Pasted image 20240117225947.png]]
![[Pasted image 20240117230002.png]]
![[Pasted image 20240117230040.png]]
Идея SCST заключается в том, чтобы исходить из результатов алгоритма вывода времени тестирования
При награждении выдаём их только тем моделям, результат которых лучше уже полученного
![[Pasted image 20240117230242.png]]
Обычно используют greedy параметр
![[Pasted image 20240117230718.png]]
![[Pasted image 20240117230535.png]]

