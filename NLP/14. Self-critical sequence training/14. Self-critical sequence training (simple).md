Let's break down Self-Critical Sequence Training (SCST) in simpler terms:

1. **Regular Training vs. SCST:**
    
    - Normally, when training a language model, we teach it to generate sentences that look similar to what we already have. It's like learning to write by imitating existing texts.
        
    - SCST, on the other hand, teaches the model to write better sentences by looking at how well its sentences match what we consider good or accurate. It's like learning to write well by getting feedback on the quality of your writing.
        
2. **The Improvement Idea:**
    
    - SCST cares about how good the sentences are, not just how similar they are to what it has seen before.
        
    - Instead of saying "write something like this," SCST says "try to write something good, and I'll tell you how good it is."
        
3. **Comparing to a Baseline:**
    
    - SCST compares what the model writes to a baseline, which is like a reference point or a standard. This baseline helps measure how good the sentences are.
4. **Adjusting to Write Better:**
    
    - If the model writes something better than expected (according to the baseline), it gets a pat on the back (positive reward).
        
    - If it writes something not as good, it learns to do better next time (negative reward).
        
5. **Getting Better with Feedback:**
    
    - The model keeps getting better by continuously adjusting its writing approach based on the feedback it receives about the quality of its sentences.

In a nutshell, SCST is like teaching a language model to write by focusing on how good its sentences are, using feedback to improve, rather than just copying what it has seen before. It's a bit like learning to write well by aiming for quality rather than imitation.

**Main idea**
Self-Critical Sequence Training (SCST) is a reinforcement learning (RL)-related approach used in natural language processing (NLP), particularly for sequence generation tasks such as machine translation and image captioning. The main idea behind SCST is to train sequence generation models by directly optimizing the evaluation metric, such as BLEU score or CIDEr, rather than using traditional maximum likelihood estimation (MLE) objectives. Here's the main idea and key components of SCST:

1. **Objective Function:**
    
    - Unlike MLE, where the model is trained to maximize the likelihood of the ground truth sequences, SCST optimizes a reward-based objective.
    - The reward is typically a performance metric like BLEU score, which evaluates the quality of generated sequences.
2. **Baseline Comparison:**
    
    - SCST introduces a baseline to compare against the model's generated sequences.
    - The baseline is usually the expected reward under the current model distribution.
3. **Training Process:**
    
    - In the training process, the model generates a sequence, and the baseline is used to compute an advantage or reward.
    - The advantage is the difference between the actual reward (e.g., BLEU score) and the baseline.
    - The model is then updated based on this advantage using policy gradient methods.
4. **Policy Gradient Optimization:**
    
    - SCST employs policy gradient optimization to update the model's parameters.
    - The objective is to increase the probability of generating sequences with higher rewards and decrease the probability of sequences with lower rewards.
5. **Reinforcement Learning Framework:**
    
    - SCST can be seen as a form of reinforcement learning, where the model learns to generate sequences that are more likely to receive higher rewards according to the chosen evaluation metric.
6. **Sample Efficiency:**
    
    - By directly optimizing for the evaluation metric, SCST can potentially lead to more sample-efficient training.
    - It allows the model to focus on generating sequences that perform well in terms of the desired evaluation metric.
7. **Applications:**
    
    - SCST has been applied to various sequence generation tasks in NLP, including machine translation, image captioning, and text summarization.
    - It has shown effectiveness in improving the performance of models in terms of the chosen evaluation metrics.

SCST provides an alternative training paradigm for sequence generation models, offering the advantage of directly optimizing for task-specific evaluation metrics, which can be beneficial in scenarios where these metrics align well with the desired model performance.

**How BLEU is calculated in SCST**
Self-Critical Sequence Training (SCST) utilizes BLEU as an evaluation metric during reinforcement learning training. The calculation of BLEU in SCST follows a similar process to the standard BLEU calculation but with a few nuances due to its use in a reinforcement learning context. Here's a simplified explanation:

1. **Generate Candidate Sequences:**
    
    - The model generates candidate sequences during training, which are the machine-generated sequences that you want to evaluate using BLEU.
2. **Compute BLEU against Baseline:**
    
    - SCST introduces a baseline comparison, which is often the expected reward under the current model distribution. This baseline is used to calculate an advantage.
        
    - For each candidate sequence, BLEU is computed not just against ground truth references but also against the baseline.
        
3. **Advantage Calculation:**
    
    - The advantage is the difference between the BLEU score of the candidate sequence and the BLEU score of the baseline.
        
    - Advantage = BLEU_candidate - BLEU_baseline
        
4. **Policy Gradient Optimization:**
    
    - The model is then updated based on this advantage using policy gradient optimization.
        
    - The objective is to increase the probability of generating sequences that receive higher BLEU scores compared to the baseline and decrease the probability of sequences with lower scores.
        
5. **Repeat and Update:**
    
    - The training process iterates, with the model generating new sequences, computing BLEU scores against both the ground truth references and the baseline, and updating the model based on the advantage.
6. **Training Iterations:**
    
    - This process is repeated over multiple training iterations, allowing the model to adjust its parameters to generate sequences that are more likely to receive higher BLEU scores.

In summary, the BLEU calculation in SCST involves comparing the machine-generated sequences not only to ground truth references but also to a baseline. The advantage, representing the difference in BLEU scores, is then used to guide the optimization of the model's parameters through policy gradient methods. This approach aims to directly optimize the model for the evaluation metric (BLEU) used in the reinforcement learning framework.