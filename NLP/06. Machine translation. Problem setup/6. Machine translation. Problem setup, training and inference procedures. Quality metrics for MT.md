[Источник](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)

==в процессе, Евгений==

# Problem setup
Formally, in the machine translation task, we have an input sequence $x_1, x_2, \dots, x_m$ and an output sequence $y_1, y_2, \dots, y_n$ (note that their lengths can be different). Translation can be thought of as finding the target sequence that is the most probable given the input; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x).$

If you are bilingual and can translate between languages easily, you have an intuitive feeling of $p(y|x)$ and can say something like "...well, this translation is kind of more natural for this sentence". But in machine translation, we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta).$

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/human_machine_translation-min.png)

To define a machine translation system, we need to answer three questions:

- modeling - how does the model for $p(y|x, \theta)$ look like?
- learning - how to find the parameters $\theta$?
- inference - how to find the best $y$?

==test zone==


# Sequence to Sequence Basics

Formally, in the machine translation task, we have an input sequence $x_1, x_2, \dots, x_m$ and an output sequence $y_1, y_2, \dots, y_n$ (note that their lengths can be different). Translation can be thought of as finding the target sequence that is the most probable given the input; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x).$

If you are bilingual and can translate between languages easily, you have an intuitive feeling of $p(y|x)$ and can say something like "...well, this translation is kind of more natural for this sentence". But in machine translation, we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta).$

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/human_machine_translation-min.png)

To define a machine translation system, we need to answer three questions:

- modeling - how does the model for $p(y|x, \theta)$ look like?
- learning - how to find the parameters $\theta$?
- inference - how to find the best $y$?

In this section, we will answer the second and third questions in full, but consider only the simplest model. The more "real" models will be considered later in sections [Attention](https://lena-voita.github.io/nlp_course/attention) and [Transformer](https://lena-voita.github.io/nlp_course/transformer).

## Encoder-Decoder Framework

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec-min.png)

Encoder-decoder is the standard modeling paradigm for sequence-to-sequence tasks. This framework consists of two components:

- encoder - reads source sequence and produces its representation;
- decoder - uses source representation from the encoder to generate the target sequence.

In this lecture, we'll see different models, but they all have this encoder-decoder structure.

## Conditional Language Models

In the [Language Modeling](https://lena-voita.github.io/nlp_course/language_modeling.html) lecture, we learned to estimate the probability $p(y)$ of sequences of tokens $y=(y_1, y_2, \dots, y_n)$. While language models estimate the unconditional probability $p(y)$ of a sequence $y$, sequence-to-sequence models need to estimate the conditional probability p(y|x) of a sequence $y$ given a source $x$. That's why sequence-to-sequence tasks can be modeled as Conditional Language Models (CLM) - they operate similarly to LMs, but additionally receive source information $x$.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/lm_clm-min.png)

Lena: Note that Conditional Language Modeling is something more than just a way to solve sequence-to-sequence tasks. In the most general sense, $x$ can be something other than a sequence of tokens. For example, in the Image Captioning task, $x$ is an image and $y$ is a description of this image.

[×<MEDIA>@https://lena-voita.github.io](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_prob_idea.mp4 "<MEDIA>@https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_prob_idea.mp4")

Since the only difference from LMs is the presence of source $x$, the modeling and training is very similar to language models. In particular, the high-level pipeline is as follows:

- feed source and previously generated target words into a network;
- get vector representation of context (both source and previous target) from the networks decoder;
- from this vector representation, predict a probability distribution for the next token.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_linear_out-min.png)

Similarly to neural classifiers and language models, we can think about the classification part (i.e., how to get token probabilities from a vector representation of a text) in a very simple way. Vector representation of a text has some dimensionality $d$, but in the end, we need a vector of size $|V|$ (probabilities for $|V|$ tokens/classes). To get a $|V|$-sized vector from a $d$-sized, we can use a linear layer. Once we have a $|V|$-sized vector, all is left is to apply the softmax operation to convert the raw numbers into token probabilities.

## The Simplest Model: Two RNNs for Encoder and Decoder

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_simple_rnn-min.png)

The simplest encoder-decoder model consists of two RNNs (LSTMs): one for the encoder and another for the decoder. Encoder RNN reads the source sentence, and the final state is used as the initial state of the decoder RNN. The hope is that the final encoder state "encodes" all information about the source, and the decoder can generate the target sentence based on this vector.

This model can have different modifications: for example, the encoder and decoder can have several layers. Such a model with several layers was used, for example, in the paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf) - one of the first attempts to solve sequence-to-sequence tasks using neural networks.

In the same paper, the authors looked at the last encoder state and visualized several examples - look below. Interestingly, representations of sentences with similar meaning but different structure are close!

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/rnn_simple_examples-min.png)  
The examples are from the paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf).

![](https://lena-voita.github.io/resources/lectures/ico/bulb_empty.png)

The paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf) introduced an elegant trick to make such a simple LSTM model work better. Learn more in [this exercise](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#research_reverse_order_in_lstm) in the [Research Thinking](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#research_thinking) section.

## Training: The Cross-Entropy Loss (Once Again)

Lena: This is the same cross-entropy loss we discussed before in the [Text Classification](https://lena-voita.github.io/nlp_course/text_classification.html) and in the [Language Modeling](https://lena-voita.github.io/nlp_course/language_modeling.html) lectures - you can skip this part or go through it quite easily :)

Similarly to neural LMs, neural seq2seq models are trained to predict probability distributions of the next token given previous context (source and previous target tokens). Intuitively, at each step we maximize the probability a model assigns to the correct token.

Formally, let's assume we have a training instance with the source $x=(x_1, \dots, x_m)$ and the target $y=(y_1, \dots, y_n)$. Then at the timestep $t$, a model predicts a probability distribution $p^{(t)} = p(\ast|y_1, \dots, y_{t-1}, x_1, \dots, x_m)$. The target at this step is $p^{\ast}=\mbox{one-hot}(y_t)$, i.e., we want a model to assign probability 1 to the correct token, $y_t$, and zero to the rest.

The standard loss function is the cross-entropy loss. Cross-entropy loss for the target distribution $p^{\ast}$ and the predicted distribution $p^{}$ is \[Loss(p^{\ast}, p^{})= - p^{\ast} \log(p) = -\sum\limits_{i=1}^{|V|}p_i^{\ast} \log(p_i).\] Since only one of $p_i^{\ast}$ is non-zero (for the correct token $y_t$), we will get \[Loss(p^{\ast}, p) = -\log(p_{y_t})=-\log(p(y_t| y_{\mbox{<}t}, x)).\] At each step, we maximize the probability a model assigns to the correct token. Look at the illustration for a single timestep.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/one_step_loss_intuition-min.png)

For the whole example, the loss will be $-\sum\limits_{t=1}^n\log(p(y_t| y_{\mbox{<}t}, x))$. Look at the illustration of the training process (the illustration is for the RNN model, but the model can be different).

[×<MEDIA>@https://lena-voita.github.io](https://lena-voita.github.io/resources/lectures/seq2seq/general/seq2seq_training_with_target.mp4 "<MEDIA>@https://lena-voita.github.io/resources/lectures/seq2seq/general/seq2seq_training_with_target.mp4")

  
  

## Inference: Greedy Decoding and Beam Search

Now when we understand how a model can look like and how to train this model, let's think how to generate a translation using this model. We model the probability of a sentence as follows:

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/inference_formula-min.png)

Now the main question is: how to find the argmax?

Note that we can not find the exact solution. The total number of hypotheses we need to check is $|V|^n$, which is not feasible in practice. Therefore, we will find an approximate solution.

Lena: In reality, the exact solution is usually worse than the approximate ones we will be using.

### • Greedy Decoding: At each step, pick the most probable token

The straightforward decoding strategy is greedy - at each step, generate a token with the highest probability. This can be a good baseline, but this method is inherently flawed: the best token at the current step does not necessarily lead to the best sequence.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/greedy_is_bad-min.png)

### • Beam Search: Keep track of several most probably hypotheses

Instead, let's keep several hypotheses. At each step, we will be continuing each of the current hypotheses and pick top-N of them. This is called beam search.

[×<MEDIA>@https://lena-voita.github.io](https://lena-voita.github.io/resources/lectures/seq2seq/general/beam_search.mp4 "<MEDIA>@https://lena-voita.github.io/resources/lectures/seq2seq/general/beam_search.mp4")

Usually, the beam size is 4-10. Increasing beam size is computationally inefficient and, what is more important, leads to worse quality.
