[Источник](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)

==в процессе, Евгений==

# Problem setup
Formally, in the machine translation task, we have an input sequence $x_1, x_2, \dots, x_m$ and an output sequence $y_1, y_2, \dots, y_n$ (note that their lengths can be different). Translation can be thought of as finding the target sequence that is the most probable given the input; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x).$

If you are bilingual and can translate between languages easily, you have an intuitive feeling of $p(y|x)$ and can say something like "...well, this translation is kind of more natural for this sentence". But in machine translation, we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta).$

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/human_machine_translation-min.png)

To define a machine translation system, we need to answer three questions:

- modeling - how does the model for $p(y|x, \theta)$ look like?
- learning - how to find the parameters $\theta$?
- inference - how to find the best $y$?

