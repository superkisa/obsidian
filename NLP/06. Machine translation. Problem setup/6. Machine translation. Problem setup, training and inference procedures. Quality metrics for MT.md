[Источник](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)

==в процессе, Евгений==

# Problem setup
Formally, in the machine translation task, we have an input sequence $x_1, x_2, \dots, x_m$ and an output sequence $y_1, y_2, \dots, y_n$ (note that their lengths can be different). Translation can be thought of as finding the target sequence that is the most probable given the input; formally, the target sequence that maximizes the conditional probability $p(y|x)$: $y^{\ast}=\arg\max\limits_{y}p(y|x).$

If you are bilingual and can translate between languages easily, you have an intuitive feeling of $p(y|x)$ and can say something like "...well, this translation is kind of more natural for this sentence". But in machine translation, we learn a function $p(y|x, \theta)$ with some parameters $\theta$, and then find its argmax for a given input: $y'=\arg\max\limits_{y}p(y|x, \theta).$

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/human_machine_translation-min.png)

To define a machine translation system, we need to answer three questions:

- modeling - how does the model for $p(y|x, \theta)$ look like?
- learning - how to find the parameters $\theta$?
- inference - how to find the best $y$?

## Conditional Language Models

In the [Language Modeling](https://lena-voita.github.io/nlp_course/language_modeling.html) lecture, we learned to estimate the probability $p(y)$ of sequences of tokens $y=(y_1, y_2, \dots, y_n)$. While language models estimate the unconditional probability $p(y)$ of a sequence $y$, sequence-to-sequence models need to estimate the conditional probability p(y|x) of a sequence $y$ given a source $x$. That's why sequence-to-sequence tasks can be modeled as Conditional Language Models (CLM) - they operate similarly to LMs, but additionally receive source information $x$.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/lm_clm-min.png)

Lena: Note that Conditional Language Modeling is something more than just a way to solve sequence-to-sequence tasks. In the most general sense, $x$ can be something other than a sequence of tokens. For example, in the Image Captioning task, $x$ is an image and $y$ is a description of this image.

![[https%3A%2F%2Flena-voita.github.io%2Fresources%2Flectures%2Fseq2seq%2Fgeneral%2Fenc_dec_prob_idea.mp4]]

Since the only difference from LMs is the presence of source $x$, the modeling and training is very similar to language models. In particular, the high-level pipeline is as follows:

- feed source and previously generated target words into a network;
- get vector representation of context (both source and previous target) from the networks decoder;
- from this vector representation, predict a probability distribution for the next token.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_linear_out-min.png)

Similarly to neural classifiers and language models, we can think about the classification part (i.e., how to get token probabilities from a vector representation of a text) in a very simple way. Vector representation of a text has some dimensionality $d$, but in the end, we need a vector of size $|V|$ (probabilities for $|V|$ tokens/classes). To get a $|V|$-sized vector from a $d$-sized, we can use a linear layer. Once we have a $|V|$-sized vector, all is left is to apply the softmax operation to convert the raw numbers into token probabilities.

## The Simplest Model: Two RNNs for Encoder and Decoder

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_simple_rnn-min.png)

The simplest encoder-decoder model consists of two RNNs (LSTMs): one for the encoder and another for the decoder. Encoder RNN reads the source sentence, and the final state is used as the initial state of the decoder RNN. The hope is that the final encoder state "encodes" all information about the source, and the decoder can generate the target sentence based on this vector.

==(кстати, улучшить производительность такой модели можно просто перевернув порядок входной последовательности)==

# Training: The Cross-Entropy Loss (Once Again)

Lena: This is the same cross-entropy loss we discussed before in the [Text Classification](https://lena-voita.github.io/nlp_course/text_classification.html) and in the [Language Modeling](https://lena-voita.github.io/nlp_course/language_modeling.html) lectures - you can skip this part or go through it quite easily :)

Similarly to neural LMs, neural seq2seq models are trained to predict probability distributions of the next token given previous context (source and previous target tokens). Intuitively, at each step we maximize the probability a model assigns to the correct token.

Formally, let's assume we have a training instance with the source $x=(x_1, \dots, x_m)$ and the target $y=(y_1, \dots, y_n)$. Then at the timestep $t$, a model predicts a probability distribution $p^{(t)} = p(\ast|y_1, \dots, y_{t-1}, x_1, \dots, x_m)$. The target at this step is $p^{\ast}=\mbox{one-hot}(y_t)$, i.e., we want a model to assign probability 1 to the correct token, $y_t$, and zero to the rest.

The standard loss function is the cross-entropy loss. Cross-entropy loss for the target distribution $p^{\ast}$ and the predicted distribution $p^{}$ is $$Loss(p^{\ast}, p^{})= - p^{\ast} \log(p) = -\sum\limits_{i=1}^{|V|}p_i^{\ast} \log(p_i).$$ Since only one of $p_i^{\ast}$ is non-zero (for the correct token $y_t$), we will get $$Loss(p^{\ast}, p) = -\log(p_{y_t})=-\log(p(y_t| y_{\mbox{<}t}, x)).$$ At each step, we maximize the probability a model assigns to the correct token. Look at the illustration for a single timestep.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/one_step_loss_intuition-min.png)

For the whole example, the loss will be $-\sum\limits_{t=1}^n\log(p(y_t| y_{\mbox{<}t}, x))$. 
Look at the illustration of the training process (the illustration is for the RNN model, but the model can be different).

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/seq2seq_training_with_target.mp4)
  

# Inference: Greedy Decoding and Beam Search

Now when we understand how a model can look like and how to train this model, let's think how to generate a translation using this model. We model the probability of a sentence as follows:

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/inference_formula-min.png)

Now the main question is: how to find the argmax?

Note that we can not find the exact solution. The total number of hypotheses we need to check is $|V|^n$, which is not feasible in practice. Therefore, we will find an approximate solution.

Lena: In reality, the exact solution is usually worse than the approximate ones we will be using.

### • Greedy Decoding: At each step, pick the most probable token

The straightforward decoding strategy is greedy - at each step, generate a token with the highest probability. This can be a good baseline, but this method is inherently flawed: the best token at the current step does not necessarily lead to the best sequence.

![](https://lena-voita.github.io/resources/lectures/seq2seq/general/greedy_is_bad-min.png)

### • Beam Search: Keep track of several most probably hypotheses

Instead, let's keep several hypotheses. At each step, we will be continuing each of the current hypotheses and pick top-N of them. This is called beam search.

[×<MEDIA>@https://lena-voita.github.io](https://lena-voita.github.io/resources/lectures/seq2seq/general/beam_search.mp4 "<MEDIA>@https://lena-voita.github.io/resources/lectures/seq2seq/general/beam_search.mp4")

Usually, the beam size is 4-10. Increasing beam size is computationally inefficient and, what is more important, leads to worse quality.
