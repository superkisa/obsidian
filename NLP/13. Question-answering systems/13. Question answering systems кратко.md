==лекции и чатик==

How SQUAD was trained:
1. **Dataset Preparation:**
    
    - SQuAD consists of a collection of Wikipedia articles with associated question-answer pairs.
    - The dataset is split into training, validation, and test sets.
2. **Preprocessing:**
    
    - Texts are tokenized into words or subwords.
    - Context passages, questions, and answers are converted into a suitable format for model training.
3. **Model Architecture:**
    
    - Models used for SQuAD are often based on deep learning architectures, such as bidirectional LSTMs, transformers, or other neural network variants.
    - The model takes the context passage and question as input and outputs the predicted answer span.
4. **Loss Function:**
    
    - The training objective is typically to minimize a loss function, such as the categorical cross-entropy loss.
    - The loss is calculated based on the predicted answer span and the ground truth answer span.
5. **Training Procedure:**
    
    - The model is trained using backpropagation and gradient descent.
    - The optimizer adjusts the model's parameters to minimize the loss on the training set.
    - Learning rate schedules or other optimization techniques may be applied to enhance training stability.
6. **Validation:**
    
    - The model's performance is monitored on the validation set during training.
    - Early stopping or other regularization techniques may be employed based on the validation performance.
7. **Evaluation:**
    
    - After training, the model is evaluated on the test set using metrics like Exact Match (EM) and F1 score.
    - EM measures the percentage of predicted answers that match the ground truth exactly.
    - F1 score computes the harmonic mean of precision and recall for answer spans.
8. **Fine-Tuning (Optional):**
    
    - Some approaches involve fine-tuning the model on domain-specific data or incorporating additional techniques to improve performance.

SQuAD data collection and evaluation procedures:
	**Data Collection Procedure:**
1. **Document Selection:**
    
    - Relevant Wikipedia articles are selected to serve as the source of context passages.
    - These articles cover a diverse range of topics to ensure a broad dataset.
2. **Annotation Process:**
    
    - Human annotators create question-answer pairs based on the content of the selected articles.
    - The annotators formulate questions that can be answered using information present in the context passages.
3. **Answer Span Annotation:**
    
    - For each question, annotators highlight the span of text in the corresponding context passage that contains the answer.
    - The answer span becomes the ground truth for training and evaluation.
4. **Quality Control:**
    
    - Quality control measures are implemented to ensure the accuracy and consistency of annotations.
    - Annotators may receive feedback and undergo training to maintain annotation standards.
5. **Dataset Splitting:**
    
    - The dataset is divided into training, validation, and test sets.
    - This split ensures that models are trained on one portion, validated on another, and tested on a separate unseen portion.

**Evaluation Procedure:**

1. **Metric: Exact Match (EM) and F1 Score:**
    
    - EM measures the percentage of predicted answer spans that match the ground truth exactly.
    - F1 score computes the harmonic mean of precision and recall for the predicted and true answer spans.
2. **Scoring:**
    
    - For each question in the test set, the predicted answer span is compared to the ground truth answer span.
    - If the predicted span exactly matches the ground truth, the EM score is incremented.
    - The F1 score is calculated based on precision and recall, giving a more nuanced measure.
3. **Aggregation:**
    
    - The overall EM and F1 scores for the entire test set are calculated by aggregating scores across individual questions.
4. **Comparative Analysis:**
    
    - SQuAD leaderboard and research papers often present comparative analyses of different models' performance.
    - Models are ranked based on their EM and F1 scores.
5. **Adversarial Evaluation:**
    
    - To assess a model's robustness, adversarial evaluation may be performed by introducing challenging examples or modifying questions.
6. **Error Analysis:**
    
    - Researchers often conduct error analysis to understand the types of mistakes models make and identify areas for improvement.

![[Screen Shot 2024-01-19 at 11.34.58.png]]![[Screen Shot 2024-01-19 at 11.38.23.png]]![[Screen Shot 2024-01-19 at 11.53.38.png]]![[Screen Shot 2024-01-19 at 11.55.52.png]]![[Screen Shot 2024-01-19 at 11.58.37.png]]![[Screen Shot 2024-01-19 at 12.10.44.png]]![[Screen Shot 2024-01-19 at 12.12.29.png]]![[Screen Shot 2024-01-19 at 12.14.05.png]]![[Screen Shot 2024-01-19 at 12.18.36.png]]
