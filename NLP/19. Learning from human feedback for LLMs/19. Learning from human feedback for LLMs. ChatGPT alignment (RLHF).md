**Игорь в работе**


Это предполагает точечную коррекцию со стороны пользователя без переобучения всей модели заново
 LLM, которые обычно предсказывают следующий токен в последовательности, по существу изучая распределение вероятностей слов. Было показано, что они генерируют текст, очень похожий на человеческий. Однако это может привести к проблеме несовпадения, когда сгенерированный текст не соответствует тому, что задумал пользователь, или является вредным.  
Несовпадение — это общий термин в области искусственного интеллекта и машинного обучения, который относится к проблеме модели. не выполняет поставленную перед ним задачу. В этом случае модель может хорошо работать при оценках, но не давать желаемого решения.
При обучении с подкреплением модель учится с помощью системы действий и вознаграждений, в которой хорошее поведение вознаграждается, а плохое не поощряется. Включив RLHF, модель может узнать, какие результаты LLM предпочитают люди, что позволит ей генерировать более подходящие токены.

![[Pasted image 20240119210211.png]]




 It is essential to consider user feedback to improve the model and avoid biases or discriminatory behavior.
