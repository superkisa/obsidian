**Игорь в работе**


Это предполагает точечную коррекцию со стороны пользователя без переобучения всей модели заново
 LLM, которые обычно предсказывают следующий токен в последовательности, по существу изучая распределение вероятностей слов. Было показано, что они генерируют текст, очень похожий на человеческий. Однако это может привести к проблеме несовпадения, когда сгенерированный текст не соответствует тому, что задумал пользователь, или является вредным.  
Несовпадение — это общий термин в области искусственного интеллекта и машинного обучения, который относится к проблеме модели. не выполняет поставленную перед ним задачу. В этом случае модель может хорошо работать при оценках, но не давать желаемого решения.
При обучении с подкреплением модель учится с помощью системы действий и вознаграждений, в которой хорошее поведение вознаграждается, а плохое не поощряется. Включив RLHF, модель может узнать, какие результаты LLM предпочитают люди, что позволит ей генерировать более подходящие токены.

оптимизировать LLM для обеспечения качества. Это означает изменение целевой функции для определения приоритета качества. Это достигается за счет обучения с подкреплением с обратной связью человека. Здесь агент обучения с подкреплением обучается генерировать текст, который люди считают высококачественным. Обучение с подкреплением обучает агента выполнять задачу, заставляя его учиться на своих действиях в окружающей среде. Агент обучается через систему вознаграждения за действие, в которой за любое предпринятое действие предусмотрена награда (положительная или отрицательная). В этом случае агент получает положительное или отрицательное вознаграждение в зависимости от своего результата (сгенерированного текста). Люди-оценщики помогают определить, является ли результат агента качественным или нет.

![[Pasted image 20240119210211.png]]


В своей работе Stiennon et al. использовал набор данных TL; DR, который содержит сообщения Reddit и их резюме. Обучение проходит в 3 этапа:

Начиная с контролируемого базового уровня, то есть предварительно обученного LLM, точно настроенного с помощью контролируемого обучения на наборе данных TL; DR. Этот базовый уровень используется для создания резюме для сообщений. Резюме из данного поста объединяются в пары и передаются людям, навешивающим ярлыки, которые выбирают, какое из двух резюме они сочтут лучшим.

Используя данные, собранные от людей, навешивающих ярлыки, модель вознаграждения обучается выводить скалярное значение (вознаграждение) при получении публикации и сгенерированной сводки. Модель вознаграждения также представляет собой точно настроенный LLM с замененным выходным слоем, поэтому модель выводит скаляр. В модели используется функция потерь, которая сравнивает качество двух резюме для поста.

Учитывая сообщение x и сводки {y₀, y₁} и учитывая, что человек выбирает сводку yᵢ как лучшую сводку, тогда потеря представлена ​​​​как

![](https://i2.wp.com/miro.medium.com/1*wOlyqgSYlVmERZkvkHi4GQ.png)

где,

- r(x, yi) — прогнозируемое вознаграждение за сводку yi с учетом поста x
- rθ(x, y1−i) — прогнозируемая награда за другую сводку y1−i с учетом поста x
- σ - сигмовидная функция
- E(x,y0,y1,i)∼D представляет математическое ожидание по данным D всех комбинаций x, y0, y1 и i, где i равно 0 или 1, и указывает, какая сводка предпочтительнее для оценщика-человека. ”


OpenAI выпустила документ по InstructGPT, который, похоже, является основой, на которой был построен ChatGPT. Эта статья под названием «Обучение языковых моделей следованию инструкциям с обратной связью от человека» Ouyang et al.
Процесс обучения также состоит из трех этапов, как показано ниже.
![[Pasted image 20240119210819.png]]