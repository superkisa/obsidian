https://www.evogeek.ru/articles/700017/

Это предполагает точечную коррекцию со стороны пользователя без переобучения всей модели заново
 LLM, которые обычно предсказывают следующий токен в последовательности, по существу изучая распределение вероятностей слов. Было показано, что они генерируют текст, очень похожий на человеческий. Однако это может привести к проблеме несовпадения, когда сгенерированный текст не соответствует тому, что задумал пользователь, или является вредным.  
Несовпадение — это общий термин в области искусственного интеллекта и машинного обучения, который относится к проблеме модели. не выполняет поставленную перед ним задачу. В этом случае модель может хорошо работать при оценках, но не давать желаемого решения.
При обучении с подкреплением модель учится с помощью системы действий и вознаграждений, в которой хорошее поведение вознаграждается, а плохое не поощряется. Включив RLHF, модель может узнать, какие результаты LLM предпочитают люди, что позволит ей генерировать более подходящие токены.

оптимизировать LLM для обеспечения качества. Это означает изменение целевой функции для определения приоритета качества. Это достигается за счет обучения с подкреплением с обратной связью человека. Здесь агент обучения с подкреплением обучается генерировать текст, который люди считают высококачественным. Обучение с подкреплением обучает агента выполнять задачу, заставляя его учиться на своих действиях в окружающей среде. Агент обучается через систему вознаграждения за действие, в которой за любое предпринятое действие предусмотрена награда (положительная или отрицательная). В этом случае агент получает положительное или отрицательное вознаграждение в зависимости от своего результата (сгенерированного текста). Люди-оценщики помогают определить, является ли результат агента качественным или нет.

![[Pasted image 20240119210211.png]]


В своей работе Stiennon et al. использовал набор данных TL; DR, который содержит сообщения Reddit и их резюме. Обучение проходит в 3 этапа:

Начиная с контролируемого базового уровня, то есть предварительно обученного LLM, точно настроенного с помощью контролируемого обучения на наборе данных TL; DR. Этот базовый уровень используется для создания резюме для сообщений. Резюме из данного поста объединяются в пары и передаются людям, навешивающим ярлыки, которые выбирают, какое из двух резюме они сочтут лучшим.

Используя данные, собранные от людей, навешивающих ярлыки, модель вознаграждения обучается выводить скалярное значение (вознаграждение) при получении публикации и сгенерированной сводки. Модель вознаграждения также представляет собой точно настроенный LLM с замененным выходным слоем, поэтому модель выводит скаляр. В модели используется функция потерь, которая сравнивает качество двух резюме для поста.

Учитывая сообщение x и сводки {y₀, y₁} и учитывая, что человек выбирает сводку yᵢ как лучшую сводку, тогда потеря представлена ​​​​как

![](https://i2.wp.com/miro.medium.com/1*wOlyqgSYlVmERZkvkHi4GQ.png)

где,

- r(x, yi) — прогнозируемое вознаграждение за сводку yi с учетом поста x
- rθ(x, y1−i) — прогнозируемая награда за другую сводку y1−i с учетом поста x
- σ - сигмовидная функция
- E(x,y0,y1,i)∼D представляет математическое ожидание по данным D всех комбинаций x, y0, y1 и i, где i равно 0 или 1, и указывает, какая сводка предпочтительнее для оценщика-человека. ”


OpenAI выпустила документ по InstructGPT, который, похоже, является основой, на которой был построен ChatGPT. Эта статья под названием «Обучение языковых моделей следованию инструкциям с обратной связью от человека» Ouyang et al.
Процесс обучения также состоит из трех этапов, как показано ниже.
![[Pasted image 20240119210819.png]]

Модель с точной настройкой под наблюдением (называемая в документе SFT) также используется в качестве модели вознаграждения (RM) с заменой конечных выходных слоев линейной головкой, поэтому модель выводит одно скалярное значение. Но в отличие от предыдущего, когда Siennon et al. сравнил только два выхода, чтобы увидеть, какой из них лучше, здесь для каждой подсказки сравнивалось от 4 до 9 выходов. Поэтому маркировщиков попросили ранжировать результаты. Они оценивают каждый вывод на основе его общего качества, а также проверяют, содержит ли он, среди прочего, вредный или неприемлемый контент.

![](https://i2.wp.com/miro.medium.com/1*fe5uB1xv3set0CCvh4MRvg.png)

Этот новый метод немного меняет функцию потерь. Ранее для каждого поста выполнялось только одно сравнение, поскольку генерировались только два вывода. Но здесь для каждой подсказки генерируется от четырех до девяти выходов, поэтому для их парного сравнения потребуется от 6 до 66 сравнений. В общем, при заданных K выходах необходимо было бы провести сравнения «KC2» (комбинация K 2). Функция потерь для модели вознаграждения меняется на

![](https://i2.wp.com/miro.medium.com/1*tBepxy5seEPkj6zEldJVNw.png)

1/(KC2) добавляется, чтобы все сравнения оказывали одинаковое влияние на потери.  
Функция вознаграждения используется для обучения политики для получения лучших ответов. Политика здесь также является тонко настроенным LLM и обучается с использованием алгоритма PPO. Это следует тому же методу, который использовали Siennon et. др.


Рекомендации

1. [Изучение резюме на основе отзывов людей](https://arxiv.org/abs/2009.01325): Нисан Стиннон, Лонг Оуян, Джефф Ву, Дэниел М. Зиглер, Райан Лоу, Челси Восс, Алек Рэдфорд, Дарио Амодей, Пол Кристиано
2. [Обучение языковых моделей следованию инструкциям с обратной связью человека](https://arxiv.org/abs/2203.02155): Лонг Оуян, Джефф Ву, Сюй Цзян, Диого Алмейда, Кэрролл Л. Уэйнрайт, Памела Мишкин, Чонг Чжан, Сандхини Агарвал, Катарина Слама, Алекс Рэй, Джон Шульман, Джейкоб Хилтон, Фрейзер Келтон, Люк Миллер, Мэдди Сименс, Аманда Аскелл, Питер Велиндер, Пол Кристиано, Ян Лейке, Райан Лоу