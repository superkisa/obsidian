
- BoW - bag of words, steps:
	- tokenisation
	- vocabulary building
	- frequencies counting
- TF-IDF
	- $$ \text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d} $$
	- $$ \text{IDF}(t, D) = \log\left(\frac{\text{Total number of documents in the corpus } |D|}{\text{Number of documents containing term } t + 1}\right) $$
	- $$ \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D) $$
- **Word Embeddings - Word2Vec:**
	- Word2Vec Models:
		- Two main architectures: CBOW (Continuous Bag of Words) and Skip-Gram.
		- CBOW predicts target word from context words, while Skip-Gram predicts context words from a target word.
		- Developed by Tomas Mikolov and Google.
	- Training Technique - Negative Sampling:
		- Addresses computational inefficiency of training on large vocabularies.
		- Randomly selects a small number of negative samples (words not in context) for weight updates.
	- Word2Vec Properties:
		- **Semantic Similarity:**
		    - Words with similar meanings have similar vector representations.
		- **Analogies:**
		    - Exhibits analogy relationships, e.g., "king" - "man" + "woman" ≈ "queen."
		- **Vector Arithmetic:**
		    - Operations like addition and subtraction on word vectors produce meaningful results.
		- **Contextual Information:**
		    - Learns word representations based on surrounding context, capturing nuances in semantics
	- _Applications:_
		- Enhances NLP tasks like sentiment analysis, machine translation, and named entity recognition.
		- Provides continuous vector space representations for words, improving model understanding of language.
- Translation 
	- Problem statement
		- we have a training instance with the source $x=(x_1, \dots, x_m)$ and the target $y=(y_1, \dots, y_n)$. Then at the timestep $t$, a model predicts a probability distribution $p^{(t)} = p(\ast|y_1, \dots, y_{t-1}, x_1, \dots, x_m)$. The target at this step is $p^{\ast}=\mbox{one-hot}(y_t)$, i.e., we want a model to assign probability 1 to the correct token, $y_t$, and zero to the rest
	- Loss function
		- The standard loss function is the cross-entropy loss. Cross-entropy loss for the target distribution $p^{\ast}$ and the predicted distribution $p^{}$ is $$Loss(p^{\ast}, p^{})= - p^{\ast} \log(p) = -\sum\limits_{i=1}^{|V|}p_i^{\ast} \log(p_i).$$ Since only one of $p_i^{\ast}$ is non-zero (for the correct token $y_t$), we will get $$Loss(p^{\ast}, p) = -\log(p_{y_t})=-\log(p(y_t| y_{\mbox{<}t}, x)).$$ At each step, we maximize the probability a model assigns to the correct token. Look at the illustration for a single timestep.
	- Inference
		- greedy
		- sampling
		- beam search
			- create several hypotheses instead of one
- Seq2seq, encoder-decoder
	- BPE tokenisation
		- repeat until size of voc is sufficient
- Attention
	- Decoder has access to all hidden states of encoder => similarity calculated => weighted sum => concatenated with decoder
	- Problems:
		- quadratic in computation cost
- Self-attention
	- Get representation of the whole sentence from the point of some word => calculate some priors, query(q), key(k), value(v) are some abstractions![[Screen Shot 2024-01-18 at 17.27.44.png]]
- Transformer
	- ![[Screen Shot 2024-01-18 at 17.15.48.png]]
- Bert
	- encoder from a transformer that is pretrained on NSP and MLM tasks
	- MLM
	- NSP
	- AdamW is Adam adapted for the transformers
	- works well for discriminative tasks
- Q&A systems
	- decompose task into two binary classification tasks: is this a start token/end token of the answer
	- BIDAF
		- question to context and context to question attention layer added
	- Bert
		- [cls]question[sep]context
	- SQUaAD dataset trained by humans on wikipedia articles
- SCST (self-critical sequence training)
	- RL method for text generation: optimises metric/reward
	- saves baseline seq at training => calculates BLEU against ground truth and baseline => uses for reward calculation
	- if model gives better BLEU than baseline => positive reward
	- if model gives lower BLEU than baseline => negative reward
- T0
	- обучение в формате "инструкция: текст инструкции"
- T5
	- helps the model to generalise one task to another through the means of natural language instructions - текстовые инструкции заменены естественным языком
	- trained on cross entropy
		- **Relative position embeddings** - instead of long matrix of embeddings (that combine semantic and positional information) it maintains two distinct matrices. Attention performs two attention computations for semantic and positional matrices (positional embeddings keep distance between the tokens in the attn mechanism, not the abs position in the seq).
- RLHF
	- ![[Screen Shot 2024-01-20 at 11.01.02.png]]