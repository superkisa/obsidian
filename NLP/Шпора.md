
- BoW - bag of words, steps:
	- tokenisation
	- vocabulary building
	- frequencies counting
- TF-IDF
	- $$ \text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d} $$
	- $$ \text{IDF}(t, D) = \log\left(\frac{\text{Total number of documents in the corpus } |D|}{\text{Number of documents containing term } t + 1}\right) $$
	- $$ \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D) $$
- **Word Embeddings - Word2Vec:**
	- Word2Vec Models:
		- Two main architectures: CBOW (Continuous Bag of Words) and Skip-Gram.
		- CBOW predicts target word from context words, while Skip-Gram predicts context words from a target word.
		- Developed by Tomas Mikolov and Google.
	- Training Technique - Negative Sampling:
		- Addresses computational inefficiency of training on large vocabularies.
		- Randomly selects a small number of negative samples (words not in context) for weight updates.
	- Word2Vec Properties:
		- **Semantic Similarity:**
		    - Words with similar meanings have similar vector representations.
		- **Analogies:**
		    - Exhibits analogy relationships, e.g., "king" - "man" + "woman" â‰ˆ "queen."
		- **Vector Arithmetic:**
		    - Operations like addition and subtraction on word vectors produce meaningful results.
		- **Contextual Information:**
		    - Learns word representations based on surrounding context, capturing nuances in semantics
	- _Applications:_
		- Enhances NLP tasks like sentiment analysis, machine translation, and named entity recognition.
		- Provides continuous vector space representations for words, improving model understanding of language.
- ...
- Seq2seq, encoder-decoder
	- BPE tokenisation
		- repeat until size of voc is sufficient
	- Decoding:
		- greedy
		- sampling
		- beam search
			- create several hypotheses instead of one
- Attention
	- Decoder has access to all hidden states of encoder => similarity calculated => weighted sum => concatenated with decoder
	- Problems:
		- quadratic in computation cost
- Self-attention
	- Get representation of the whole sentence from the point of some word => calculate some priors, query(q), key(k), value(v) are some abstractions