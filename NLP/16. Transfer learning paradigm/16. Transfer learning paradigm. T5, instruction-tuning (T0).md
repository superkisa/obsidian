в процессе (Лиза)
[Ссылка](https://habr.com/ru/articles/543412/)
# Общий Text-to-Text фреймворк

Наряду с Т5 авторы предлагают переосмыслить все NLP задачи и представить их в формате text-to-text, где вход и выход модели представляются текстовыми строками, в отличие от моделей типа BERT, подающие на выход или метку класса, или фрагмент входной последовательности. Предложенный же фреймворк позволяет использовать одну и ту же модель, функцию потерь и гиперпараметры для любой задачи NLP, включая машинный перевод, суммаризацию документов, вопросно-ответные системы, задачу классификации (например, анализ тональности). Модель Т5 можно использовать даже для задачи регрессии, обучив ее предсказывать строковое представление числа вместо самого числа.
![[Pasted image 20240117214316.png]]
_Диаграмма предложенного фреймворка. Для каждой из рассматриваемых задач на вход модели подается текст; обучение состоит в генерации некоторого целевого текста. Это позволяет использовать одни и те же модель, функцию потерь и гиперпараметры для решения различных задач, включая перевод (зеленым), лингвистическую состоятельность (красным), семантическую близость предложений (желтым), суммаризацию документа (синим). Это также позволяет иметь стандартный набор тестов для методов, рассмотренных в эмпирическом исследовании._

# Большой набор данных для обучения (С4)

Важная составляющая трансферного обучения – это наличие неразмеченного набора данных, используемого для предварительного обучения. Для того, чтобы точно измерить эффективность масштабирования предварительного обучения, необходимо иметь не просто качественные и разнообразные данные, но и большие их объемы. Существующие наборы данных не соответствуют сразу всем этим трем критериям: например, тексты из [Википедии](https://www.wikipedia.org/) обычно высокого качества, но довольно однообразны стилистически и имеют достаточно скромный общий объем. В то же время тексты [Common Crawl](https://commoncrawl.org/) имеют просто огромный размер и очень разнообразный состав, но их качество оставляет желать лучшего.

Для того, чтобы удовлетворить требованиям, описанным выше, был разработан Colossal Clean Crawled Corpus (C4) – вычищенная версия Common Crawl, объем которой на два порядка превышает объем Википедии. Процесс очистки набора данных включал удаление дубликатов, неполных предложений, а также неприемлемых или мусорных текстов. Подобная фильтрация способствовала получению лучших результатов в прикладных задачах, в то время как большой объем позволил увеличить размер модели без риска переобучения. С4 доступен в [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/c4).

# Систематическое исследование методологии трансферного обучения

С помощью предложенного T5 text-to-text фреймворка и нового набора данных для предварительного обучения С4 авторы смогли исследовать довольно большое количество идей и методов, предложенных в сфере трансферного обучения в NLP в последние несколько лет. Все детали исследования можно найти в соответствующей статье, включая эксперименты:
- _архитектурой_, в ходе которых выяснилось, что модели с энкодером и декодером обычно превосходят языковые модели с одним декодером;
- _целями предварительного обучения_, в ходе чего подтвердилось, что задачи типа заполни_пропуск (где модель обучается восстанавливать пропущенные слова во входном тексте) являются наилучшим решением и что самым важным фактором оказываются затраты на вычисление;
- _неразмеченными наборами данных_, в ходе которых удалось показать, что обучение на данных определенной предметной области может оказаться полезным, но что предварительное обучение на небольших наборах данных может привести к переобучению;
- _стратегиями обучения_, в ходе которых выяснилось, что многозадачное обучение может быть сравнимо с предварительным обучением и последующей тонкой настройкой, однако первое требует внимательного выбора частоты обучения модели на определенную задачу;
- _масштабами_, в ходе которых сравнивались увеличение размера модели, времени обучения и числа моделей в ансамбле для определения наиболее эффективного использования имеющейся вычислительной мощности.

  

# Инсайты + Масштаб = State-of-the-Art

Для нахождения существующих ограничений трансферного обучения в NLP авторы проделали финальный набор экспериментов, в которых они объединили все лучшие методы, определенные в их систематическом исследовании, и масштабировали их подход с помощью [TPU-ускорителей Google Cloud](https://cloud.google.com/tpu/). Крупнейшая модель имела 11 миллиардов параметров и достигла уровня state-of-the-art в рамках бенчмарков [GLUE](https://gluebenchmark.com/), [SuperGLUE](https://super.gluebenchmark.com/), [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) и [CNN/Daily Mail](https://github.com/abisee/cnn-dailymail). Особенно неожиданным результатом оказалось достижение околочеловеческого уровня в понимании естественного языка в рамках бенчмарка SuperGLUE, который разрабатывался намеренно сложным для моделей машинного обучения, но легким для человека.